Filling queue with 2400 CIFAR images before starting to train. This will take a few minutes.
2016-11-28 17:40:09.516707: step 0, loss = 6.17, acc = 0.03 (39.3 examples/sec; 3.256 sec/batch)
At step 0 cross validation precision: 0.383
2016-11-28 17:40:12.386624: step 10, loss = 5.60, acc = 0.27 (588.4 examples/sec; 0.218 sec/batch)
2016-11-28 17:40:14.473606: step 20, loss = 5.53, acc = 0.34 (610.0 examples/sec; 0.210 sec/batch)
2016-11-28 17:40:16.565201: step 30, loss = 5.55, acc = 0.38 (622.8 examples/sec; 0.206 sec/batch)
2016-11-28 17:40:18.691639: step 40, loss = 5.62, acc = 0.27 (634.1 examples/sec; 0.202 sec/batch)
2016-11-28 17:40:20.752274: step 50, loss = 5.47, acc = 0.35 (611.3 examples/sec; 0.209 sec/batch)
2016-11-28 17:40:22.811295: step 60, loss = 5.43, acc = 0.43 (595.0 examples/sec; 0.215 sec/batch)
2016-11-28 17:40:24.862274: step 70, loss = 5.42, acc = 0.38 (616.0 examples/sec; 0.208 sec/batch)
2016-11-28 17:40:26.939428: step 80, loss = 5.43, acc = 0.33 (618.6 examples/sec; 0.207 sec/batch)
2016-11-28 17:40:29.026220: step 90, loss = 5.32, acc = 0.38 (582.3 examples/sec; 0.220 sec/batch)
2016-11-28 17:40:31.092468: step 100, loss = 5.22, acc = 0.36 (605.2 examples/sec; 0.212 sec/batch)
At step 100 cross validation precision: 0.398
2016-11-28 17:40:33.947645: step 110, loss = 5.24, acc = 0.38 (585.5 examples/sec; 0.219 sec/batch)
2016-11-28 17:40:36.015991: step 120, loss = 5.10, acc = 0.34 (604.7 examples/sec; 0.212 sec/batch)
2016-11-28 17:40:38.066565: step 130, loss = 5.06, acc = 0.34 (639.5 examples/sec; 0.200 sec/batch)
2016-11-28 17:40:40.163243: step 140, loss = 5.18, acc = 0.34 (592.9 examples/sec; 0.216 sec/batch)
2016-11-28 17:40:42.211569: step 150, loss = 4.96, acc = 0.44 (607.6 examples/sec; 0.211 sec/batch)
2016-11-28 17:40:44.288487: step 160, loss = 5.19, acc = 0.31 (638.0 examples/sec; 0.201 sec/batch)
2016-11-28 17:40:46.355648: step 170, loss = 5.00, acc = 0.41 (613.2 examples/sec; 0.209 sec/batch)
2016-11-28 17:40:48.390113: step 180, loss = 5.02, acc = 0.48 (628.1 examples/sec; 0.204 sec/batch)
2016-11-28 17:40:50.469474: step 190, loss = 5.01, acc = 0.38 (604.7 examples/sec; 0.212 sec/batch)
2016-11-28 17:40:52.515214: step 200, loss = 4.88, acc = 0.37 (639.7 examples/sec; 0.200 sec/batch)
At step 200 cross validation precision: 0.312
2016-11-28 17:40:55.284902: step 210, loss = 4.94, acc = 0.36 (595.7 examples/sec; 0.215 sec/batch)
2016-11-28 17:40:57.362874: step 220, loss = 4.77, acc = 0.40 (649.2 examples/sec; 0.197 sec/batch)
2016-11-28 17:40:59.577818: step 230, loss = 4.84, acc = 0.43 (535.3 examples/sec; 0.239 sec/batch)
2016-11-28 17:41:01.671310: step 240, loss = 4.81, acc = 0.38 (618.5 examples/sec; 0.207 sec/batch)
2016-11-28 17:41:03.743744: step 250, loss = 4.82, acc = 0.38 (632.7 examples/sec; 0.202 sec/batch)
2016-11-28 17:41:05.841829: step 260, loss = 4.67, acc = 0.52 (609.1 examples/sec; 0.210 sec/batch)
2016-11-28 17:41:07.916003: step 270, loss = 4.80, acc = 0.44 (607.5 examples/sec; 0.211 sec/batch)
2016-11-28 17:41:09.994797: step 280, loss = 4.67, acc = 0.38 (643.9 examples/sec; 0.199 sec/batch)
2016-11-28 17:41:12.085343: step 290, loss = 4.67, acc = 0.39 (608.2 examples/sec; 0.210 sec/batch)
2016-11-28 17:41:14.129585: step 300, loss = 4.79, acc = 0.39 (634.0 examples/sec; 0.202 sec/batch)
At step 300 cross validation precision: 0.391
2016-11-28 17:41:16.993666: step 310, loss = 4.43, acc = 0.56 (629.7 examples/sec; 0.203 sec/batch)
2016-11-28 17:41:19.083069: step 320, loss = 4.53, acc = 0.45 (614.9 examples/sec; 0.208 sec/batch)
2016-11-28 17:41:21.158821: step 330, loss = 4.43, acc = 0.53 (619.2 examples/sec; 0.207 sec/batch)
2016-11-28 17:41:23.212768: step 340, loss = 4.58, acc = 0.42 (631.3 examples/sec; 0.203 sec/batch)
2016-11-28 17:41:25.272490: step 350, loss = 4.58, acc = 0.38 (604.5 examples/sec; 0.212 sec/batch)
2016-11-28 17:41:27.333386: step 360, loss = 4.55, acc = 0.42 (641.6 examples/sec; 0.199 sec/batch)
2016-11-28 17:41:29.398603: step 370, loss = 4.47, acc = 0.44 (635.9 examples/sec; 0.201 sec/batch)
2016-11-28 17:41:31.484953: step 380, loss = 4.44, acc = 0.42 (623.3 examples/sec; 0.205 sec/batch)
2016-11-28 17:41:33.568452: step 390, loss = 4.42, acc = 0.38 (615.1 examples/sec; 0.208 sec/batch)
2016-11-28 17:41:35.637853: step 400, loss = 4.40, acc = 0.43 (647.5 examples/sec; 0.198 sec/batch)
At step 400 cross validation precision: 0.453
2016-11-28 17:41:38.379012: step 410, loss = 4.31, acc = 0.42 (608.6 examples/sec; 0.210 sec/batch)
2016-11-28 17:41:40.450013: step 420, loss = 4.26, acc = 0.43 (629.9 examples/sec; 0.203 sec/batch)
2016-11-28 17:41:42.539154: step 430, loss = 4.25, acc = 0.47 (594.0 examples/sec; 0.215 sec/batch)
2016-11-28 17:41:44.657539: step 440, loss = 4.23, acc = 0.48 (573.6 examples/sec; 0.223 sec/batch)
2016-11-28 17:41:46.729929: step 450, loss = 4.24, acc = 0.40 (634.5 examples/sec; 0.202 sec/batch)
2016-11-28 17:41:48.801623: step 460, loss = 4.09, acc = 0.47 (620.7 examples/sec; 0.206 sec/batch)
2016-11-28 17:41:50.885221: step 470, loss = 4.19, acc = 0.43 (573.2 examples/sec; 0.223 sec/batch)
2016-11-28 17:41:52.941946: step 480, loss = 4.24, acc = 0.38 (603.7 examples/sec; 0.212 sec/batch)
2016-11-28 17:41:55.058542: step 490, loss = 4.11, acc = 0.47 (591.6 examples/sec; 0.216 sec/batch)
2016-11-28 17:41:57.150913: step 500, loss = 4.01, acc = 0.48 (600.9 examples/sec; 0.213 sec/batch)
At step 500 cross validation precision: 0.438
2016-11-28 17:41:59.964968: step 510, loss = 4.03, acc = 0.48 (613.0 examples/sec; 0.209 sec/batch)
2016-11-28 17:42:02.069676: step 520, loss = 4.00, acc = 0.45 (611.6 examples/sec; 0.209 sec/batch)
2016-11-28 17:42:04.167734: step 530, loss = 3.96, acc = 0.46 (621.1 examples/sec; 0.206 sec/batch)
2016-11-28 17:42:06.241571: step 540, loss = 4.04, acc = 0.40 (633.6 examples/sec; 0.202 sec/batch)
2016-11-28 17:42:08.367088: step 550, loss = 3.96, acc = 0.52 (565.7 examples/sec; 0.226 sec/batch)
2016-11-28 17:42:10.442436: step 560, loss = 3.93, acc = 0.47 (638.4 examples/sec; 0.200 sec/batch)
2016-11-28 17:42:12.530381: step 570, loss = 3.86, acc = 0.45 (605.7 examples/sec; 0.211 sec/batch)
2016-11-28 17:42:14.663261: step 580, loss = 3.90, acc = 0.47 (617.7 examples/sec; 0.207 sec/batch)
2016-11-28 17:42:16.788816: step 590, loss = 3.90, acc = 0.49 (582.7 examples/sec; 0.220 sec/batch)
2016-11-28 17:42:18.901027: step 600, loss = 3.84, acc = 0.46 (581.9 examples/sec; 0.220 sec/batch)
At step 600 cross validation precision: 0.438
2016-11-28 17:42:21.743518: step 610, loss = 3.95, acc = 0.47 (607.4 examples/sec; 0.211 sec/batch)
2016-11-28 17:42:23.845719: step 620, loss = 3.74, acc = 0.47 (617.5 examples/sec; 0.207 sec/batch)
2016-11-28 17:42:25.956528: step 630, loss = 3.75, acc = 0.52 (618.9 examples/sec; 0.207 sec/batch)
2016-11-28 17:42:28.050933: step 640, loss = 3.73, acc = 0.48 (631.8 examples/sec; 0.203 sec/batch)
2016-11-28 17:42:30.115680: step 650, loss = 3.66, acc = 0.51 (634.0 examples/sec; 0.202 sec/batch)
2016-11-28 17:42:32.245896: step 660, loss = 3.71, acc = 0.47 (619.4 examples/sec; 0.207 sec/batch)
2016-11-28 17:42:34.352055: step 670, loss = 3.77, acc = 0.44 (620.5 examples/sec; 0.206 sec/batch)
2016-11-28 17:42:36.467870: step 680, loss = 3.60, acc = 0.45 (588.1 examples/sec; 0.218 sec/batch)
2016-11-28 17:42:38.558697: step 690, loss = 3.76, acc = 0.50 (614.6 examples/sec; 0.208 sec/batch)
2016-11-28 17:42:40.651147: step 700, loss = 3.64, acc = 0.48 (606.7 examples/sec; 0.211 sec/batch)
At step 700 cross validation precision: 0.414
2016-11-28 17:42:43.484031: step 710, loss = 3.54, acc = 0.51 (635.6 examples/sec; 0.201 sec/batch)
2016-11-28 17:42:45.590499: step 720, loss = 3.49, acc = 0.56 (584.4 examples/sec; 0.219 sec/batch)
2016-11-28 17:42:47.688947: step 730, loss = 3.64, acc = 0.46 (631.6 examples/sec; 0.203 sec/batch)
2016-11-28 17:42:49.814524: step 740, loss = 3.45, acc = 0.49 (581.1 examples/sec; 0.220 sec/batch)
2016-11-28 17:42:51.928187: step 750, loss = 3.75, acc = 0.34 (581.0 examples/sec; 0.220 sec/batch)
2016-11-28 17:42:54.019557: step 760, loss = 3.41, acc = 0.55 (627.1 examples/sec; 0.204 sec/batch)
2016-11-28 17:42:56.094219: step 770, loss = 3.52, acc = 0.45 (639.7 examples/sec; 0.200 sec/batch)
2016-11-28 17:42:58.215014: step 780, loss = 3.47, acc = 0.48 (598.6 examples/sec; 0.214 sec/batch)
2016-11-28 17:43:00.339590: step 790, loss = 3.45, acc = 0.46 (562.8 examples/sec; 0.227 sec/batch)
2016-11-28 17:43:02.440311: step 800, loss = 3.41, acc = 0.55 (611.2 examples/sec; 0.209 sec/batch)
At step 800 cross validation precision: 0.453
2016-11-28 17:43:05.340859: step 810, loss = 3.58, acc = 0.47 (574.5 examples/sec; 0.223 sec/batch)
2016-11-28 17:43:07.443721: step 820, loss = 3.52, acc = 0.46 (570.3 examples/sec; 0.224 sec/batch)
2016-11-28 17:43:09.541588: step 830, loss = 3.32, acc = 0.50 (614.2 examples/sec; 0.208 sec/batch)
2016-11-28 17:43:11.636757: step 840, loss = 3.33, acc = 0.52 (623.3 examples/sec; 0.205 sec/batch)
2016-11-28 17:43:13.733454: step 850, loss = 3.44, acc = 0.41 (604.2 examples/sec; 0.212 sec/batch)
2016-11-28 17:43:15.831092: step 860, loss = 3.26, acc = 0.54 (612.9 examples/sec; 0.209 sec/batch)
2016-11-28 17:43:17.926095: step 870, loss = 3.34, acc = 0.41 (591.5 examples/sec; 0.216 sec/batch)
2016-11-28 17:43:20.025453: step 880, loss = 3.38, acc = 0.48 (610.8 examples/sec; 0.210 sec/batch)
2016-11-28 17:43:22.123156: step 890, loss = 3.32, acc = 0.47 (627.4 examples/sec; 0.204 sec/batch)
2016-11-28 17:43:24.219264: step 900, loss = 3.22, acc = 0.55 (573.6 examples/sec; 0.223 sec/batch)
At step 900 cross validation precision: 0.430
2016-11-28 17:43:27.020043: step 910, loss = 3.18, acc = 0.52 (636.7 examples/sec; 0.201 sec/batch)
2016-11-28 17:43:29.107146: step 920, loss = 3.11, acc = 0.63 (643.4 examples/sec; 0.199 sec/batch)
2016-11-28 17:43:31.198838: step 930, loss = 3.29, acc = 0.41 (603.7 examples/sec; 0.212 sec/batch)
2016-11-28 17:43:33.294276: step 940, loss = 3.01, acc = 0.59 (582.4 examples/sec; 0.220 sec/batch)
2016-11-28 17:43:35.396470: step 950, loss = 3.29, acc = 0.48 (591.5 examples/sec; 0.216 sec/batch)
2016-11-28 17:43:37.498934: step 960, loss = 3.06, acc = 0.59 (612.3 examples/sec; 0.209 sec/batch)
2016-11-28 17:43:39.588931: step 970, loss = 3.07, acc = 0.51 (570.2 examples/sec; 0.224 sec/batch)
2016-11-28 17:43:41.680082: step 980, loss = 2.97, acc = 0.55 (614.8 examples/sec; 0.208 sec/batch)
2016-11-28 17:43:43.776267: step 990, loss = 2.94, acc = 0.59 (617.7 examples/sec; 0.207 sec/batch)
2016-11-28 17:43:45.875894: step 1000, loss = 3.11, acc = 0.50 (608.1 examples/sec; 0.210 sec/batch)
At step 1000 cross validation precision: 0.414
2016-11-28 17:43:48.741262: step 1010, loss = 3.03, acc = 0.55 (615.0 examples/sec; 0.208 sec/batch)
2016-11-28 17:43:50.853284: step 1020, loss = 3.04, acc = 0.55 (629.4 examples/sec; 0.203 sec/batch)
2016-11-28 17:43:52.958852: step 1030, loss = 3.01, acc = 0.48 (600.2 examples/sec; 0.213 sec/batch)
2016-11-28 17:43:55.016842: step 1040, loss = 2.89, acc = 0.58 (601.8 examples/sec; 0.213 sec/batch)
2016-11-28 17:43:57.097748: step 1050, loss = 2.97, acc = 0.55 (609.5 examples/sec; 0.210 sec/batch)
2016-11-28 17:43:59.226990: step 1060, loss = 2.93, acc = 0.53 (607.2 examples/sec; 0.211 sec/batch)
2016-11-28 17:44:01.342296: step 1070, loss = 2.90, acc = 0.53 (610.9 examples/sec; 0.210 sec/batch)
2016-11-28 17:44:03.426949: step 1080, loss = 2.91, acc = 0.55 (600.2 examples/sec; 0.213 sec/batch)
2016-11-28 17:44:05.517340: step 1090, loss = 2.92, acc = 0.55 (663.8 examples/sec; 0.193 sec/batch)
2016-11-28 17:44:07.640423: step 1100, loss = 2.84, acc = 0.59 (609.0 examples/sec; 0.210 sec/batch)
At step 1100 cross validation precision: 0.516
2016-11-28 17:44:10.406620: step 1110, loss = 2.82, acc = 0.59 (617.4 examples/sec; 0.207 sec/batch)
2016-11-28 17:44:12.506576: step 1120, loss = 2.83, acc = 0.59 (618.5 examples/sec; 0.207 sec/batch)
2016-11-28 17:44:14.621366: step 1130, loss = 2.79, acc = 0.55 (603.3 examples/sec; 0.212 sec/batch)
2016-11-28 17:44:16.703886: step 1140, loss = 2.77, acc = 0.53 (561.9 examples/sec; 0.228 sec/batch)
2016-11-28 17:44:18.793091: step 1150, loss = 2.88, acc = 0.52 (645.0 examples/sec; 0.198 sec/batch)
2016-11-28 17:44:20.881648: step 1160, loss = 2.89, acc = 0.45 (602.8 examples/sec; 0.212 sec/batch)
2016-11-28 17:44:22.983938: step 1170, loss = 2.70, acc = 0.64 (602.4 examples/sec; 0.212 sec/batch)
2016-11-28 17:44:25.081145: step 1180, loss = 2.71, acc = 0.52 (632.5 examples/sec; 0.202 sec/batch)
2016-11-28 17:44:27.190242: step 1190, loss = 2.69, acc = 0.59 (592.6 examples/sec; 0.216 sec/batch)
2016-11-28 17:44:29.301694: step 1200, loss = 2.71, acc = 0.60 (611.7 examples/sec; 0.209 sec/batch)
At step 1200 cross validation precision: 0.445
2016-11-28 17:44:32.175593: step 1210, loss = 2.83, acc = 0.54 (610.5 examples/sec; 0.210 sec/batch)
2016-11-28 17:44:34.259789: step 1220, loss = 2.56, acc = 0.59 (604.4 examples/sec; 0.212 sec/batch)
2016-11-28 17:44:36.353836: step 1230, loss = 2.63, acc = 0.59 (568.1 examples/sec; 0.225 sec/batch)
2016-11-28 17:44:38.431615: step 1240, loss = 2.86, acc = 0.50 (647.0 examples/sec; 0.198 sec/batch)
2016-11-28 17:44:40.582787: step 1250, loss = 2.62, acc = 0.55 (583.5 examples/sec; 0.219 sec/batch)
2016-11-28 17:44:42.721228: step 1260, loss = 2.69, acc = 0.59 (578.3 examples/sec; 0.221 sec/batch)
2016-11-28 17:44:44.875565: step 1270, loss = 2.53, acc = 0.59 (575.7 examples/sec; 0.222 sec/batch)
2016-11-28 17:44:47.170725: step 1280, loss = 2.67, acc = 0.57 (547.1 examples/sec; 0.234 sec/batch)
2016-11-28 17:44:49.696098: step 1290, loss = 2.56, acc = 0.56 (491.5 examples/sec; 0.260 sec/batch)
2016-11-28 17:44:52.301864: step 1300, loss = 2.65, acc = 0.54 (464.9 examples/sec; 0.275 sec/batch)
At step 1300 cross validation precision: 0.461
2016-11-28 17:44:55.759709: step 1310, loss = 2.68, acc = 0.55 (474.1 examples/sec; 0.270 sec/batch)
2016-11-28 17:44:59.259681: step 1320, loss = 2.52, acc = 0.55 (390.4 examples/sec; 0.328 sec/batch)
2016-11-28 17:45:02.953719: step 1330, loss = 2.46, acc = 0.64 (328.7 examples/sec; 0.389 sec/batch)
2016-11-28 17:45:06.890431: step 1340, loss = 2.39, acc = 0.65 (323.5 examples/sec; 0.396 sec/batch)
2016-11-28 17:45:10.858827: step 1350, loss = 2.42, acc = 0.64 (323.8 examples/sec; 0.395 sec/batch)
2016-11-28 17:45:15.005703: step 1360, loss = 2.75, acc = 0.49 (284.7 examples/sec; 0.450 sec/batch)
2016-11-28 17:45:19.577498: step 1370, loss = 2.41, acc = 0.64 (289.7 examples/sec; 0.442 sec/batch)
2016-11-28 17:45:24.139108: step 1380, loss = 2.40, acc = 0.62 (283.8 examples/sec; 0.451 sec/batch)
2016-11-28 17:45:28.804861: step 1390, loss = 2.34, acc = 0.63 (273.3 examples/sec; 0.468 sec/batch)
2016-11-28 17:45:33.413530: step 1400, loss = 2.33, acc = 0.65 (282.5 examples/sec; 0.453 sec/batch)
At step 1400 cross validation precision: 0.523
2016-11-28 17:45:38.869110: step 1410, loss = 2.65, acc = 0.57 (291.1 examples/sec; 0.440 sec/batch)
2016-11-28 17:45:43.437968: step 1420, loss = 2.18, acc = 0.76 (289.8 examples/sec; 0.442 sec/batch)
2016-11-28 17:45:48.128463: step 1430, loss = 2.32, acc = 0.59 (284.9 examples/sec; 0.449 sec/batch)
2016-11-28 17:45:52.768205: step 1440, loss = 2.30, acc = 0.63 (284.2 examples/sec; 0.450 sec/batch)
2016-11-28 17:45:57.423535: step 1450, loss = 2.39, acc = 0.65 (289.3 examples/sec; 0.442 sec/batch)
2016-11-28 17:46:02.147851: step 1460, loss = 2.27, acc = 0.66 (284.5 examples/sec; 0.450 sec/batch)
2016-11-28 17:46:06.810724: step 1470, loss = 2.31, acc = 0.64 (275.5 examples/sec; 0.465 sec/batch)
2016-11-28 17:46:11.514511: step 1480, loss = 2.32, acc = 0.60 (244.7 examples/sec; 0.523 sec/batch)
2016-11-28 17:46:16.023685: step 1490, loss = 2.27, acc = 0.67 (283.7 examples/sec; 0.451 sec/batch)
2016-11-28 17:46:20.612271: step 1500, loss = 2.43, acc = 0.56 (286.1 examples/sec; 0.447 sec/batch)
At step 1500 cross validation precision: 0.367
2016-11-28 17:46:26.249875: step 1510, loss = 2.45, acc = 0.55 (287.2 examples/sec; 0.446 sec/batch)
2016-11-28 17:46:30.953825: step 1520, loss = 2.16, acc = 0.72 (284.8 examples/sec; 0.449 sec/batch)
2016-11-28 17:46:35.721758: step 1530, loss = 2.09, acc = 0.73 (286.8 examples/sec; 0.446 sec/batch)
2016-11-28 17:46:40.414243: step 1540, loss = 1.93, acc = 0.80 (253.2 examples/sec; 0.505 sec/batch)
2016-11-28 17:46:45.126222: step 1550, loss = 2.10, acc = 0.70 (282.8 examples/sec; 0.453 sec/batch)
2016-11-28 17:46:49.847835: step 1560, loss = 2.51, acc = 0.54 (288.5 examples/sec; 0.444 sec/batch)
2016-11-28 17:46:54.353276: step 1570, loss = 2.01, acc = 0.73 (290.4 examples/sec; 0.441 sec/batch)
2016-11-28 17:46:59.035232: step 1580, loss = 2.10, acc = 0.72 (285.8 examples/sec; 0.448 sec/batch)
2016-11-28 17:47:03.694562: step 1590, loss = 2.23, acc = 0.59 (286.3 examples/sec; 0.447 sec/batch)
2016-11-28 17:47:08.517776: step 1600, loss = 1.99, acc = 0.74 (248.0 examples/sec; 0.516 sec/batch)
At step 1600 cross validation precision: 0.445
2016-11-28 17:47:14.216954: step 1610, loss = 2.49, acc = 0.53 (285.9 examples/sec; 0.448 sec/batch)
2016-11-28 17:47:18.978679: step 1620, loss = 1.99, acc = 0.76 (250.2 examples/sec; 0.512 sec/batch)
2016-11-28 17:47:23.546727: step 1630, loss = 1.96, acc = 0.80 (281.1 examples/sec; 0.455 sec/batch)
2016-11-28 17:47:28.289571: step 1640, loss = 2.02, acc = 0.70 (213.5 examples/sec; 0.600 sec/batch)
2016-11-28 17:47:32.893897: step 1650, loss = 2.06, acc = 0.66 (288.6 examples/sec; 0.443 sec/batch)
2016-11-28 17:47:37.453743: step 1660, loss = 2.03, acc = 0.67 (294.3 examples/sec; 0.435 sec/batch)
2016-11-28 17:47:42.031232: step 1670, loss = 2.23, acc = 0.52 (285.5 examples/sec; 0.448 sec/batch)
2016-11-28 17:47:46.649085: step 1680, loss = 2.00, acc = 0.70 (282.7 examples/sec; 0.453 sec/batch)
2016-11-28 17:47:51.416949: step 1690, loss = 1.84, acc = 0.74 (212.1 examples/sec; 0.603 sec/batch)
2016-11-28 17:47:56.161252: step 1700, loss = 1.94, acc = 0.74 (246.1 examples/sec; 0.520 sec/batch)
At step 1700 cross validation precision: 0.406
2016-11-28 17:48:01.649491: step 1710, loss = 1.84, acc = 0.78 (296.0 examples/sec; 0.432 sec/batch)
2016-11-28 17:48:06.448893: step 1720, loss = 1.92, acc = 0.75 (287.2 examples/sec; 0.446 sec/batch)
2016-11-28 17:48:11.228870: step 1730, loss = 2.01, acc = 0.68 (207.8 examples/sec; 0.616 sec/batch)
2016-11-28 17:48:15.764771: step 1740, loss = 2.12, acc = 0.65 (281.3 examples/sec; 0.455 sec/batch)
2016-11-28 17:48:20.525454: step 1750, loss = 1.93, acc = 0.72 (227.3 examples/sec; 0.563 sec/batch)
2016-11-28 17:48:25.157994: step 1760, loss = 2.08, acc = 0.59 (287.2 examples/sec; 0.446 sec/batch)
2016-11-28 17:48:29.896051: step 1770, loss = 1.96, acc = 0.64 (222.8 examples/sec; 0.574 sec/batch)
2016-11-28 17:48:34.535440: step 1780, loss = 1.66, acc = 0.87 (288.3 examples/sec; 0.444 sec/batch)
2016-11-28 17:48:39.018134: step 1790, loss = 1.91, acc = 0.69 (281.4 examples/sec; 0.455 sec/batch)
2016-11-28 17:48:43.788845: step 1800, loss = 1.89, acc = 0.68 (282.7 examples/sec; 0.453 sec/batch)
At step 1800 cross validation precision: 0.398
2016-11-28 17:48:49.519015: step 1810, loss = 1.79, acc = 0.73 (272.5 examples/sec; 0.470 sec/batch)
2016-11-28 17:48:54.178535: step 1820, loss = 1.83, acc = 0.70 (256.5 examples/sec; 0.499 sec/batch)
2016-11-28 17:48:58.817259: step 1830, loss = 1.77, acc = 0.70 (289.1 examples/sec; 0.443 sec/batch)
2016-11-28 17:49:03.450135: step 1840, loss = 1.78, acc = 0.70 (296.1 examples/sec; 0.432 sec/batch)
2016-11-28 17:49:08.190449: step 1850, loss = 1.75, acc = 0.77 (289.4 examples/sec; 0.442 sec/batch)
2016-11-28 17:49:12.967677: step 1860, loss = 1.64, acc = 0.78 (290.1 examples/sec; 0.441 sec/batch)
2016-11-28 17:49:17.649680: step 1870, loss = 1.57, acc = 0.84 (285.5 examples/sec; 0.448 sec/batch)
2016-11-28 17:49:22.230690: step 1880, loss = 1.55, acc = 0.82 (296.2 examples/sec; 0.432 sec/batch)
2016-11-28 17:49:26.944204: step 1890, loss = 1.83, acc = 0.69 (288.9 examples/sec; 0.443 sec/batch)
2016-11-28 17:49:31.564879: step 1900, loss = 1.57, acc = 0.84 (286.6 examples/sec; 0.447 sec/batch)
At step 1900 cross validation precision: 0.430
2016-11-28 17:49:37.473930: step 1910, loss = 2.03, acc = 0.62 (208.1 examples/sec; 0.615 sec/batch)
2016-11-28 17:49:42.070243: step 1920, loss = 1.57, acc = 0.84 (283.9 examples/sec; 0.451 sec/batch)
2016-11-28 17:49:46.825431: step 1930, loss = 1.58, acc = 0.80 (284.5 examples/sec; 0.450 sec/batch)
2016-11-28 17:49:51.593974: step 1940, loss = 1.86, acc = 0.66 (293.1 examples/sec; 0.437 sec/batch)
2016-11-28 17:49:56.391660: step 1950, loss = 1.45, acc = 0.86 (284.2 examples/sec; 0.450 sec/batch)
2016-11-28 17:50:01.016060: step 1960, loss = 1.63, acc = 0.81 (293.1 examples/sec; 0.437 sec/batch)
2016-11-28 17:50:05.796106: step 1970, loss = 1.62, acc = 0.77 (285.0 examples/sec; 0.449 sec/batch)
2016-11-28 17:50:10.436972: step 1980, loss = 1.48, acc = 0.84 (291.1 examples/sec; 0.440 sec/batch)
2016-11-28 17:50:15.151430: step 1990, loss = 1.45, acc = 0.80 (287.9 examples/sec; 0.445 sec/batch)
2016-11-28 17:50:19.642841: step 2000, loss = 2.06, acc = 0.62 (292.3 examples/sec; 0.438 sec/batch)
At step 2000 cross validation precision: 0.414
2016-11-28 17:50:25.295396: step 2010, loss = 1.47, acc = 0.86 (203.7 examples/sec; 0.628 sec/batch)
2016-11-28 17:50:29.910800: step 2020, loss = 1.80, acc = 0.70 (289.9 examples/sec; 0.442 sec/batch)
2016-11-28 17:50:34.540038: step 2030, loss = 1.46, acc = 0.84 (287.7 examples/sec; 0.445 sec/batch)
2016-11-28 17:50:39.305893: step 2040, loss = 1.40, acc = 0.86 (286.0 examples/sec; 0.448 sec/batch)
2016-11-28 17:50:44.162848: step 2050, loss = 1.40, acc = 0.85 (248.5 examples/sec; 0.515 sec/batch)
2016-11-28 17:50:48.865906: step 2060, loss = 1.40, acc = 0.86 (286.5 examples/sec; 0.447 sec/batch)
2016-11-28 17:50:53.773333: step 2070, loss = 1.76, acc = 0.67 (249.4 examples/sec; 0.513 sec/batch)
2016-11-28 17:50:58.413860: step 2080, loss = 1.28, acc = 0.91 (291.0 examples/sec; 0.440 sec/batch)
2016-11-28 17:51:03.151961: step 2090, loss = 1.89, acc = 0.64 (204.9 examples/sec; 0.625 sec/batch)
2016-11-28 17:51:07.669090: step 2100, loss = 1.74, acc = 0.70 (284.7 examples/sec; 0.450 sec/batch)
At step 2100 cross validation precision: 0.344
2016-11-28 17:51:13.474718: step 2110, loss = 1.28, acc = 0.88 (248.5 examples/sec; 0.515 sec/batch)
2016-11-28 17:51:18.197822: step 2120, loss = 1.79, acc = 0.67 (289.3 examples/sec; 0.443 sec/batch)
2016-11-28 17:51:22.943841: step 2130, loss = 1.26, acc = 0.90 (283.1 examples/sec; 0.452 sec/batch)
2016-11-28 17:51:27.792208: step 2140, loss = 1.34, acc = 0.88 (293.2 examples/sec; 0.437 sec/batch)
2016-11-28 17:51:32.598256: step 2150, loss = 1.33, acc = 0.85 (287.7 examples/sec; 0.445 sec/batch)
2016-11-28 17:51:37.271916: step 2160, loss = 1.36, acc = 0.86 (267.2 examples/sec; 0.479 sec/batch)
2016-11-28 17:51:42.005958: step 2170, loss = 1.23, acc = 0.91 (290.6 examples/sec; 0.440 sec/batch)
2016-11-28 17:51:46.667228: step 2180, loss = 1.45, acc = 0.80 (294.9 examples/sec; 0.434 sec/batch)
2016-11-28 17:51:51.378951: step 2190, loss = 1.22, acc = 0.91 (283.8 examples/sec; 0.451 sec/batch)
2016-11-28 17:51:56.162614: step 2200, loss = 1.21, acc = 0.95 (282.4 examples/sec; 0.453 sec/batch)
At step 2200 cross validation precision: 0.352
2016-11-28 17:52:01.701119: step 2210, loss = 1.64, acc = 0.71 (294.6 examples/sec; 0.434 sec/batch)
2016-11-28 17:52:06.514505: step 2220, loss = 1.22, acc = 0.88 (290.0 examples/sec; 0.441 sec/batch)
2016-11-28 17:52:11.337216: step 2230, loss = 1.49, acc = 0.77 (278.3 examples/sec; 0.460 sec/batch)
2016-11-28 17:52:16.007034: step 2240, loss = 1.21, acc = 0.88 (287.8 examples/sec; 0.445 sec/batch)
2016-11-28 17:52:20.795121: step 2250, loss = 1.19, acc = 0.93 (277.0 examples/sec; 0.462 sec/batch)
2016-11-28 17:52:25.700752: step 2260, loss = 1.93, acc = 0.66 (271.1 examples/sec; 0.472 sec/batch)
2016-11-28 17:52:30.200879: step 2270, loss = 1.25, acc = 0.89 (291.8 examples/sec; 0.439 sec/batch)
2016-11-28 17:52:35.085757: step 2280, loss = 1.26, acc = 0.88 (292.4 examples/sec; 0.438 sec/batch)
2016-11-28 17:52:39.706106: step 2290, loss = 1.12, acc = 0.91 (284.2 examples/sec; 0.450 sec/batch)
2016-11-28 17:52:44.509456: step 2300, loss = 1.27, acc = 0.84 (286.3 examples/sec; 0.447 sec/batch)
At step 2300 cross validation precision: 0.453
2016-11-28 17:52:50.265290: step 2310, loss = 1.17, acc = 0.91 (245.4 examples/sec; 0.522 sec/batch)
2016-11-28 17:52:55.098438: step 2320, loss = 1.41, acc = 0.80 (286.6 examples/sec; 0.447 sec/batch)
2016-11-28 17:52:59.702134: step 2330, loss = 1.14, acc = 0.90 (298.7 examples/sec; 0.428 sec/batch)
2016-11-28 17:53:04.454808: step 2340, loss = 1.14, acc = 0.90 (286.0 examples/sec; 0.448 sec/batch)
2016-11-28 17:53:09.189216: step 2350, loss = 1.14, acc = 0.95 (288.1 examples/sec; 0.444 sec/batch)
2016-11-28 17:53:13.903650: step 2360, loss = 1.22, acc = 0.88 (287.9 examples/sec; 0.445 sec/batch)
2016-11-28 17:53:18.606292: step 2370, loss = 1.24, acc = 0.86 (289.0 examples/sec; 0.443 sec/batch)
2016-11-28 17:53:23.426028: step 2380, loss = 1.12, acc = 0.91 (233.9 examples/sec; 0.547 sec/batch)
2016-11-28 17:53:28.010727: step 2390, loss = 1.16, acc = 0.90 (285.4 examples/sec; 0.448 sec/batch)
2016-11-28 17:53:32.862651: step 2400, loss = 1.09, acc = 0.94 (222.2 examples/sec; 0.576 sec/batch)
At step 2400 cross validation precision: 0.398
2016-11-28 17:53:38.443662: step 2410, loss = 1.38, acc = 0.80 (227.8 examples/sec; 0.562 sec/batch)
2016-11-28 17:53:43.222275: step 2420, loss = 1.09, acc = 0.92 (286.5 examples/sec; 0.447 sec/batch)
2016-11-28 17:53:48.019833: step 2430, loss = 1.22, acc = 0.91 (284.7 examples/sec; 0.450 sec/batch)
2016-11-28 17:53:52.755671: step 2440, loss = 1.36, acc = 0.84 (295.4 examples/sec; 0.433 sec/batch)
2016-11-28 17:53:57.542800: step 2450, loss = 1.02, acc = 0.95 (196.6 examples/sec; 0.651 sec/batch)
2016-11-28 17:54:02.144812: step 2460, loss = 1.25, acc = 0.84 (288.6 examples/sec; 0.444 sec/batch)
2016-11-28 17:54:06.896886: step 2470, loss = 1.01, acc = 0.93 (288.0 examples/sec; 0.444 sec/batch)
2016-11-28 17:54:11.606149: step 2480, loss = 1.08, acc = 0.91 (288.8 examples/sec; 0.443 sec/batch)
2016-11-28 17:54:16.358453: step 2490, loss = 1.01, acc = 0.95 (220.5 examples/sec; 0.580 sec/batch)
2016-11-28 17:54:20.939798: step 2500, loss = 1.16, acc = 0.88 (287.0 examples/sec; 0.446 sec/batch)
At step 2500 cross validation precision: 0.352
2016-11-28 17:54:26.876028: step 2510, loss = 1.07, acc = 0.89 (257.6 examples/sec; 0.497 sec/batch)
2016-11-28 17:54:31.650242: step 2520, loss = 0.98, acc = 0.92 (291.4 examples/sec; 0.439 sec/batch)
2016-11-28 17:54:36.410231: step 2530, loss = 1.03, acc = 0.92 (227.7 examples/sec; 0.562 sec/batch)
2016-11-28 17:54:41.136665: step 2540, loss = 1.01, acc = 0.91 (299.9 examples/sec; 0.427 sec/batch)
2016-11-28 17:54:45.872248: step 2550, loss = 0.93, acc = 0.98 (206.3 examples/sec; 0.620 sec/batch)
2016-11-28 17:54:50.708276: step 2560, loss = 1.16, acc = 0.86 (204.1 examples/sec; 0.627 sec/batch)
2016-11-28 17:54:55.554823: step 2570, loss = 1.02, acc = 0.94 (252.3 examples/sec; 0.507 sec/batch)
2016-11-28 17:55:00.296486: step 2580, loss = 1.07, acc = 0.91 (293.9 examples/sec; 0.435 sec/batch)
2016-11-28 17:55:04.961243: step 2590, loss = 1.00, acc = 0.91 (298.1 examples/sec; 0.429 sec/batch)
2016-11-28 17:55:09.805760: step 2600, loss = 1.18, acc = 0.83 (293.7 examples/sec; 0.436 sec/batch)
At step 2600 cross validation precision: 0.461
2016-11-28 17:55:15.500931: step 2610, loss = 1.03, acc = 0.89 (231.3 examples/sec; 0.553 sec/batch)
2016-11-28 17:55:20.314814: step 2620, loss = 0.98, acc = 0.93 (285.2 examples/sec; 0.449 sec/batch)
2016-11-28 17:55:25.080857: step 2630, loss = 0.95, acc = 0.94 (224.4 examples/sec; 0.570 sec/batch)
2016-11-28 17:55:29.844196: step 2640, loss = 0.94, acc = 0.94 (285.7 examples/sec; 0.448 sec/batch)
2016-11-28 17:55:34.677380: step 2650, loss = 0.95, acc = 0.93 (252.1 examples/sec; 0.508 sec/batch)
2016-11-28 17:55:39.430767: step 2660, loss = 1.58, acc = 0.77 (208.0 examples/sec; 0.615 sec/batch)
2016-11-28 17:55:44.014450: step 2670, loss = 1.02, acc = 0.91 (227.2 examples/sec; 0.563 sec/batch)
2016-11-28 17:55:48.801601: step 2680, loss = 1.01, acc = 0.93 (290.0 examples/sec; 0.441 sec/batch)
2016-11-28 17:55:53.417765: step 2690, loss = 0.90, acc = 0.96 (254.1 examples/sec; 0.504 sec/batch)
2016-11-28 17:55:58.203262: step 2700, loss = 0.91, acc = 0.95 (207.8 examples/sec; 0.616 sec/batch)
At step 2700 cross validation precision: 0.406
2016-11-28 17:56:03.893439: step 2710, loss = 0.92, acc = 0.94 (244.8 examples/sec; 0.523 sec/batch)
2016-11-28 17:56:08.745260: step 2720, loss = 1.29, acc = 0.84 (252.1 examples/sec; 0.508 sec/batch)
2016-11-28 17:56:13.314915: step 2730, loss = 0.97, acc = 0.92 (292.1 examples/sec; 0.438 sec/batch)
2016-11-28 17:56:18.269552: step 2740, loss = 0.89, acc = 0.97 (239.7 examples/sec; 0.534 sec/batch)
2016-11-28 17:56:22.865764: step 2750, loss = 0.90, acc = 0.95 (287.8 examples/sec; 0.445 sec/batch)
2016-11-28 17:56:27.587008: step 2760, loss = 0.96, acc = 0.92 (291.8 examples/sec; 0.439 sec/batch)
2016-11-28 17:56:32.339847: step 2770, loss = 0.85, acc = 0.98 (283.9 examples/sec; 0.451 sec/batch)
2016-11-28 17:56:37.028612: step 2780, loss = 0.90, acc = 0.95 (299.9 examples/sec; 0.427 sec/batch)
2016-11-28 17:56:41.732174: step 2790, loss = 0.83, acc = 0.96 (232.9 examples/sec; 0.550 sec/batch)
2016-11-28 17:56:46.540973: step 2800, loss = 0.86, acc = 0.97 (239.2 examples/sec; 0.535 sec/batch)
At step 2800 cross validation precision: 0.500
2016-11-28 17:56:52.381676: step 2810, loss = 0.91, acc = 0.94 (252.4 examples/sec; 0.507 sec/batch)
2016-11-28 17:56:57.116418: step 2820, loss = 0.87, acc = 0.95 (286.5 examples/sec; 0.447 sec/batch)
2016-11-28 17:57:01.859033: step 2830, loss = 0.82, acc = 0.97 (209.9 examples/sec; 0.610 sec/batch)
2016-11-28 17:57:06.629138: step 2840, loss = 0.84, acc = 0.96 (293.2 examples/sec; 0.437 sec/batch)
2016-11-28 17:57:11.194066: step 2850, loss = 0.85, acc = 0.96 (286.9 examples/sec; 0.446 sec/batch)
2016-11-28 17:57:15.997471: step 2860, loss = 0.81, acc = 0.97 (291.7 examples/sec; 0.439 sec/batch)
2016-11-28 17:57:20.832132: step 2870, loss = 0.78, acc = 0.98 (207.3 examples/sec; 0.618 sec/batch)
2016-11-28 17:57:25.459574: step 2880, loss = 2.29, acc = 0.57 (287.7 examples/sec; 0.445 sec/batch)
2016-11-28 17:57:30.154071: step 2890, loss = 0.94, acc = 0.95 (222.6 examples/sec; 0.575 sec/batch)
2016-11-28 17:57:34.761423: step 2900, loss = 1.00, acc = 0.89 (294.8 examples/sec; 0.434 sec/batch)
At step 2900 cross validation precision: 0.406
2016-11-28 17:57:40.667675: step 2910, loss = 0.90, acc = 0.93 (288.3 examples/sec; 0.444 sec/batch)
2016-11-28 17:57:45.457678: step 2920, loss = 0.85, acc = 0.97 (284.9 examples/sec; 0.449 sec/batch)
2016-11-28 17:57:50.053545: step 2930, loss = 0.82, acc = 0.95 (287.8 examples/sec; 0.445 sec/batch)
2016-11-28 17:57:54.943134: step 2940, loss = 0.84, acc = 0.97 (231.9 examples/sec; 0.552 sec/batch)
2016-11-28 17:57:59.621088: step 2950, loss = 0.87, acc = 0.91 (288.6 examples/sec; 0.444 sec/batch)
2016-11-28 17:58:04.470596: step 2960, loss = 0.79, acc = 0.98 (294.1 examples/sec; 0.435 sec/batch)
2016-11-28 17:58:09.166193: step 2970, loss = 0.81, acc = 0.96 (283.2 examples/sec; 0.452 sec/batch)
2016-11-28 17:58:13.790569: step 2980, loss = 0.86, acc = 0.95 (287.2 examples/sec; 0.446 sec/batch)
2016-11-28 17:58:18.586458: step 2990, loss = 0.76, acc = 0.98 (233.0 examples/sec; 0.549 sec/batch)
2016-11-28 17:58:23.347968: step 3000, loss = 0.84, acc = 0.95 (291.4 examples/sec; 0.439 sec/batch)
At step 3000 cross validation precision: 0.391
2016-11-28 17:58:29.013326: step 3010, loss = 0.85, acc = 0.94 (210.5 examples/sec; 0.608 sec/batch)
2016-11-28 17:58:33.804848: step 3020, loss = 0.86, acc = 0.94 (207.0 examples/sec; 0.618 sec/batch)
2016-11-28 17:58:38.397805: step 3030, loss = 0.80, acc = 0.93 (287.8 examples/sec; 0.445 sec/batch)
2016-11-28 17:58:43.332125: step 3040, loss = 0.79, acc = 0.95 (292.9 examples/sec; 0.437 sec/batch)
2016-11-28 17:58:48.076225: step 3050, loss = 0.81, acc = 0.95 (207.4 examples/sec; 0.617 sec/batch)
2016-11-28 17:58:52.779089: step 3060, loss = 0.79, acc = 0.96 (282.9 examples/sec; 0.453 sec/batch)
2016-11-28 17:58:57.562951: step 3070, loss = 0.76, acc = 0.97 (293.5 examples/sec; 0.436 sec/batch)
2016-11-28 17:59:02.426628: step 3080, loss = 1.00, acc = 0.88 (287.4 examples/sec; 0.445 sec/batch)
2016-11-28 17:59:07.137233: step 3090, loss = 0.81, acc = 0.95 (285.7 examples/sec; 0.448 sec/batch)
2016-11-28 17:59:11.971596: step 3100, loss = 0.72, acc = 0.98 (292.5 examples/sec; 0.438 sec/batch)
At step 3100 cross validation precision: 0.398
2016-11-28 17:59:17.772236: step 3110, loss = 0.75, acc = 0.98 (261.6 examples/sec; 0.489 sec/batch)
2016-11-28 17:59:22.423031: step 3120, loss = 0.85, acc = 0.92 (286.4 examples/sec; 0.447 sec/batch)
2016-11-28 17:59:27.159258: step 3130, loss = 0.95, acc = 0.88 (287.0 examples/sec; 0.446 sec/batch)
2016-11-28 17:59:31.953800: step 3140, loss = 0.85, acc = 0.91 (296.2 examples/sec; 0.432 sec/batch)
2016-11-28 17:59:36.839644: step 3150, loss = 0.75, acc = 0.98 (195.0 examples/sec; 0.656 sec/batch)
2016-11-28 17:59:41.655064: step 3160, loss = 0.75, acc = 0.95 (198.7 examples/sec; 0.644 sec/batch)
2016-11-28 17:59:46.278402: step 3170, loss = 0.82, acc = 0.95 (278.3 examples/sec; 0.460 sec/batch)
2016-11-28 17:59:51.122632: step 3180, loss = 0.76, acc = 0.98 (286.2 examples/sec; 0.447 sec/batch)
2016-11-28 17:59:55.823186: step 3190, loss = 0.79, acc = 0.94 (289.4 examples/sec; 0.442 sec/batch)
2016-11-28 18:00:00.585702: step 3200, loss = 0.73, acc = 0.96 (290.6 examples/sec; 0.440 sec/batch)
At step 3200 cross validation precision: 0.352
2016-11-28 18:00:06.477239: step 3210, loss = 0.69, acc = 0.98 (210.0 examples/sec; 0.609 sec/batch)
2016-11-28 18:00:11.032129: step 3220, loss = 0.73, acc = 0.95 (282.8 examples/sec; 0.453 sec/batch)
2016-11-28 18:00:15.949704: step 3230, loss = 0.73, acc = 0.95 (198.7 examples/sec; 0.644 sec/batch)
2016-11-28 18:00:20.597445: step 3240, loss = 0.69, acc = 0.97 (294.0 examples/sec; 0.435 sec/batch)
2016-11-28 18:00:25.392339: step 3250, loss = 0.77, acc = 0.94 (283.5 examples/sec; 0.451 sec/batch)
2016-11-28 18:00:30.256426: step 3260, loss = 0.91, acc = 0.92 (252.5 examples/sec; 0.507 sec/batch)
2016-11-28 18:00:34.923708: step 3270, loss = 0.77, acc = 0.96 (248.2 examples/sec; 0.516 sec/batch)
2016-11-28 18:00:39.756313: step 3280, loss = 0.75, acc = 0.94 (284.2 examples/sec; 0.450 sec/batch)
2016-11-28 18:00:44.482471: step 3290, loss = 0.75, acc = 0.95 (287.2 examples/sec; 0.446 sec/batch)
2016-11-28 18:00:49.267644: step 3300, loss = 0.67, acc = 0.99 (288.3 examples/sec; 0.444 sec/batch)
At step 3300 cross validation precision: 0.383
2016-11-28 18:00:55.015113: step 3310, loss = 0.79, acc = 0.93 (292.2 examples/sec; 0.438 sec/batch)
2016-11-28 18:00:59.792785: step 3320, loss = 0.94, acc = 0.89 (286.2 examples/sec; 0.447 sec/batch)
2016-11-28 18:01:04.573819: step 3330, loss = 0.75, acc = 0.95 (286.8 examples/sec; 0.446 sec/batch)
2016-11-28 18:01:09.457810: step 3340, loss = 0.67, acc = 0.98 (208.4 examples/sec; 0.614 sec/batch)
2016-11-28 18:01:14.179380: step 3350, loss = 0.97, acc = 0.90 (291.3 examples/sec; 0.439 sec/batch)
2016-11-28 18:01:18.807775: step 3360, loss = 0.67, acc = 0.99 (290.9 examples/sec; 0.440 sec/batch)
2016-11-28 18:01:23.718094: step 3370, loss = 0.70, acc = 0.96 (239.8 examples/sec; 0.534 sec/batch)
2016-11-28 18:01:28.645310: step 3380, loss = 0.72, acc = 0.97 (209.2 examples/sec; 0.612 sec/batch)
2016-11-28 18:01:33.414513: step 3390, loss = 0.65, acc = 0.98 (247.8 examples/sec; 0.517 sec/batch)
2016-11-28 18:01:38.195185: step 3400, loss = 0.64, acc = 0.98 (287.1 examples/sec; 0.446 sec/batch)
At step 3400 cross validation precision: 0.391
2016-11-28 18:01:43.919912: step 3410, loss = 0.66, acc = 0.98 (277.5 examples/sec; 0.461 sec/batch)
2016-11-28 18:01:48.687053: step 3420, loss = 0.73, acc = 0.95 (286.2 examples/sec; 0.447 sec/batch)
2016-11-28 18:01:53.516486: step 3430, loss = 0.69, acc = 0.98 (288.1 examples/sec; 0.444 sec/batch)
2016-11-28 18:01:58.189410: step 3440, loss = 0.75, acc = 0.95 (301.2 examples/sec; 0.425 sec/batch)
2016-11-28 18:02:02.953735: step 3450, loss = 0.91, acc = 0.90 (295.5 examples/sec; 0.433 sec/batch)
2016-11-28 18:02:07.594732: step 3460, loss = 0.76, acc = 0.94 (288.3 examples/sec; 0.444 sec/batch)
2016-11-28 18:02:12.399473: step 3470, loss = 0.73, acc = 0.93 (297.6 examples/sec; 0.430 sec/batch)
2016-11-28 18:02:17.234337: step 3480, loss = 0.67, acc = 0.97 (273.2 examples/sec; 0.468 sec/batch)
2016-11-28 18:02:21.900685: step 3490, loss = 0.71, acc = 0.95 (228.2 examples/sec; 0.561 sec/batch)
2016-11-28 18:02:26.807668: step 3500, loss = 0.68, acc = 0.97 (281.9 examples/sec; 0.454 sec/batch)
At step 3500 cross validation precision: 0.398
2016-11-28 18:02:32.565273: step 3510, loss = 0.75, acc = 0.95 (295.4 examples/sec; 0.433 sec/batch)
2016-11-28 18:02:37.288926: step 3520, loss = 0.66, acc = 0.96 (224.6 examples/sec; 0.570 sec/batch)
2016-11-28 18:02:41.897295: step 3530, loss = 0.63, acc = 0.98 (282.7 examples/sec; 0.453 sec/batch)
2016-11-28 18:02:46.640066: step 3540, loss = 0.65, acc = 0.98 (285.6 examples/sec; 0.448 sec/batch)
2016-11-28 18:02:51.422917: step 3550, loss = 0.64, acc = 0.98 (285.3 examples/sec; 0.449 sec/batch)
2016-11-28 18:02:56.258208: step 3560, loss = 0.69, acc = 0.94 (284.0 examples/sec; 0.451 sec/batch)
2016-11-28 18:03:01.039669: step 3570, loss = 0.64, acc = 0.97 (285.9 examples/sec; 0.448 sec/batch)
2016-11-28 18:03:05.803288: step 3580, loss = 1.45, acc = 0.65 (286.8 examples/sec; 0.446 sec/batch)
2016-11-28 18:03:10.751770: step 3590, loss = 0.79, acc = 0.92 (199.9 examples/sec; 0.640 sec/batch)
2016-11-28 18:03:15.553201: step 3600, loss = 0.76, acc = 0.94 (235.1 examples/sec; 0.544 sec/batch)
At step 3600 cross validation precision: 0.398
2016-11-28 18:03:21.264054: step 3610, loss = 0.72, acc = 0.93 (284.2 examples/sec; 0.450 sec/batch)
2016-11-28 18:03:26.028820: step 3620, loss = 0.73, acc = 0.96 (248.1 examples/sec; 0.516 sec/batch)
2016-11-28 18:03:30.628306: step 3630, loss = 0.68, acc = 0.97 (287.1 examples/sec; 0.446 sec/batch)
2016-11-28 18:03:35.403586: step 3640, loss = 0.62, acc = 0.97 (286.2 examples/sec; 0.447 sec/batch)
2016-11-28 18:03:40.171140: step 3650, loss = 0.64, acc = 0.97 (285.1 examples/sec; 0.449 sec/batch)
2016-11-28 18:03:44.898296: step 3660, loss = 0.70, acc = 0.95 (229.3 examples/sec; 0.558 sec/batch)
2016-11-28 18:03:49.707739: step 3670, loss = 0.68, acc = 0.96 (226.1 examples/sec; 0.566 sec/batch)
2016-11-28 18:03:54.349107: step 3680, loss = 0.72, acc = 0.95 (294.5 examples/sec; 0.435 sec/batch)
2016-11-28 18:03:59.031863: step 3690, loss = 0.67, acc = 0.96 (298.9 examples/sec; 0.428 sec/batch)
2016-11-28 18:04:03.877723: step 3700, loss = 0.61, acc = 0.98 (296.7 examples/sec; 0.431 sec/batch)
At step 3700 cross validation precision: 0.391
2016-11-28 18:04:09.593630: step 3710, loss = 0.61, acc = 0.98 (298.3 examples/sec; 0.429 sec/batch)
2016-11-28 18:04:14.578312: step 3720, loss = 0.64, acc = 0.98 (272.2 examples/sec; 0.470 sec/batch)
2016-11-28 18:04:19.398795: step 3730, loss = 0.65, acc = 0.95 (287.5 examples/sec; 0.445 sec/batch)
2016-11-28 18:04:24.233914: step 3740, loss = 0.69, acc = 0.95 (284.7 examples/sec; 0.450 sec/batch)
2016-11-28 18:04:29.004289: step 3750, loss = 0.63, acc = 0.97 (301.6 examples/sec; 0.424 sec/batch)
2016-11-28 18:04:33.827476: step 3760, loss = 0.63, acc = 0.96 (289.4 examples/sec; 0.442 sec/batch)
2016-11-28 18:04:38.453114: step 3770, loss = 0.81, acc = 0.91 (287.6 examples/sec; 0.445 sec/batch)
2016-11-28 18:04:43.294745: step 3780, loss = 0.60, acc = 0.98 (205.2 examples/sec; 0.624 sec/batch)
2016-11-28 18:04:48.078852: step 3790, loss = 0.69, acc = 0.93 (253.0 examples/sec; 0.506 sec/batch)
2016-11-28 18:04:52.825007: step 3800, loss = 0.69, acc = 0.93 (234.0 examples/sec; 0.547 sec/batch)
At step 3800 cross validation precision: 0.406
2016-11-28 18:04:58.602757: step 3810, loss = 0.61, acc = 0.96 (265.8 examples/sec; 0.482 sec/batch)
2016-11-28 18:05:03.413552: step 3820, loss = 0.64, acc = 0.96 (284.9 examples/sec; 0.449 sec/batch)
2016-11-28 18:05:08.185461: step 3830, loss = 0.63, acc = 0.96 (239.3 examples/sec; 0.535 sec/batch)
2016-11-28 18:05:12.925768: step 3840, loss = 0.62, acc = 0.98 (278.5 examples/sec; 0.460 sec/batch)
2016-11-28 18:05:17.632239: step 3850, loss = 0.66, acc = 0.95 (230.6 examples/sec; 0.555 sec/batch)
2016-11-28 18:05:22.427420: step 3860, loss = 0.56, acc = 0.98 (286.9 examples/sec; 0.446 sec/batch)
2016-11-28 18:05:27.214444: step 3870, loss = 0.71, acc = 0.94 (289.5 examples/sec; 0.442 sec/batch)
2016-11-28 18:05:32.055146: step 3880, loss = 0.71, acc = 0.95 (284.4 examples/sec; 0.450 sec/batch)
2016-11-28 18:05:36.753072: step 3890, loss = 0.60, acc = 0.97 (288.2 examples/sec; 0.444 sec/batch)
2016-11-28 18:05:41.567018: step 3900, loss = 0.56, acc = 0.98 (208.7 examples/sec; 0.613 sec/batch)
At step 3900 cross validation precision: 0.445
2016-11-28 18:05:47.366115: step 3910, loss = 0.56, acc = 0.98 (293.7 examples/sec; 0.436 sec/batch)
2016-11-28 18:05:52.345173: step 3920, loss = 0.57, acc = 0.96 (216.8 examples/sec; 0.590 sec/batch)
2016-11-28 18:05:57.127561: step 3930, loss = 0.67, acc = 0.94 (233.8 examples/sec; 0.548 sec/batch)
2016-11-28 18:06:01.823146: step 3940, loss = 0.57, acc = 0.97 (284.3 examples/sec; 0.450 sec/batch)
2016-11-28 18:06:06.613086: step 3950, loss = 0.53, acc = 0.99 (292.9 examples/sec; 0.437 sec/batch)
2016-11-28 18:06:11.374014: step 3960, loss = 0.61, acc = 0.97 (284.0 examples/sec; 0.451 sec/batch)
2016-11-28 18:06:16.227419: step 3970, loss = 0.73, acc = 0.94 (210.9 examples/sec; 0.607 sec/batch)
2016-11-28 18:06:20.997821: step 3980, loss = 0.56, acc = 0.98 (219.0 examples/sec; 0.585 sec/batch)
2016-11-28 18:06:25.825087: step 3990, loss = 0.56, acc = 0.97 (209.8 examples/sec; 0.610 sec/batch)
2016-11-28 18:06:30.447333: step 4000, loss = 1.25, acc = 0.71 (265.4 examples/sec; 0.482 sec/batch)
At step 4000 cross validation precision: 0.414
2016-11-28 18:06:36.159331: step 4010, loss = 0.65, acc = 0.96 (282.6 examples/sec; 0.453 sec/batch)
2016-11-28 18:06:40.965551: step 4020, loss = 0.63, acc = 0.96 (288.4 examples/sec; 0.444 sec/batch)
2016-11-28 18:06:45.585914: step 4030, loss = 0.62, acc = 0.95 (286.1 examples/sec; 0.447 sec/batch)
2016-11-28 18:06:50.325852: step 4040, loss = 0.58, acc = 0.98 (291.2 examples/sec; 0.440 sec/batch)
2016-11-28 18:06:55.247420: step 4050, loss = 0.54, acc = 0.98 (288.7 examples/sec; 0.443 sec/batch)
2016-11-28 18:07:00.008074: step 4060, loss = 0.66, acc = 0.96 (296.9 examples/sec; 0.431 sec/batch)
2016-11-28 18:07:04.602757: step 4070, loss = 0.54, acc = 0.98 (291.3 examples/sec; 0.439 sec/batch)
2016-11-28 18:07:09.545055: step 4080, loss = 0.63, acc = 0.96 (233.3 examples/sec; 0.549 sec/batch)
2016-11-28 18:07:14.390468: step 4090, loss = 0.53, acc = 0.99 (290.8 examples/sec; 0.440 sec/batch)
2016-11-28 18:07:19.136423: step 4100, loss = 0.54, acc = 0.97 (290.9 examples/sec; 0.440 sec/batch)
At step 4100 cross validation precision: 0.398
2016-11-28 18:07:24.890203: step 4110, loss = 0.60, acc = 0.98 (218.0 examples/sec; 0.587 sec/batch)
2016-11-28 18:07:29.621538: step 4120, loss = 0.66, acc = 0.95 (286.7 examples/sec; 0.446 sec/batch)
2016-11-28 18:07:34.400951: step 4130, loss = 0.59, acc = 0.96 (300.1 examples/sec; 0.427 sec/batch)
2016-11-28 18:07:39.178543: step 4140, loss = 0.66, acc = 0.95 (286.2 examples/sec; 0.447 sec/batch)
2016-11-28 18:07:43.991454: step 4150, loss = 0.57, acc = 0.97 (288.6 examples/sec; 0.444 sec/batch)
2016-11-28 18:07:48.828797: step 4160, loss = 0.53, acc = 0.98 (291.2 examples/sec; 0.440 sec/batch)
2016-11-28 18:07:53.526196: step 4170, loss = 0.59, acc = 0.98 (288.1 examples/sec; 0.444 sec/batch)
2016-11-28 18:07:58.284442: step 4180, loss = 0.53, acc = 0.99 (290.4 examples/sec; 0.441 sec/batch)
2016-11-28 18:08:03.043634: step 4190, loss = 0.58, acc = 0.95 (291.4 examples/sec; 0.439 sec/batch)
2016-11-28 18:08:07.748471: step 4200, loss = 0.52, acc = 0.98 (285.2 examples/sec; 0.449 sec/batch)
At step 4200 cross validation precision: 0.430
2016-11-28 18:08:13.445283: step 4210, loss = 0.50, acc = 0.98 (281.2 examples/sec; 0.455 sec/batch)
2016-11-28 18:08:18.256659: step 4220, loss = 0.57, acc = 0.98 (291.8 examples/sec; 0.439 sec/batch)
2016-11-28 18:08:23.251491: step 4230, loss = 0.56, acc = 0.95 (285.5 examples/sec; 0.448 sec/batch)
2016-11-28 18:08:27.982088: step 4240, loss = 0.52, acc = 0.98 (282.0 examples/sec; 0.454 sec/batch)
2016-11-28 18:08:32.548708: step 4250, loss = 0.51, acc = 0.98 (289.2 examples/sec; 0.443 sec/batch)
2016-11-28 18:08:37.331502: step 4260, loss = 0.50, acc = 0.99 (286.9 examples/sec; 0.446 sec/batch)
2016-11-28 18:08:42.225954: step 4270, loss = 1.76, acc = 0.58 (227.4 examples/sec; 0.563 sec/batch)
2016-11-28 18:08:46.962928: step 4280, loss = 0.60, acc = 0.96 (224.7 examples/sec; 0.570 sec/batch)
2016-11-28 18:08:51.551979: step 4290, loss = 0.59, acc = 0.95 (291.8 examples/sec; 0.439 sec/batch)
2016-11-28 18:08:56.332793: step 4300, loss = 0.73, acc = 0.91 (286.3 examples/sec; 0.447 sec/batch)
At step 4300 cross validation precision: 0.430
2016-11-28 18:09:02.199456: step 4310, loss = 0.53, acc = 0.98 (285.4 examples/sec; 0.448 sec/batch)
2016-11-28 18:09:06.938636: step 4320, loss = 0.55, acc = 0.96 (288.2 examples/sec; 0.444 sec/batch)
2016-11-28 18:09:11.769360: step 4330, loss = 0.54, acc = 0.96 (282.6 examples/sec; 0.453 sec/batch)
2016-11-28 18:09:16.689062: step 4340, loss = 0.56, acc = 0.98 (290.3 examples/sec; 0.441 sec/batch)
2016-11-28 18:09:21.289798: step 4350, loss = 0.55, acc = 0.95 (288.2 examples/sec; 0.444 sec/batch)
2016-11-28 18:09:26.066144: step 4360, loss = 0.51, acc = 0.98 (289.0 examples/sec; 0.443 sec/batch)
2016-11-28 18:09:30.963611: step 4370, loss = 0.56, acc = 0.95 (284.9 examples/sec; 0.449 sec/batch)
2016-11-28 18:09:35.724392: step 4380, loss = 0.51, acc = 0.98 (285.6 examples/sec; 0.448 sec/batch)
2016-11-28 18:09:40.572028: step 4390, loss = 0.56, acc = 0.95 (285.3 examples/sec; 0.449 sec/batch)
2016-11-28 18:09:45.476719: step 4400, loss = 0.51, acc = 0.98 (286.7 examples/sec; 0.446 sec/batch)
At step 4400 cross validation precision: 0.398
2016-11-28 18:09:51.126360: step 4410, loss = 0.52, acc = 0.98 (293.4 examples/sec; 0.436 sec/batch)
2016-11-28 18:09:56.104486: step 4420, loss = 0.51, acc = 0.98 (207.4 examples/sec; 0.617 sec/batch)
2016-11-28 18:10:00.904955: step 4430, loss = 0.59, acc = 0.95 (221.6 examples/sec; 0.578 sec/batch)
2016-11-28 18:10:05.665078: step 4440, loss = 0.61, acc = 0.95 (295.4 examples/sec; 0.433 sec/batch)
2016-11-28 18:10:10.498373: step 4450, loss = 0.54, acc = 0.96 (198.3 examples/sec; 0.645 sec/batch)
2016-11-28 18:10:15.168161: step 4460, loss = 0.49, acc = 0.98 (267.8 examples/sec; 0.478 sec/batch)
2016-11-28 18:10:19.844031: step 4470, loss = 0.47, acc = 0.98 (287.6 examples/sec; 0.445 sec/batch)
2016-11-28 18:10:24.679777: step 4480, loss = 0.51, acc = 0.97 (229.4 examples/sec; 0.558 sec/batch)
2016-11-28 18:10:29.302950: step 4490, loss = 0.46, acc = 0.99 (291.8 examples/sec; 0.439 sec/batch)
2016-11-28 18:10:34.104215: step 4500, loss = 0.46, acc = 0.99 (286.5 examples/sec; 0.447 sec/batch)
At step 4500 cross validation precision: 0.406
2016-11-28 18:10:39.819972: step 4510, loss = 0.48, acc = 0.98 (287.2 examples/sec; 0.446 sec/batch)
2016-11-28 18:10:44.559474: step 4520, loss = 0.46, acc = 0.98 (284.4 examples/sec; 0.450 sec/batch)
2016-11-28 18:10:49.427488: step 4530, loss = 0.54, acc = 0.97 (295.1 examples/sec; 0.434 sec/batch)
2016-11-28 18:10:54.220100: step 4540, loss = 0.49, acc = 0.98 (227.6 examples/sec; 0.562 sec/batch)
2016-11-28 18:10:59.052121: step 4550, loss = 0.48, acc = 0.98 (248.4 examples/sec; 0.515 sec/batch)
2016-11-28 18:11:03.750573: step 4560, loss = 0.52, acc = 0.97 (293.1 examples/sec; 0.437 sec/batch)
2016-11-28 18:11:08.620292: step 4570, loss = 0.63, acc = 0.91 (240.5 examples/sec; 0.532 sec/batch)
2016-11-28 18:11:13.343721: step 4580, loss = 0.54, acc = 0.95 (285.7 examples/sec; 0.448 sec/batch)
2016-11-28 18:11:17.981500: step 4590, loss = 0.48, acc = 1.00 (287.6 examples/sec; 0.445 sec/batch)
2016-11-28 18:11:22.850649: step 4600, loss = 0.55, acc = 0.95 (232.1 examples/sec; 0.552 sec/batch)
At step 4600 cross validation precision: 0.344
2016-11-28 18:11:28.609644: step 4610, loss = 0.48, acc = 0.98 (262.1 examples/sec; 0.488 sec/batch)
2016-11-28 18:11:33.253590: step 4620, loss = 0.50, acc = 0.96 (285.2 examples/sec; 0.449 sec/batch)
2016-11-28 18:11:38.109986: step 4630, loss = 0.62, acc = 0.94 (298.4 examples/sec; 0.429 sec/batch)
2016-11-28 18:11:43.003746: step 4640, loss = 0.47, acc = 0.98 (242.8 examples/sec; 0.527 sec/batch)
2016-11-28 18:11:47.645330: step 4650, loss = 0.44, acc = 1.00 (287.0 examples/sec; 0.446 sec/batch)
2016-11-28 18:11:52.606674: step 4660, loss = 0.45, acc = 1.00 (285.8 examples/sec; 0.448 sec/batch)
2016-11-28 18:11:57.300864: step 4670, loss = 0.45, acc = 0.99 (286.3 examples/sec; 0.447 sec/batch)
2016-11-28 18:12:02.307958: step 4680, loss = 0.49, acc = 0.98 (244.1 examples/sec; 0.524 sec/batch)
2016-11-28 18:12:07.027388: step 4690, loss = 0.44, acc = 0.98 (249.9 examples/sec; 0.512 sec/batch)
2016-11-28 18:12:11.852748: step 4700, loss = 0.46, acc = 0.98 (287.6 examples/sec; 0.445 sec/batch)
At step 4700 cross validation precision: 0.438
2016-11-28 18:12:17.671713: step 4710, loss = 0.42, acc = 1.00 (275.4 examples/sec; 0.465 sec/batch)
2016-11-28 18:12:22.487957: step 4720, loss = 1.57, acc = 0.74 (294.8 examples/sec; 0.434 sec/batch)
2016-11-28 18:12:27.301326: step 4730, loss = 0.59, acc = 0.95 (288.0 examples/sec; 0.444 sec/batch)
2016-11-28 18:12:32.080297: step 4740, loss = 0.65, acc = 0.92 (269.1 examples/sec; 0.476 sec/batch)
2016-11-28 18:12:37.044772: step 4750, loss = 0.51, acc = 0.96 (296.5 examples/sec; 0.432 sec/batch)
2016-11-28 18:12:41.747424: step 4760, loss = 0.52, acc = 0.97 (281.6 examples/sec; 0.455 sec/batch)
2016-11-28 18:12:46.739263: step 4770, loss = 0.56, acc = 0.94 (244.2 examples/sec; 0.524 sec/batch)
2016-11-28 18:12:51.429029: step 4780, loss = 0.50, acc = 0.97 (206.0 examples/sec; 0.621 sec/batch)
2016-11-28 18:12:56.062901: step 4790, loss = 0.45, acc = 1.00 (292.2 examples/sec; 0.438 sec/batch)
2016-11-28 18:13:00.919726: step 4800, loss = 0.44, acc = 1.00 (287.4 examples/sec; 0.445 sec/batch)
At step 4800 cross validation precision: 0.383
2016-11-28 18:13:06.727008: step 4810, loss = 0.48, acc = 0.99 (286.9 examples/sec; 0.446 sec/batch)
2016-11-28 18:13:11.582959: step 4820, loss = 0.53, acc = 0.98 (284.1 examples/sec; 0.451 sec/batch)
2016-11-28 18:13:16.406629: step 4830, loss = 0.43, acc = 1.00 (285.0 examples/sec; 0.449 sec/batch)
2016-11-28 18:13:21.211216: step 4840, loss = 0.50, acc = 0.98 (226.6 examples/sec; 0.565 sec/batch)
2016-11-28 18:13:25.850354: step 4850, loss = 0.51, acc = 0.95 (287.6 examples/sec; 0.445 sec/batch)
2016-11-28 18:13:30.781466: step 4860, loss = 0.46, acc = 0.98 (227.1 examples/sec; 0.564 sec/batch)
2016-11-28 18:13:35.559131: step 4870, loss = 0.49, acc = 0.95 (253.6 examples/sec; 0.505 sec/batch)
2016-11-28 18:13:40.243319: step 4880, loss = 0.48, acc = 0.98 (290.6 examples/sec; 0.440 sec/batch)
2016-11-28 18:13:45.021649: step 4890, loss = 0.44, acc = 0.99 (209.3 examples/sec; 0.611 sec/batch)
2016-11-28 18:13:49.612392: step 4900, loss = 0.51, acc = 0.95 (292.1 examples/sec; 0.438 sec/batch)
At step 4900 cross validation precision: 0.398
2016-11-28 18:13:55.273338: step 4910, loss = 0.44, acc = 0.98 (231.1 examples/sec; 0.554 sec/batch)
2016-11-28 18:14:00.017828: step 4920, loss = 0.45, acc = 0.98 (291.6 examples/sec; 0.439 sec/batch)
2016-11-28 18:14:04.743227: step 4930, loss = 0.41, acc = 1.00 (283.9 examples/sec; 0.451 sec/batch)
2016-11-28 18:14:09.506689: step 4940, loss = 0.51, acc = 0.96 (279.7 examples/sec; 0.458 sec/batch)
2016-11-28 18:14:14.246937: step 4950, loss = 0.48, acc = 0.97 (287.0 examples/sec; 0.446 sec/batch)
2016-11-28 18:14:19.053157: step 4960, loss = 0.46, acc = 0.98 (203.8 examples/sec; 0.628 sec/batch)
2016-11-28 18:14:23.786281: step 4970, loss = 0.48, acc = 0.95 (284.9 examples/sec; 0.449 sec/batch)
2016-11-28 18:14:28.695036: step 4980, loss = 0.45, acc = 0.98 (200.7 examples/sec; 0.638 sec/batch)
2016-11-28 18:14:33.621650: step 4990, loss = 0.50, acc = 0.95 (251.8 examples/sec; 0.508 sec/batch)
2016-11-28 18:14:38.402065: step 5000, loss = 0.55, acc = 0.97 (239.0 examples/sec; 0.536 sec/batch)
At step 5000 cross validation precision: 0.453
2016-11-28 18:14:44.169013: step 5010, loss = 0.48, acc = 0.96 (237.2 examples/sec; 0.540 sec/batch)
2016-11-28 18:14:48.754015: step 5020, loss = 0.50, acc = 0.95 (293.4 examples/sec; 0.436 sec/batch)
2016-11-28 18:14:53.642699: step 5030, loss = 0.42, acc = 0.98 (226.5 examples/sec; 0.565 sec/batch)
2016-11-28 18:14:58.430554: step 5040, loss = 0.48, acc = 0.97 (230.4 examples/sec; 0.556 sec/batch)
2016-11-28 18:15:03.288162: step 5050, loss = 0.42, acc = 0.99 (207.8 examples/sec; 0.616 sec/batch)
2016-11-28 18:15:08.015678: step 5060, loss = 0.45, acc = 0.98 (292.2 examples/sec; 0.438 sec/batch)
2016-11-28 18:15:12.593779: step 5070, loss = 0.41, acc = 0.99 (278.0 examples/sec; 0.460 sec/batch)
2016-11-28 18:15:18.329901: step 5080, loss = 0.45, acc = 0.98 (183.7 examples/sec; 0.697 sec/batch)
2016-11-28 18:15:24.835026: step 5090, loss = 0.46, acc = 0.96 (195.3 examples/sec; 0.656 sec/batch)
2016-11-28 18:15:29.821574: step 5100, loss = 0.42, acc = 0.98 (269.0 examples/sec; 0.476 sec/batch)
At step 5100 cross validation precision: 0.414
2016-11-28 18:15:35.530493: step 5110, loss = 0.44, acc = 0.98 (218.5 examples/sec; 0.586 sec/batch)
2016-11-28 18:15:40.085010: step 5120, loss = 0.41, acc = 0.99 (286.9 examples/sec; 0.446 sec/batch)
2016-11-28 18:15:44.839519: step 5130, loss = 0.40, acc = 1.00 (228.4 examples/sec; 0.561 sec/batch)
2016-11-28 18:15:49.513081: step 5140, loss = 0.50, acc = 0.96 (293.7 examples/sec; 0.436 sec/batch)
2016-11-28 18:15:54.310869: step 5150, loss = 0.41, acc = 0.99 (288.0 examples/sec; 0.444 sec/batch)
2016-11-28 18:15:59.097447: step 5160, loss = 1.56, acc = 0.70 (284.8 examples/sec; 0.450 sec/batch)
2016-11-28 18:16:03.849889: step 5170, loss = 0.52, acc = 0.96 (286.0 examples/sec; 0.448 sec/batch)
2016-11-28 18:16:08.535307: step 5180, loss = 0.48, acc = 0.96 (232.1 examples/sec; 0.551 sec/batch)
2016-11-28 18:16:13.358600: step 5190, loss = 0.47, acc = 0.98 (282.3 examples/sec; 0.453 sec/batch)
2016-11-28 18:16:18.056900: step 5200, loss = 0.47, acc = 0.98 (261.5 examples/sec; 0.490 sec/batch)
At step 5200 cross validation precision: 0.383
2016-11-28 18:16:24.042346: step 5210, loss = 0.42, acc = 0.98 (285.9 examples/sec; 0.448 sec/batch)
2016-11-28 18:16:29.010682: step 5220, loss = 0.42, acc = 0.98 (212.8 examples/sec; 0.601 sec/batch)
2016-11-28 18:16:33.808462: step 5230, loss = 0.47, acc = 0.98 (286.5 examples/sec; 0.447 sec/batch)
2016-11-28 18:16:38.415554: step 5240, loss = 0.42, acc = 0.99 (280.9 examples/sec; 0.456 sec/batch)
2016-11-28 18:16:43.219256: step 5250, loss = 0.40, acc = 0.98 (289.9 examples/sec; 0.442 sec/batch)
2016-11-28 18:16:48.027869: step 5260, loss = 0.41, acc = 0.98 (286.7 examples/sec; 0.447 sec/batch)
2016-11-28 18:16:52.735214: step 5270, loss = 0.41, acc = 0.98 (256.3 examples/sec; 0.500 sec/batch)
2016-11-28 18:16:57.532793: step 5280, loss = 0.45, acc = 0.98 (242.6 examples/sec; 0.528 sec/batch)
2016-11-28 18:17:02.283517: step 5290, loss = 0.40, acc = 0.98 (287.6 examples/sec; 0.445 sec/batch)
2016-11-28 18:17:07.076928: step 5300, loss = 0.41, acc = 0.98 (283.1 examples/sec; 0.452 sec/batch)
At step 5300 cross validation precision: 0.461
2016-11-28 18:17:12.994806: step 5310, loss = 0.47, acc = 0.96 (217.3 examples/sec; 0.589 sec/batch)
2016-11-28 18:17:17.593897: step 5320, loss = 0.51, acc = 0.95 (293.0 examples/sec; 0.437 sec/batch)
2016-11-28 18:17:22.412033: step 5330, loss = 0.52, acc = 0.95 (285.7 examples/sec; 0.448 sec/batch)
2016-11-28 18:17:27.149381: step 5340, loss = 0.39, acc = 0.99 (289.3 examples/sec; 0.442 sec/batch)
2016-11-28 18:17:31.871112: step 5350, loss = 0.43, acc = 0.97 (290.8 examples/sec; 0.440 sec/batch)
2016-11-28 18:17:36.711317: step 5360, loss = 0.41, acc = 0.98 (285.8 examples/sec; 0.448 sec/batch)
2016-11-28 18:17:41.425840: step 5370, loss = 0.39, acc = 0.98 (286.4 examples/sec; 0.447 sec/batch)
2016-11-28 18:17:46.269202: step 5380, loss = 0.45, acc = 0.95 (299.6 examples/sec; 0.427 sec/batch)
2016-11-28 18:17:51.029853: step 5390, loss = 0.62, acc = 0.90 (251.0 examples/sec; 0.510 sec/batch)
2016-11-28 18:17:55.813055: step 5400, loss = 0.52, acc = 0.97 (284.8 examples/sec; 0.449 sec/batch)
At step 5400 cross validation precision: 0.352
2016-11-28 18:18:01.778868: step 5410, loss = 0.39, acc = 0.99 (246.9 examples/sec; 0.518 sec/batch)
2016-11-28 18:18:06.484201: step 5420, loss = 0.42, acc = 0.98 (286.4 examples/sec; 0.447 sec/batch)
2016-11-28 18:18:11.314013: step 5430, loss = 0.45, acc = 0.97 (288.4 examples/sec; 0.444 sec/batch)
2016-11-28 18:18:16.006706: step 5440, loss = 0.38, acc = 1.00 (296.6 examples/sec; 0.432 sec/batch)
2016-11-28 18:18:20.867655: step 5450, loss = 0.40, acc = 0.98 (254.0 examples/sec; 0.504 sec/batch)
2016-11-28 18:18:25.533740: step 5460, loss = 0.46, acc = 0.96 (290.4 examples/sec; 0.441 sec/batch)
2016-11-28 18:18:30.367613: step 5470, loss = 0.43, acc = 0.98 (287.2 examples/sec; 0.446 sec/batch)
2016-11-28 18:18:35.015413: step 5480, loss = 0.42, acc = 0.98 (269.9 examples/sec; 0.474 sec/batch)
2016-11-28 18:18:39.812255: step 5490, loss = 0.40, acc = 0.98 (215.1 examples/sec; 0.595 sec/batch)
2016-11-28 18:18:44.477849: step 5500, loss = 0.42, acc = 0.98 (291.1 examples/sec; 0.440 sec/batch)
At step 5500 cross validation precision: 0.422
2016-11-28 18:18:50.438816: step 5510, loss = 0.53, acc = 0.97 (264.9 examples/sec; 0.483 sec/batch)
2016-11-28 18:18:55.237718: step 5520, loss = 0.43, acc = 0.98 (284.8 examples/sec; 0.449 sec/batch)
2016-11-28 18:19:00.015169: step 5530, loss = 0.39, acc = 0.99 (284.9 examples/sec; 0.449 sec/batch)
2016-11-28 18:19:04.597214: step 5540, loss = 0.41, acc = 0.98 (289.6 examples/sec; 0.442 sec/batch)
2016-11-28 18:19:09.551847: step 5550, loss = 0.57, acc = 0.94 (224.3 examples/sec; 0.571 sec/batch)
2016-11-28 18:19:14.153443: step 5560, loss = 0.56, acc = 0.94 (283.9 examples/sec; 0.451 sec/batch)
2016-11-28 18:19:18.972997: step 5570, loss = 0.45, acc = 0.95 (284.7 examples/sec; 0.450 sec/batch)
2016-11-28 18:19:23.772784: step 5580, loss = 0.45, acc = 0.96 (284.8 examples/sec; 0.449 sec/batch)
2016-11-28 18:19:28.445705: step 5590, loss = 0.42, acc = 0.98 (229.4 examples/sec; 0.558 sec/batch)
2016-11-28 18:19:33.171087: step 5600, loss = 0.50, acc = 0.95 (228.4 examples/sec; 0.560 sec/batch)
At step 5600 cross validation precision: 0.391
2016-11-28 18:19:38.815445: step 5610, loss = 0.42, acc = 0.98 (285.4 examples/sec; 0.448 sec/batch)
2016-11-28 18:19:43.614255: step 5620, loss = 0.46, acc = 0.96 (290.9 examples/sec; 0.440 sec/batch)
2016-11-28 18:19:48.299990: step 5630, loss = 0.41, acc = 0.98 (283.1 examples/sec; 0.452 sec/batch)
2016-11-28 18:19:53.029780: step 5640, loss = 0.39, acc = 0.98 (252.7 examples/sec; 0.506 sec/batch)
2016-11-28 18:19:57.764399: step 5650, loss = 0.40, acc = 0.98 (287.8 examples/sec; 0.445 sec/batch)
2016-11-28 18:20:02.601508: step 5660, loss = 0.44, acc = 0.98 (286.3 examples/sec; 0.447 sec/batch)
2016-11-28 18:20:07.488423: step 5670, loss = 0.38, acc = 1.00 (279.5 examples/sec; 0.458 sec/batch)
2016-11-28 18:20:12.227776: step 5680, loss = 0.40, acc = 0.98 (286.0 examples/sec; 0.447 sec/batch)
2016-11-28 18:20:16.992123: step 5690, loss = 0.42, acc = 0.98 (227.4 examples/sec; 0.563 sec/batch)
2016-11-28 18:20:21.676285: step 5700, loss = 0.37, acc = 1.00 (286.1 examples/sec; 0.447 sec/batch)
At step 5700 cross validation precision: 0.438
2016-11-28 18:20:27.525675: step 5710, loss = 0.45, acc = 0.97 (290.6 examples/sec; 0.441 sec/batch)
2016-11-28 18:20:32.366720: step 5720, loss = 0.38, acc = 0.99 (297.3 examples/sec; 0.431 sec/batch)
2016-11-28 18:20:37.183434: step 5730, loss = 0.45, acc = 0.95 (202.5 examples/sec; 0.632 sec/batch)
2016-11-28 18:20:41.875684: step 5740, loss = 0.41, acc = 0.98 (229.3 examples/sec; 0.558 sec/batch)
2016-11-28 18:20:46.627975: step 5750, loss = 0.49, acc = 0.95 (249.3 examples/sec; 0.513 sec/batch)
2016-11-28 18:20:51.364791: step 5760, loss = 0.39, acc = 0.99 (284.1 examples/sec; 0.451 sec/batch)
2016-11-28 18:20:56.195231: step 5770, loss = 0.41, acc = 0.98 (235.5 examples/sec; 0.544 sec/batch)
2016-11-28 18:21:01.039908: step 5780, loss = 0.43, acc = 0.96 (216.3 examples/sec; 0.592 sec/batch)
2016-11-28 18:21:05.779961: step 5790, loss = 0.59, acc = 0.91 (288.6 examples/sec; 0.443 sec/batch)
2016-11-28 18:21:10.658418: step 5800, loss = 0.41, acc = 0.99 (287.9 examples/sec; 0.445 sec/batch)
At step 5800 cross validation precision: 0.414
2016-11-28 18:21:16.235054: step 5810, loss = 0.41, acc = 0.98 (264.0 examples/sec; 0.485 sec/batch)
2016-11-28 18:21:21.185699: step 5820, loss = 0.44, acc = 0.96 (215.1 examples/sec; 0.595 sec/batch)
2016-11-28 18:21:25.854486: step 5830, loss = 0.40, acc = 0.98 (286.8 examples/sec; 0.446 sec/batch)
2016-11-28 18:21:30.682018: step 5840, loss = 0.43, acc = 0.97 (266.4 examples/sec; 0.480 sec/batch)
2016-11-28 18:21:35.430664: step 5850, loss = 0.46, acc = 0.97 (286.4 examples/sec; 0.447 sec/batch)
2016-11-28 18:21:40.352476: step 5860, loss = 0.38, acc = 1.00 (281.8 examples/sec; 0.454 sec/batch)
2016-11-28 18:21:45.093236: step 5870, loss = 0.36, acc = 1.00 (287.3 examples/sec; 0.446 sec/batch)
2016-11-28 18:21:49.966350: step 5880, loss = 0.39, acc = 0.98 (285.5 examples/sec; 0.448 sec/batch)
2016-11-28 18:21:54.625050: step 5890, loss = 0.47, acc = 0.96 (291.5 examples/sec; 0.439 sec/batch)
2016-11-28 18:21:59.387768: step 5900, loss = 0.43, acc = 0.97 (284.4 examples/sec; 0.450 sec/batch)
At step 5900 cross validation precision: 0.398
2016-11-28 18:22:05.190807: step 5910, loss = 0.39, acc = 0.98 (289.3 examples/sec; 0.442 sec/batch)
2016-11-28 18:22:10.063641: step 5920, loss = 0.37, acc = 1.00 (299.9 examples/sec; 0.427 sec/batch)
2016-11-28 18:22:14.837078: step 5930, loss = 0.46, acc = 0.97 (258.0 examples/sec; 0.496 sec/batch)
2016-11-28 18:22:19.519112: step 5940, loss = 0.55, acc = 0.91 (241.9 examples/sec; 0.529 sec/batch)
2016-11-28 18:22:24.217044: step 5950, loss = 0.42, acc = 0.98 (286.5 examples/sec; 0.447 sec/batch)
2016-11-28 18:22:29.087512: step 5960, loss = 0.40, acc = 0.98 (291.6 examples/sec; 0.439 sec/batch)
2016-11-28 18:22:33.831636: step 5970, loss = 0.40, acc = 0.98 (289.1 examples/sec; 0.443 sec/batch)
2016-11-28 18:22:38.783335: step 5980, loss = 0.38, acc = 0.98 (285.4 examples/sec; 0.449 sec/batch)
2016-11-28 18:22:43.605184: step 5990, loss = 0.44, acc = 0.96 (273.8 examples/sec; 0.468 sec/batch)
2016-11-28 18:22:48.300317: step 6000, loss = 0.38, acc = 0.99 (222.2 examples/sec; 0.576 sec/batch)
At step 6000 cross validation precision: 0.453
2016-11-28 18:22:54.024252: step 6010, loss = 0.42, acc = 0.98 (290.1 examples/sec; 0.441 sec/batch)
2016-11-28 18:22:58.847509: step 6020, loss = 0.45, acc = 0.98 (234.5 examples/sec; 0.546 sec/batch)
2016-11-28 18:23:03.504933: step 6030, loss = 0.45, acc = 0.96 (290.9 examples/sec; 0.440 sec/batch)
2016-11-28 18:23:08.344347: step 6040, loss = 0.38, acc = 0.99 (279.5 examples/sec; 0.458 sec/batch)
2016-11-28 18:23:13.073284: step 6050, loss = 0.38, acc = 0.99 (283.8 examples/sec; 0.451 sec/batch)
2016-11-28 18:23:17.824314: step 6060, loss = 0.39, acc = 0.99 (280.1 examples/sec; 0.457 sec/batch)
2016-11-28 18:23:22.624358: step 6070, loss = 0.53, acc = 0.95 (274.3 examples/sec; 0.467 sec/batch)
2016-11-28 18:23:27.408636: step 6080, loss = 0.42, acc = 0.97 (290.7 examples/sec; 0.440 sec/batch)
2016-11-28 18:23:32.236988: step 6090, loss = 0.37, acc = 1.00 (291.0 examples/sec; 0.440 sec/batch)
2016-11-28 18:23:36.967124: step 6100, loss = 0.35, acc = 1.00 (286.8 examples/sec; 0.446 sec/batch)
At step 6100 cross validation precision: 0.375
2016-11-28 18:23:42.596235: step 6110, loss = 0.43, acc = 0.97 (292.0 examples/sec; 0.438 sec/batch)
2016-11-28 18:23:47.358325: step 6120, loss = 0.43, acc = 0.97 (278.2 examples/sec; 0.460 sec/batch)
2016-11-28 18:23:52.247299: step 6130, loss = 0.44, acc = 0.98 (287.4 examples/sec; 0.445 sec/batch)
2016-11-28 18:23:56.896782: step 6140, loss = 0.39, acc = 0.98 (294.1 examples/sec; 0.435 sec/batch)
2016-11-28 18:24:01.729405: step 6150, loss = 0.37, acc = 0.98 (281.6 examples/sec; 0.454 sec/batch)
2016-11-28 18:24:06.524477: step 6160, loss = 0.54, acc = 0.94 (294.2 examples/sec; 0.435 sec/batch)
2016-11-28 18:24:11.191524: step 6170, loss = 0.48, acc = 0.95 (287.8 examples/sec; 0.445 sec/batch)
2016-11-28 18:24:16.307744: step 6180, loss = 0.37, acc = 1.00 (227.4 examples/sec; 0.563 sec/batch)
2016-11-28 18:24:21.142593: step 6190, loss = 0.37, acc = 0.98 (300.7 examples/sec; 0.426 sec/batch)
2016-11-28 18:24:25.787091: step 6200, loss = 0.45, acc = 0.95 (238.1 examples/sec; 0.538 sec/batch)
At step 6200 cross validation precision: 0.336
2016-11-28 18:24:31.598245: step 6210, loss = 0.39, acc = 0.98 (283.1 examples/sec; 0.452 sec/batch)
2016-11-28 18:24:36.297471: step 6220, loss = 0.37, acc = 0.99 (286.0 examples/sec; 0.448 sec/batch)
2016-11-28 18:24:41.116242: step 6230, loss = 0.50, acc = 0.95 (283.9 examples/sec; 0.451 sec/batch)
2016-11-28 18:24:46.057686: step 6240, loss = 0.47, acc = 0.95 (285.5 examples/sec; 0.448 sec/batch)
2016-11-28 18:24:50.855832: step 6250, loss = 0.37, acc = 0.98 (285.8 examples/sec; 0.448 sec/batch)
2016-11-28 18:24:55.560053: step 6260, loss = 0.39, acc = 0.98 (288.4 examples/sec; 0.444 sec/batch)
2016-11-28 18:25:00.268096: step 6270, loss = 0.38, acc = 0.98 (289.0 examples/sec; 0.443 sec/batch)
2016-11-28 18:25:05.221289: step 6280, loss = 0.42, acc = 0.98 (254.1 examples/sec; 0.504 sec/batch)
2016-11-28 18:25:09.771999: step 6290, loss = 0.35, acc = 1.00 (284.3 examples/sec; 0.450 sec/batch)
2016-11-28 18:25:14.535735: step 6300, loss = 0.37, acc = 0.98 (293.2 examples/sec; 0.437 sec/batch)
At step 6300 cross validation precision: 0.430
2016-11-28 18:25:20.295934: step 6310, loss = 0.57, acc = 0.94 (295.1 examples/sec; 0.434 sec/batch)
2016-11-28 18:25:25.051073: step 6320, loss = 0.40, acc = 0.99 (295.5 examples/sec; 0.433 sec/batch)
2016-11-28 18:25:29.828745: step 6330, loss = 0.40, acc = 0.98 (288.7 examples/sec; 0.443 sec/batch)
2016-11-28 18:25:34.588909: step 6340, loss = 0.38, acc = 0.98 (298.2 examples/sec; 0.429 sec/batch)
2016-11-28 18:25:39.321318: step 6350, loss = 0.38, acc = 0.98 (289.8 examples/sec; 0.442 sec/batch)
2016-11-28 18:25:44.003981: step 6360, loss = 0.38, acc = 0.98 (287.1 examples/sec; 0.446 sec/batch)
2016-11-28 18:25:48.727187: step 6370, loss = 0.42, acc = 0.97 (288.8 examples/sec; 0.443 sec/batch)
2016-11-28 18:25:53.416488: step 6380, loss = 0.37, acc = 0.98 (296.8 examples/sec; 0.431 sec/batch)
2016-11-28 18:25:58.216004: step 6390, loss = 0.38, acc = 0.98 (288.1 examples/sec; 0.444 sec/batch)
2016-11-28 18:26:02.968003: step 6400, loss = 0.40, acc = 0.98 (285.8 examples/sec; 0.448 sec/batch)
At step 6400 cross validation precision: 0.383
2016-11-28 18:26:08.661365: step 6410, loss = 0.38, acc = 0.98 (283.1 examples/sec; 0.452 sec/batch)
2016-11-28 18:26:13.477009: step 6420, loss = 0.39, acc = 0.98 (285.3 examples/sec; 0.449 sec/batch)
2016-11-28 18:26:18.354416: step 6430, loss = 0.40, acc = 0.98 (290.5 examples/sec; 0.441 sec/batch)
2016-11-28 18:26:23.077461: step 6440, loss = 0.36, acc = 1.00 (228.4 examples/sec; 0.560 sec/batch)
2016-11-28 18:26:27.854061: step 6450, loss = 0.35, acc = 1.00 (235.0 examples/sec; 0.545 sec/batch)
2016-11-28 18:26:32.620689: step 6460, loss = 0.38, acc = 0.99 (283.6 examples/sec; 0.451 sec/batch)
2016-11-28 18:26:37.289963: step 6470, loss = 0.42, acc = 0.97 (294.0 examples/sec; 0.435 sec/batch)
2016-11-28 18:26:42.046113: step 6480, loss = 0.38, acc = 0.98 (289.1 examples/sec; 0.443 sec/batch)
2016-11-28 18:26:46.718062: step 6490, loss = 0.43, acc = 0.96 (281.4 examples/sec; 0.455 sec/batch)
2016-11-28 18:26:51.499706: step 6500, loss = 0.43, acc = 0.95 (291.7 examples/sec; 0.439 sec/batch)
At step 6500 cross validation precision: 0.430
2016-11-28 18:26:57.272262: step 6510, loss = 0.40, acc = 0.98 (288.8 examples/sec; 0.443 sec/batch)
2016-11-28 18:27:02.029978: step 6520, loss = 0.45, acc = 0.95 (288.5 examples/sec; 0.444 sec/batch)
2016-11-28 18:27:06.806240: step 6530, loss = 0.46, acc = 0.96 (286.6 examples/sec; 0.447 sec/batch)
2016-11-28 18:27:11.628072: step 6540, loss = 0.37, acc = 0.98 (290.1 examples/sec; 0.441 sec/batch)
2016-11-28 18:27:16.392789: step 6550, loss = 0.38, acc = 0.98 (208.3 examples/sec; 0.615 sec/batch)
2016-11-28 18:27:20.961272: step 6560, loss = 0.40, acc = 0.98 (285.8 examples/sec; 0.448 sec/batch)
2016-11-28 18:27:25.748910: step 6570, loss = 0.39, acc = 0.98 (287.5 examples/sec; 0.445 sec/batch)
2016-11-28 18:27:30.591725: step 6580, loss = 0.37, acc = 0.98 (284.3 examples/sec; 0.450 sec/batch)
2016-11-28 18:27:35.397911: step 6590, loss = 0.43, acc = 0.95 (223.0 examples/sec; 0.574 sec/batch)
2016-11-28 18:27:40.142089: step 6600, loss = 0.36, acc = 0.98 (272.4 examples/sec; 0.470 sec/batch)
At step 6600 cross validation precision: 0.375
2016-11-28 18:27:45.910204: step 6610, loss = 0.40, acc = 0.97 (285.4 examples/sec; 0.448 sec/batch)
2016-11-28 18:27:50.669943: step 6620, loss = 0.42, acc = 0.98 (291.5 examples/sec; 0.439 sec/batch)
2016-11-28 18:27:55.475563: step 6630, loss = 0.37, acc = 0.98 (283.8 examples/sec; 0.451 sec/batch)
2016-11-28 18:28:00.356683: step 6640, loss = 0.39, acc = 0.96 (277.3 examples/sec; 0.462 sec/batch)
2016-11-28 18:28:05.185637: step 6650, loss = 0.34, acc = 1.00 (288.0 examples/sec; 0.444 sec/batch)
2016-11-28 18:28:09.922089: step 6660, loss = 0.36, acc = 0.99 (287.2 examples/sec; 0.446 sec/batch)
2016-11-28 18:28:14.565660: step 6670, loss = 0.36, acc = 0.98 (291.3 examples/sec; 0.439 sec/batch)
2016-11-28 18:28:19.282583: step 6680, loss = 0.35, acc = 0.98 (229.1 examples/sec; 0.559 sec/batch)
2016-11-28 18:28:24.069978: step 6690, loss = 0.40, acc = 0.97 (202.6 examples/sec; 0.632 sec/batch)
2016-11-28 18:28:28.797930: step 6700, loss = 0.46, acc = 0.97 (288.2 examples/sec; 0.444 sec/batch)
At step 6700 cross validation precision: 0.406
2016-11-28 18:28:34.669486: step 6710, loss = 0.40, acc = 0.98 (288.4 examples/sec; 0.444 sec/batch)
2016-11-28 18:28:39.376686: step 6720, loss = 0.36, acc = 0.99 (288.7 examples/sec; 0.443 sec/batch)
2016-11-28 18:28:44.219595: step 6730, loss = 0.63, acc = 0.93 (285.5 examples/sec; 0.448 sec/batch)
2016-11-28 18:28:48.927227: step 6740, loss = 0.33, acc = 1.00 (289.4 examples/sec; 0.442 sec/batch)
2016-11-28 18:28:53.785281: step 6750, loss = 0.52, acc = 0.92 (287.1 examples/sec; 0.446 sec/batch)
2016-11-28 18:28:58.428755: step 6760, loss = 0.37, acc = 0.98 (287.4 examples/sec; 0.445 sec/batch)
2016-11-28 18:29:03.207297: step 6770, loss = 0.44, acc = 0.95 (291.0 examples/sec; 0.440 sec/batch)
2016-11-28 18:29:08.066133: step 6780, loss = 0.38, acc = 0.98 (252.1 examples/sec; 0.508 sec/batch)
2016-11-28 18:29:12.827588: step 6790, loss = 0.38, acc = 0.98 (234.6 examples/sec; 0.546 sec/batch)
2016-11-28 18:29:17.602797: step 6800, loss = 0.43, acc = 0.97 (241.0 examples/sec; 0.531 sec/batch)
At step 6800 cross validation precision: 0.328
2016-11-28 18:29:23.448096: step 6810, loss = 0.48, acc = 0.94 (288.9 examples/sec; 0.443 sec/batch)
2016-11-28 18:29:28.237492: step 6820, loss = 0.45, acc = 0.97 (202.8 examples/sec; 0.631 sec/batch)
2016-11-28 18:29:32.979544: step 6830, loss = 0.39, acc = 0.98 (286.4 examples/sec; 0.447 sec/batch)
2016-11-28 18:29:37.690503: step 6840, loss = 0.38, acc = 0.98 (288.3 examples/sec; 0.444 sec/batch)
2016-11-28 18:29:42.469635: step 6850, loss = 0.37, acc = 0.98 (287.5 examples/sec; 0.445 sec/batch)
2016-11-28 18:29:47.335917: step 6860, loss = 0.35, acc = 1.00 (289.1 examples/sec; 0.443 sec/batch)
2016-11-28 18:29:52.122331: step 6870, loss = 0.36, acc = 0.99 (229.0 examples/sec; 0.559 sec/batch)
2016-11-28 18:29:56.918692: step 6880, loss = 0.37, acc = 0.98 (287.1 examples/sec; 0.446 sec/batch)
2016-11-28 18:30:01.705484: step 6890, loss = 0.37, acc = 0.98 (293.2 examples/sec; 0.437 sec/batch)
2016-11-28 18:30:06.435505: step 6900, loss = 0.40, acc = 0.98 (294.0 examples/sec; 0.435 sec/batch)
At step 6900 cross validation precision: 0.367
2016-11-28 18:30:12.232514: step 6910, loss = 0.40, acc = 0.98 (293.2 examples/sec; 0.437 sec/batch)
2016-11-28 18:30:16.986401: step 6920, loss = 0.38, acc = 0.98 (226.9 examples/sec; 0.564 sec/batch)
2016-11-28 18:30:21.799554: step 6930, loss = 0.35, acc = 0.99 (244.0 examples/sec; 0.525 sec/batch)
2016-11-28 18:30:26.528283: step 6940, loss = 0.36, acc = 0.99 (289.3 examples/sec; 0.442 sec/batch)
2016-11-28 18:30:31.445853: step 6950, loss = 0.40, acc = 0.98 (203.5 examples/sec; 0.629 sec/batch)
2016-11-28 18:30:36.036000: step 6960, loss = 0.38, acc = 0.97 (289.1 examples/sec; 0.443 sec/batch)
2016-11-28 18:30:40.842789: step 6970, loss = 0.41, acc = 0.98 (290.2 examples/sec; 0.441 sec/batch)
2016-11-28 18:30:45.667682: step 6980, loss = 0.37, acc = 0.99 (287.5 examples/sec; 0.445 sec/batch)
2016-11-28 18:30:50.489716: step 6990, loss = 0.39, acc = 0.97 (288.4 examples/sec; 0.444 sec/batch)
2016-11-28 18:30:55.286661: step 7000, loss = 0.41, acc = 0.97 (284.7 examples/sec; 0.450 sec/batch)
At step 7000 cross validation precision: 0.453
2016-11-28 18:31:01.099090: step 7010, loss = 0.33, acc = 1.00 (286.4 examples/sec; 0.447 sec/batch)
2016-11-28 18:31:05.739858: step 7020, loss = 0.36, acc = 0.98 (240.9 examples/sec; 0.531 sec/batch)
2016-11-28 18:31:10.651367: step 7030, loss = 0.36, acc = 0.99 (286.6 examples/sec; 0.447 sec/batch)
2016-11-28 18:31:15.235576: step 7040, loss = 0.43, acc = 0.95 (295.2 examples/sec; 0.434 sec/batch)
2016-11-28 18:31:19.977841: step 7050, loss = 0.40, acc = 0.98 (289.1 examples/sec; 0.443 sec/batch)
2016-11-28 18:31:24.756078: step 7060, loss = 0.44, acc = 0.95 (281.8 examples/sec; 0.454 sec/batch)
2016-11-28 18:31:29.653000: step 7070, loss = 0.36, acc = 0.98 (287.2 examples/sec; 0.446 sec/batch)
2016-11-28 18:31:34.414044: step 7080, loss = 0.35, acc = 0.98 (285.1 examples/sec; 0.449 sec/batch)
2016-11-28 18:31:39.162721: step 7090, loss = 0.34, acc = 1.00 (289.2 examples/sec; 0.443 sec/batch)
2016-11-28 18:31:44.023883: step 7100, loss = 0.35, acc = 0.99 (289.3 examples/sec; 0.442 sec/batch)
At step 7100 cross validation precision: 0.406
2016-11-28 18:31:49.525646: step 7110, loss = 0.40, acc = 0.97 (288.0 examples/sec; 0.445 sec/batch)
2016-11-28 18:31:54.331004: step 7120, loss = 0.36, acc = 0.98 (287.3 examples/sec; 0.445 sec/batch)
2016-11-28 18:31:59.108761: step 7130, loss = 0.39, acc = 0.99 (289.3 examples/sec; 0.443 sec/batch)
2016-11-28 18:32:04.030523: step 7140, loss = 0.37, acc = 0.98 (242.7 examples/sec; 0.527 sec/batch)
2016-11-28 18:32:08.854754: step 7150, loss = 0.37, acc = 0.98 (288.3 examples/sec; 0.444 sec/batch)
2016-11-28 18:32:13.563609: step 7160, loss = 0.35, acc = 0.98 (284.0 examples/sec; 0.451 sec/batch)
2016-11-28 18:32:18.413004: step 7170, loss = 0.35, acc = 0.99 (234.2 examples/sec; 0.547 sec/batch)
2016-11-28 18:32:23.042531: step 7180, loss = 0.33, acc = 0.99 (298.1 examples/sec; 0.429 sec/batch)
2016-11-28 18:32:27.831705: step 7190, loss = 0.39, acc = 0.96 (285.5 examples/sec; 0.448 sec/batch)
2016-11-28 18:32:32.685679: step 7200, loss = 0.37, acc = 0.98 (288.2 examples/sec; 0.444 sec/batch)
At step 7200 cross validation precision: 0.406
2016-11-28 18:32:38.291994: step 7210, loss = 0.37, acc = 0.97 (287.2 examples/sec; 0.446 sec/batch)
2016-11-28 18:32:43.014853: step 7220, loss = 0.41, acc = 0.97 (211.9 examples/sec; 0.604 sec/batch)
2016-11-28 18:32:47.652884: step 7230, loss = 0.40, acc = 0.96 (291.7 examples/sec; 0.439 sec/batch)
2016-11-28 18:32:52.430579: step 7240, loss = 0.34, acc = 0.98 (210.5 examples/sec; 0.608 sec/batch)
2016-11-28 18:32:57.193216: step 7250, loss = 0.35, acc = 0.98 (232.6 examples/sec; 0.550 sec/batch)
2016-11-28 18:33:02.174594: step 7260, loss = 0.41, acc = 0.96 (210.0 examples/sec; 0.610 sec/batch)
2016-11-28 18:33:06.897228: step 7270, loss = 0.32, acc = 1.00 (291.6 examples/sec; 0.439 sec/batch)
2016-11-28 18:33:11.668248: step 7280, loss = 0.36, acc = 0.98 (286.9 examples/sec; 0.446 sec/batch)
2016-11-28 18:33:16.433969: step 7290, loss = 0.35, acc = 0.99 (247.1 examples/sec; 0.518 sec/batch)
2016-11-28 18:33:21.226714: step 7300, loss = 0.37, acc = 0.98 (289.2 examples/sec; 0.443 sec/batch)
At step 7300 cross validation precision: 0.422
2016-11-28 18:33:27.020488: step 7310, loss = 0.43, acc = 0.96 (233.9 examples/sec; 0.547 sec/batch)
2016-11-28 18:33:31.731905: step 7320, loss = 0.41, acc = 0.96 (288.0 examples/sec; 0.444 sec/batch)
2016-11-28 18:33:36.661361: step 7330, loss = 0.42, acc = 0.96 (227.6 examples/sec; 0.562 sec/batch)
2016-11-28 18:33:41.278464: step 7340, loss = 0.35, acc = 0.99 (289.0 examples/sec; 0.443 sec/batch)
2016-11-28 18:33:46.198968: step 7350, loss = 0.36, acc = 0.98 (224.2 examples/sec; 0.571 sec/batch)
2016-11-28 18:33:50.922194: step 7360, loss = 0.42, acc = 0.98 (293.8 examples/sec; 0.436 sec/batch)
2016-11-28 18:33:55.583507: step 7370, loss = 0.33, acc = 0.99 (289.4 examples/sec; 0.442 sec/batch)
2016-11-28 18:34:00.474049: step 7380, loss = 0.35, acc = 0.98 (231.0 examples/sec; 0.554 sec/batch)
2016-11-28 18:34:05.242844: step 7390, loss = 0.33, acc = 1.00 (240.2 examples/sec; 0.533 sec/batch)
2016-11-28 18:34:09.928001: step 7400, loss = 0.51, acc = 0.94 (290.7 examples/sec; 0.440 sec/batch)
At step 7400 cross validation precision: 0.391
2016-11-28 18:34:15.527453: step 7410, loss = 0.39, acc = 0.98 (216.2 examples/sec; 0.592 sec/batch)
2016-11-28 18:34:20.222894: step 7420, loss = 0.38, acc = 0.96 (295.9 examples/sec; 0.433 sec/batch)
2016-11-28 18:34:25.159229: step 7430, loss = 0.47, acc = 0.97 (213.4 examples/sec; 0.600 sec/batch)
2016-11-28 18:34:29.974043: step 7440, loss = 0.36, acc = 0.99 (291.7 examples/sec; 0.439 sec/batch)
2016-11-28 18:34:34.737260: step 7450, loss = 0.39, acc = 0.97 (289.3 examples/sec; 0.442 sec/batch)
2016-11-28 18:34:39.738096: step 7460, loss = 0.44, acc = 0.97 (243.8 examples/sec; 0.525 sec/batch)
2016-11-28 18:34:44.386392: step 7470, loss = 0.39, acc = 0.98 (292.0 examples/sec; 0.438 sec/batch)
2016-11-28 18:34:49.217813: step 7480, loss = 0.37, acc = 0.98 (284.0 examples/sec; 0.451 sec/batch)
2016-11-28 18:34:54.029584: step 7490, loss = 0.40, acc = 0.98 (286.2 examples/sec; 0.447 sec/batch)
2016-11-28 18:34:58.823066: step 7500, loss = 0.50, acc = 0.93 (294.1 examples/sec; 0.435 sec/batch)
At step 7500 cross validation precision: 0.391
2016-11-28 18:35:04.426744: step 7510, loss = 0.34, acc = 0.99 (286.2 examples/sec; 0.447 sec/batch)
2016-11-28 18:35:09.190334: step 7520, loss = 0.36, acc = 0.98 (280.7 examples/sec; 0.456 sec/batch)
2016-11-28 18:35:13.978262: step 7530, loss = 0.32, acc = 1.00 (230.5 examples/sec; 0.555 sec/batch)
2016-11-28 18:35:18.908720: step 7540, loss = 0.39, acc = 0.97 (291.8 examples/sec; 0.439 sec/batch)
2016-11-28 18:35:23.721027: step 7550, loss = 0.34, acc = 1.00 (284.3 examples/sec; 0.450 sec/batch)
2016-11-28 18:35:28.584979: step 7560, loss = 0.36, acc = 0.98 (230.0 examples/sec; 0.556 sec/batch)
2016-11-28 18:35:33.329314: step 7570, loss = 0.38, acc = 0.98 (285.9 examples/sec; 0.448 sec/batch)
2016-11-28 18:35:38.033187: step 7580, loss = 0.50, acc = 0.92 (280.4 examples/sec; 0.457 sec/batch)
2016-11-28 18:35:42.908751: step 7590, loss = 0.69, acc = 0.88 (236.0 examples/sec; 0.542 sec/batch)
2016-11-28 18:35:47.755428: step 7600, loss = 0.47, acc = 0.95 (227.1 examples/sec; 0.564 sec/batch)
At step 7600 cross validation precision: 0.391
2016-11-28 18:35:53.386104: step 7610, loss = 0.40, acc = 0.98 (291.4 examples/sec; 0.439 sec/batch)
2016-11-28 18:35:58.237874: step 7620, loss = 0.47, acc = 0.95 (292.4 examples/sec; 0.438 sec/batch)
2016-11-28 18:36:03.062836: step 7630, loss = 0.40, acc = 0.98 (283.6 examples/sec; 0.451 sec/batch)
2016-11-28 18:36:07.604168: step 7640, loss = 0.41, acc = 0.98 (286.9 examples/sec; 0.446 sec/batch)
2016-11-28 18:36:12.533441: step 7650, loss = 0.38, acc = 0.98 (287.9 examples/sec; 0.445 sec/batch)
2016-11-28 18:36:17.323183: step 7660, loss = 0.46, acc = 0.97 (303.7 examples/sec; 0.421 sec/batch)
2016-11-28 18:36:22.123634: step 7670, loss = 0.40, acc = 0.98 (209.6 examples/sec; 0.611 sec/batch)
2016-11-28 18:36:26.913287: step 7680, loss = 0.39, acc = 0.98 (275.7 examples/sec; 0.464 sec/batch)
2016-11-28 18:36:31.444144: step 7690, loss = 0.41, acc = 0.96 (291.6 examples/sec; 0.439 sec/batch)
2016-11-28 18:36:36.408154: step 7700, loss = 0.39, acc = 0.97 (300.1 examples/sec; 0.427 sec/batch)
At step 7700 cross validation precision: 0.383
2016-11-28 18:36:42.124034: step 7710, loss = 0.36, acc = 0.99 (291.2 examples/sec; 0.440 sec/batch)
2016-11-28 18:36:46.881629: step 7720, loss = 0.35, acc = 0.99 (287.4 examples/sec; 0.445 sec/batch)
2016-11-28 18:36:51.670668: step 7730, loss = 0.34, acc = 1.00 (288.0 examples/sec; 0.444 sec/batch)
2016-11-28 18:36:56.516105: step 7740, loss = 0.36, acc = 0.98 (247.0 examples/sec; 0.518 sec/batch)
2016-11-28 18:37:01.119359: step 7750, loss = 0.35, acc = 0.99 (289.2 examples/sec; 0.443 sec/batch)
2016-11-28 18:37:05.949170: step 7760, loss = 0.33, acc = 1.00 (293.4 examples/sec; 0.436 sec/batch)
2016-11-28 18:37:10.672338: step 7770, loss = 0.33, acc = 1.00 (289.6 examples/sec; 0.442 sec/batch)
2016-11-28 18:37:15.610573: step 7780, loss = 0.43, acc = 0.95 (196.7 examples/sec; 0.651 sec/batch)
2016-11-28 18:37:20.394636: step 7790, loss = 0.37, acc = 0.98 (286.3 examples/sec; 0.447 sec/batch)
2016-11-28 18:37:25.182674: step 7800, loss = 0.33, acc = 0.99 (287.8 examples/sec; 0.445 sec/batch)
At step 7800 cross validation precision: 0.430
2016-11-28 18:37:30.869126: step 7810, loss = 0.33, acc = 0.99 (289.5 examples/sec; 0.442 sec/batch)
2016-11-28 18:37:35.612014: step 7820, loss = 0.33, acc = 0.99 (282.8 examples/sec; 0.453 sec/batch)
2016-11-28 18:37:40.386933: step 7830, loss = 0.34, acc = 1.00 (285.3 examples/sec; 0.449 sec/batch)
2016-11-28 18:37:45.139614: step 7840, loss = 0.33, acc = 0.99 (224.3 examples/sec; 0.571 sec/batch)
2016-11-28 18:37:49.940851: step 7850, loss = 0.32, acc = 1.00 (252.3 examples/sec; 0.507 sec/batch)
2016-11-28 18:37:54.694710: step 7860, loss = 0.35, acc = 0.99 (209.7 examples/sec; 0.610 sec/batch)
2016-11-28 18:37:59.504821: step 7870, loss = 0.32, acc = 1.00 (288.9 examples/sec; 0.443 sec/batch)
2016-11-28 18:38:04.248480: step 7880, loss = 0.31, acc = 1.00 (289.0 examples/sec; 0.443 sec/batch)
2016-11-28 18:38:09.027304: step 7890, loss = 0.31, acc = 0.99 (287.0 examples/sec; 0.446 sec/batch)
2016-11-28 18:38:13.804854: step 7900, loss = 0.31, acc = 1.00 (287.1 examples/sec; 0.446 sec/batch)
At step 7900 cross validation precision: 0.406
2016-11-28 18:38:19.537823: step 7910, loss = 0.31, acc = 1.00 (223.8 examples/sec; 0.572 sec/batch)
2016-11-28 18:38:24.174276: step 7920, loss = 0.36, acc = 0.98 (286.7 examples/sec; 0.446 sec/batch)
2016-11-28 18:38:29.129673: step 7930, loss = 0.35, acc = 0.98 (213.2 examples/sec; 0.600 sec/batch)
2016-11-28 18:38:33.899023: step 7940, loss = 0.31, acc = 1.00 (255.2 examples/sec; 0.502 sec/batch)
2016-11-28 18:38:38.672840: step 7950, loss = 0.32, acc = 0.99 (284.9 examples/sec; 0.449 sec/batch)
2016-11-28 18:38:43.405051: step 7960, loss = 0.32, acc = 0.99 (228.4 examples/sec; 0.560 sec/batch)
2016-11-28 18:38:48.187023: step 7970, loss = 0.33, acc = 0.98 (285.2 examples/sec; 0.449 sec/batch)
2016-11-28 18:38:52.913209: step 7980, loss = 0.32, acc = 0.99 (288.4 examples/sec; 0.444 sec/batch)
2016-11-28 18:38:57.688262: step 7990, loss = 0.32, acc = 1.00 (284.9 examples/sec; 0.449 sec/batch)
2016-11-28 18:39:02.522555: step 8000, loss = 0.35, acc = 0.98 (287.4 examples/sec; 0.445 sec/batch)
At step 8000 cross validation precision: 0.398
2016-11-28 18:39:08.181626: step 8010, loss = 0.32, acc = 0.98 (224.6 examples/sec; 0.570 sec/batch)
2016-11-28 18:39:13.020083: step 8020, loss = 0.34, acc = 0.97 (219.2 examples/sec; 0.584 sec/batch)
2016-11-28 18:39:17.599489: step 8030, loss = 0.49, acc = 0.93 (287.3 examples/sec; 0.445 sec/batch)
2016-11-28 18:39:22.468319: step 8040, loss = 0.37, acc = 0.98 (266.4 examples/sec; 0.480 sec/batch)
2016-11-28 18:39:27.250490: step 8050, loss = 0.35, acc = 0.97 (232.2 examples/sec; 0.551 sec/batch)
2016-11-28 18:39:32.014161: step 8060, loss = 0.43, acc = 0.95 (216.0 examples/sec; 0.593 sec/batch)
2016-11-28 18:39:36.781218: step 8070, loss = 0.39, acc = 0.95 (291.4 examples/sec; 0.439 sec/batch)
2016-11-28 18:39:41.466665: step 8080, loss = 0.36, acc = 0.97 (293.3 examples/sec; 0.436 sec/batch)
2016-11-28 18:39:46.233083: step 8090, loss = 0.39, acc = 0.96 (283.1 examples/sec; 0.452 sec/batch)
2016-11-28 18:39:51.099896: step 8100, loss = 0.40, acc = 0.96 (208.3 examples/sec; 0.615 sec/batch)
At step 8100 cross validation precision: 0.430
2016-11-28 18:39:56.916978: step 8110, loss = 0.45, acc = 0.95 (287.2 examples/sec; 0.446 sec/batch)
2016-11-28 18:40:01.838791: step 8120, loss = 0.34, acc = 1.00 (287.5 examples/sec; 0.445 sec/batch)
2016-11-28 18:40:06.434902: step 8130, loss = 0.35, acc = 0.98 (283.8 examples/sec; 0.451 sec/batch)
2016-11-28 18:40:11.091905: step 8140, loss = 0.35, acc = 0.98 (295.4 examples/sec; 0.433 sec/batch)
2016-11-28 18:40:16.001860: step 8150, loss = 0.38, acc = 0.95 (235.4 examples/sec; 0.544 sec/batch)
2016-11-28 18:40:20.884317: step 8160, loss = 0.51, acc = 0.92 (204.6 examples/sec; 0.625 sec/batch)
2016-11-28 18:40:25.638513: step 8170, loss = 0.44, acc = 0.94 (281.6 examples/sec; 0.454 sec/batch)
2016-11-28 18:40:30.471803: step 8180, loss = 0.37, acc = 0.98 (254.1 examples/sec; 0.504 sec/batch)
2016-11-28 18:40:35.227897: step 8190, loss = 0.42, acc = 0.95 (291.7 examples/sec; 0.439 sec/batch)
2016-11-28 18:40:40.025478: step 8200, loss = 0.49, acc = 0.94 (290.7 examples/sec; 0.440 sec/batch)
At step 8200 cross validation precision: 0.414
2016-11-28 18:40:45.699179: step 8210, loss = 0.41, acc = 0.95 (292.1 examples/sec; 0.438 sec/batch)
2016-11-28 18:40:50.482533: step 8220, loss = 0.42, acc = 0.96 (298.7 examples/sec; 0.429 sec/batch)
2016-11-28 18:40:55.248659: step 8230, loss = 0.37, acc = 0.98 (286.9 examples/sec; 0.446 sec/batch)
2016-11-28 18:41:00.152834: step 8240, loss = 0.35, acc = 0.98 (288.6 examples/sec; 0.444 sec/batch)
2016-11-28 18:41:04.900323: step 8250, loss = 0.38, acc = 0.98 (229.0 examples/sec; 0.559 sec/batch)
2016-11-28 18:41:09.670726: step 8260, loss = 0.34, acc = 0.98 (247.2 examples/sec; 0.518 sec/batch)
2016-11-28 18:41:14.409910: step 8270, loss = 0.33, acc = 0.99 (284.1 examples/sec; 0.451 sec/batch)
2016-11-28 18:41:19.421016: step 8280, loss = 0.35, acc = 0.98 (273.7 examples/sec; 0.468 sec/batch)
2016-11-28 18:41:24.127407: step 8290, loss = 0.34, acc = 0.99 (211.5 examples/sec; 0.605 sec/batch)
2016-11-28 18:41:28.951789: step 8300, loss = 0.37, acc = 0.98 (244.4 examples/sec; 0.524 sec/batch)
At step 8300 cross validation precision: 0.438
2016-11-28 18:41:34.584959: step 8310, loss = 0.40, acc = 0.98 (284.5 examples/sec; 0.450 sec/batch)
2016-11-28 18:41:39.231232: step 8320, loss = 0.35, acc = 0.98 (288.0 examples/sec; 0.444 sec/batch)
2016-11-28 18:41:44.081264: step 8330, loss = 0.37, acc = 0.96 (297.9 examples/sec; 0.430 sec/batch)
2016-11-28 18:41:48.859622: step 8340, loss = 0.32, acc = 0.99 (293.3 examples/sec; 0.436 sec/batch)
2016-11-28 18:41:53.689434: step 8350, loss = 0.31, acc = 1.00 (286.9 examples/sec; 0.446 sec/batch)
2016-11-28 18:41:58.398678: step 8360, loss = 0.34, acc = 0.97 (299.8 examples/sec; 0.427 sec/batch)
2016-11-28 18:42:03.208698: step 8370, loss = 0.35, acc = 0.99 (286.1 examples/sec; 0.447 sec/batch)
2016-11-28 18:42:07.894067: step 8380, loss = 0.31, acc = 1.00 (289.7 examples/sec; 0.442 sec/batch)
2016-11-28 18:42:12.513554: step 8390, loss = 0.34, acc = 0.98 (286.4 examples/sec; 0.447 sec/batch)
2016-11-28 18:42:17.370586: step 8400, loss = 0.39, acc = 0.98 (290.5 examples/sec; 0.441 sec/batch)
At step 8400 cross validation precision: 0.406
2016-11-28 18:42:23.082511: step 8410, loss = 0.31, acc = 1.00 (283.4 examples/sec; 0.452 sec/batch)
2016-11-28 18:42:27.868031: step 8420, loss = 0.32, acc = 1.00 (292.1 examples/sec; 0.438 sec/batch)
2016-11-28 18:42:32.712328: step 8430, loss = 0.34, acc = 0.98 (286.0 examples/sec; 0.448 sec/batch)
2016-11-28 18:42:37.335479: step 8440, loss = 0.35, acc = 0.98 (289.4 examples/sec; 0.442 sec/batch)
2016-11-28 18:42:42.235193: step 8450, loss = 0.35, acc = 0.98 (200.6 examples/sec; 0.638 sec/batch)
2016-11-28 18:42:47.055093: step 8460, loss = 0.34, acc = 0.98 (210.1 examples/sec; 0.609 sec/batch)
2016-11-28 18:42:51.662440: step 8470, loss = 0.33, acc = 0.99 (290.7 examples/sec; 0.440 sec/batch)
2016-11-28 18:42:56.417432: step 8480, loss = 0.35, acc = 0.98 (289.9 examples/sec; 0.442 sec/batch)
2016-11-28 18:43:01.262924: step 8490, loss = 0.31, acc = 0.99 (287.3 examples/sec; 0.445 sec/batch)
2016-11-28 18:43:06.077817: step 8500, loss = 0.31, acc = 0.99 (286.1 examples/sec; 0.447 sec/batch)
At step 8500 cross validation precision: 0.359
2016-11-28 18:43:11.867036: step 8510, loss = 0.33, acc = 0.99 (270.6 examples/sec; 0.473 sec/batch)
2016-11-28 18:43:16.607306: step 8520, loss = 0.33, acc = 0.98 (287.3 examples/sec; 0.446 sec/batch)
2016-11-28 18:43:21.413102: step 8530, loss = 0.33, acc = 0.99 (290.8 examples/sec; 0.440 sec/batch)
2016-11-28 18:43:26.162896: step 8540, loss = 0.30, acc = 0.99 (288.7 examples/sec; 0.443 sec/batch)
2016-11-28 18:43:30.975469: step 8550, loss = 0.32, acc = 0.99 (286.0 examples/sec; 0.448 sec/batch)
2016-11-28 18:43:35.787216: step 8560, loss = 0.30, acc = 0.99 (292.5 examples/sec; 0.438 sec/batch)
2016-11-28 18:43:40.492234: step 8570, loss = 0.35, acc = 0.98 (289.3 examples/sec; 0.442 sec/batch)
2016-11-28 18:43:45.361675: step 8580, loss = 0.34, acc = 0.97 (244.4 examples/sec; 0.524 sec/batch)
2016-11-28 18:43:50.134554: step 8590, loss = 0.39, acc = 0.94 (293.7 examples/sec; 0.436 sec/batch)
2016-11-28 18:43:54.905098: step 8600, loss = 0.31, acc = 0.98 (290.9 examples/sec; 0.440 sec/batch)
At step 8600 cross validation precision: 0.445
2016-11-28 18:44:00.570783: step 8610, loss = 0.33, acc = 0.98 (286.7 examples/sec; 0.446 sec/batch)
2016-11-28 18:44:05.333071: step 8620, loss = 0.37, acc = 0.98 (287.9 examples/sec; 0.445 sec/batch)
2016-11-28 18:44:09.911181: step 8630, loss = 0.40, acc = 0.97 (296.1 examples/sec; 0.432 sec/batch)
2016-11-28 18:44:14.862138: step 8640, loss = 0.38, acc = 0.96 (230.2 examples/sec; 0.556 sec/batch)
2016-11-28 18:44:19.532948: step 8650, loss = 0.42, acc = 0.95 (289.6 examples/sec; 0.442 sec/batch)
2016-11-28 18:44:24.343512: step 8660, loss = 0.34, acc = 0.98 (286.9 examples/sec; 0.446 sec/batch)
2016-11-28 18:44:29.091179: step 8670, loss = 0.33, acc = 0.99 (292.8 examples/sec; 0.437 sec/batch)
2016-11-28 18:44:33.927333: step 8680, loss = 0.36, acc = 0.97 (280.4 examples/sec; 0.456 sec/batch)
2016-11-28 18:44:38.727881: step 8690, loss = 0.33, acc = 0.98 (286.3 examples/sec; 0.447 sec/batch)
2016-11-28 18:44:43.509189: step 8700, loss = 0.34, acc = 0.98 (284.9 examples/sec; 0.449 sec/batch)
At step 8700 cross validation precision: 0.430
2016-11-28 18:44:49.327413: step 8710, loss = 0.34, acc = 0.98 (259.2 examples/sec; 0.494 sec/batch)
2016-11-28 18:44:54.046365: step 8720, loss = 0.33, acc = 0.99 (292.8 examples/sec; 0.437 sec/batch)
2016-11-28 18:44:58.923559: step 8730, loss = 0.31, acc = 1.00 (226.3 examples/sec; 0.566 sec/batch)
2016-11-28 18:45:03.543679: step 8740, loss = 0.29, acc = 1.00 (292.3 examples/sec; 0.438 sec/batch)
2016-11-28 18:45:08.492978: step 8750, loss = 0.33, acc = 0.98 (225.6 examples/sec; 0.567 sec/batch)
2016-11-28 18:45:13.215030: step 8760, loss = 0.34, acc = 0.97 (291.2 examples/sec; 0.440 sec/batch)
2016-11-28 18:45:17.991595: step 8770, loss = 0.32, acc = 0.98 (296.5 examples/sec; 0.432 sec/batch)
2016-11-28 18:45:22.814537: step 8780, loss = 0.54, acc = 0.87 (290.5 examples/sec; 0.441 sec/batch)
2016-11-28 18:45:27.520479: step 8790, loss = 0.44, acc = 0.93 (292.6 examples/sec; 0.437 sec/batch)
2016-11-28 18:45:32.388064: step 8800, loss = 0.35, acc = 0.98 (292.2 examples/sec; 0.438 sec/batch)
At step 8800 cross validation precision: 0.289
2016-11-28 18:45:38.096900: step 8810, loss = 0.39, acc = 0.96 (289.0 examples/sec; 0.443 sec/batch)
2016-11-28 18:45:42.767547: step 8820, loss = 0.38, acc = 0.98 (284.1 examples/sec; 0.451 sec/batch)
2016-11-28 18:45:47.547076: step 8830, loss = 0.42, acc = 0.95 (288.4 examples/sec; 0.444 sec/batch)
2016-11-28 18:45:52.365690: step 8840, loss = 0.39, acc = 0.97 (287.2 examples/sec; 0.446 sec/batch)
2016-11-28 18:45:57.052262: step 8850, loss = 0.33, acc = 0.99 (289.5 examples/sec; 0.442 sec/batch)
2016-11-28 18:46:01.835989: step 8860, loss = 0.39, acc = 0.98 (288.1 examples/sec; 0.444 sec/batch)
2016-11-28 18:46:06.562692: step 8870, loss = 0.41, acc = 0.98 (293.5 examples/sec; 0.436 sec/batch)
2016-11-28 18:46:11.249840: step 8880, loss = 0.39, acc = 0.98 (225.1 examples/sec; 0.569 sec/batch)
2016-11-28 18:46:16.085189: step 8890, loss = 0.41, acc = 0.95 (210.4 examples/sec; 0.608 sec/batch)
2016-11-28 18:46:20.853052: step 8900, loss = 0.41, acc = 0.95 (258.5 examples/sec; 0.495 sec/batch)
At step 8900 cross validation precision: 0.484
2016-11-28 18:46:28.407623: step 8910, loss = 0.39, acc = 0.98 (178.6 examples/sec; 0.717 sec/batch)
2016-11-28 18:46:33.838414: step 8920, loss = 0.41, acc = 0.97 (210.1 examples/sec; 0.609 sec/batch)
2016-11-28 18:46:38.639875: step 8930, loss = 0.34, acc = 1.00 (227.6 examples/sec; 0.562 sec/batch)
2016-11-28 18:46:43.378642: step 8940, loss = 0.36, acc = 0.98 (288.8 examples/sec; 0.443 sec/batch)
2016-11-28 18:46:48.024790: step 8950, loss = 0.33, acc = 1.00 (285.5 examples/sec; 0.448 sec/batch)
2016-11-28 18:46:52.828778: step 8960, loss = 0.33, acc = 0.99 (281.3 examples/sec; 0.455 sec/batch)
2016-11-28 18:46:57.523336: step 8970, loss = 0.34, acc = 0.98 (281.3 examples/sec; 0.455 sec/batch)
2016-11-28 18:47:02.333709: step 8980, loss = 0.34, acc = 0.98 (256.9 examples/sec; 0.498 sec/batch)
2016-11-28 18:47:07.101578: step 8990, loss = 0.35, acc = 0.98 (209.3 examples/sec; 0.612 sec/batch)
2016-11-28 18:47:11.750646: step 9000, loss = 0.32, acc = 0.99 (298.8 examples/sec; 0.428 sec/batch)
At step 9000 cross validation precision: 0.398
2016-11-28 18:47:17.672421: step 9010, loss = 0.36, acc = 0.97 (285.7 examples/sec; 0.448 sec/batch)
2016-11-28 18:47:22.398073: step 9020, loss = 0.34, acc = 0.99 (223.4 examples/sec; 0.573 sec/batch)
2016-11-28 18:47:27.038469: step 9030, loss = 0.38, acc = 0.96 (285.1 examples/sec; 0.449 sec/batch)
2016-11-28 18:47:31.811841: step 9040, loss = 0.46, acc = 0.95 (290.3 examples/sec; 0.441 sec/batch)
2016-11-28 18:47:36.561239: step 9050, loss = 0.37, acc = 0.98 (284.5 examples/sec; 0.450 sec/batch)
2016-11-28 18:47:41.296647: step 9060, loss = 0.42, acc = 0.96 (289.9 examples/sec; 0.442 sec/batch)
2016-11-28 18:47:45.880911: step 9070, loss = 0.37, acc = 0.98 (284.3 examples/sec; 0.450 sec/batch)
2016-11-28 18:47:50.628252: step 9080, loss = 0.45, acc = 0.95 (287.1 examples/sec; 0.446 sec/batch)
2016-11-28 18:47:55.231844: step 9090, loss = 0.33, acc = 0.99 (293.0 examples/sec; 0.437 sec/batch)
2016-11-28 18:47:59.964405: step 9100, loss = 0.38, acc = 0.97 (287.3 examples/sec; 0.446 sec/batch)
At step 9100 cross validation precision: 0.422
2016-11-28 18:48:05.595713: step 9110, loss = 0.34, acc = 0.98 (239.5 examples/sec; 0.534 sec/batch)
2016-11-28 18:48:10.256491: step 9120, loss = 0.33, acc = 0.98 (286.2 examples/sec; 0.447 sec/batch)
2016-11-28 18:48:14.958884: step 9130, loss = 0.39, acc = 0.97 (293.0 examples/sec; 0.437 sec/batch)
2016-11-28 18:48:19.682666: step 9140, loss = 0.33, acc = 0.99 (286.0 examples/sec; 0.447 sec/batch)
2016-11-28 18:48:24.508100: step 9150, loss = 0.32, acc = 1.00 (291.3 examples/sec; 0.439 sec/batch)
2016-11-28 18:48:29.125421: step 9160, loss = 0.31, acc = 1.00 (286.7 examples/sec; 0.446 sec/batch)
2016-11-28 18:48:33.860133: step 9170, loss = 0.33, acc = 0.98 (243.9 examples/sec; 0.525 sec/batch)
2016-11-28 18:48:38.571007: step 9180, loss = 0.32, acc = 0.99 (289.6 examples/sec; 0.442 sec/batch)
2016-11-28 18:48:43.317825: step 9190, loss = 0.33, acc = 0.99 (296.5 examples/sec; 0.432 sec/batch)
2016-11-28 18:48:48.109556: step 9200, loss = 0.36, acc = 0.98 (294.8 examples/sec; 0.434 sec/batch)
At step 9200 cross validation precision: 0.367
2016-11-28 18:48:53.602078: step 9210, loss = 0.31, acc = 1.00 (290.3 examples/sec; 0.441 sec/batch)
2016-11-28 18:48:58.615087: step 9220, loss = 0.45, acc = 0.96 (209.6 examples/sec; 0.611 sec/batch)
2016-11-28 18:49:03.387571: step 9230, loss = 0.35, acc = 0.98 (225.6 examples/sec; 0.567 sec/batch)
2016-11-28 18:49:08.048616: step 9240, loss = 0.46, acc = 0.97 (288.0 examples/sec; 0.444 sec/batch)
2016-11-28 18:49:12.769894: step 9250, loss = 0.35, acc = 0.98 (254.7 examples/sec; 0.503 sec/batch)
2016-11-28 18:49:17.369230: step 9260, loss = 0.36, acc = 0.98 (291.8 examples/sec; 0.439 sec/batch)
2016-11-28 18:49:22.100321: step 9270, loss = 0.32, acc = 0.98 (290.2 examples/sec; 0.441 sec/batch)
2016-11-28 18:49:26.952158: step 9280, loss = 0.31, acc = 1.00 (261.1 examples/sec; 0.490 sec/batch)
2016-11-28 18:49:31.554251: step 9290, loss = 0.34, acc = 0.99 (295.3 examples/sec; 0.433 sec/batch)
2016-11-28 18:49:36.308354: step 9300, loss = 0.40, acc = 0.98 (208.5 examples/sec; 0.614 sec/batch)
At step 9300 cross validation precision: 0.344
2016-11-28 18:49:42.064006: step 9310, loss = 0.34, acc = 0.99 (295.7 examples/sec; 0.433 sec/batch)
2016-11-28 18:49:46.637859: step 9320, loss = 0.39, acc = 0.98 (288.0 examples/sec; 0.444 sec/batch)
2016-11-28 18:49:51.405647: step 9330, loss = 0.35, acc = 0.99 (286.1 examples/sec; 0.447 sec/batch)
2016-11-28 18:49:56.144697: step 9340, loss = 0.35, acc = 0.98 (287.9 examples/sec; 0.445 sec/batch)
2016-11-28 18:50:00.883774: step 9350, loss = 0.36, acc = 0.98 (290.2 examples/sec; 0.441 sec/batch)
2016-11-28 18:50:05.641308: step 9360, loss = 0.42, acc = 0.98 (287.4 examples/sec; 0.445 sec/batch)
2016-11-28 18:50:10.251971: step 9370, loss = 0.34, acc = 0.99 (294.9 examples/sec; 0.434 sec/batch)
2016-11-28 18:50:15.129939: step 9380, loss = 0.31, acc = 1.00 (286.9 examples/sec; 0.446 sec/batch)
2016-11-28 18:50:19.892935: step 9390, loss = 0.31, acc = 1.00 (288.2 examples/sec; 0.444 sec/batch)
2016-11-28 18:50:24.571771: step 9400, loss = 0.32, acc = 0.99 (284.8 examples/sec; 0.449 sec/batch)
At step 9400 cross validation precision: 0.492
2016-11-28 18:50:30.398697: step 9410, loss = 0.34, acc = 0.98 (265.1 examples/sec; 0.483 sec/batch)
2016-11-28 18:50:35.095427: step 9420, loss = 0.39, acc = 0.98 (284.8 examples/sec; 0.449 sec/batch)
2016-11-28 18:50:39.843686: step 9430, loss = 0.36, acc = 0.98 (282.2 examples/sec; 0.454 sec/batch)
2016-11-28 18:50:44.568372: step 9440, loss = 0.31, acc = 1.00 (287.5 examples/sec; 0.445 sec/batch)
2016-11-28 18:50:49.353005: step 9450, loss = 0.38, acc = 0.98 (286.5 examples/sec; 0.447 sec/batch)
2016-11-28 18:50:54.170814: step 9460, loss = 0.32, acc = 0.99 (287.7 examples/sec; 0.445 sec/batch)
2016-11-28 18:50:58.932956: step 9470, loss = 0.32, acc = 0.99 (286.6 examples/sec; 0.447 sec/batch)
2016-11-28 18:51:03.731353: step 9480, loss = 0.37, acc = 0.98 (283.7 examples/sec; 0.451 sec/batch)
2016-11-28 18:51:08.431771: step 9490, loss = 0.35, acc = 0.98 (282.9 examples/sec; 0.452 sec/batch)
2016-11-28 18:51:13.088048: step 9500, loss = 0.33, acc = 1.00 (282.4 examples/sec; 0.453 sec/batch)
At step 9500 cross validation precision: 0.375
2016-11-28 18:51:18.897751: step 9510, loss = 0.41, acc = 0.95 (204.5 examples/sec; 0.626 sec/batch)
2016-11-28 18:51:23.509895: step 9520, loss = 0.40, acc = 0.96 (288.2 examples/sec; 0.444 sec/batch)
2016-11-28 18:51:28.425191: step 9530, loss = 0.35, acc = 0.98 (225.3 examples/sec; 0.568 sec/batch)
2016-11-28 18:51:33.139385: step 9540, loss = 0.34, acc = 0.98 (229.9 examples/sec; 0.557 sec/batch)
2016-11-28 18:51:37.830190: step 9550, loss = 0.37, acc = 0.97 (292.1 examples/sec; 0.438 sec/batch)
2016-11-28 18:51:42.624296: step 9560, loss = 0.31, acc = 1.00 (233.5 examples/sec; 0.548 sec/batch)
2016-11-28 18:51:47.165336: step 9570, loss = 0.32, acc = 0.99 (289.9 examples/sec; 0.441 sec/batch)
2016-11-28 18:51:51.907996: step 9580, loss = 0.33, acc = 0.99 (287.8 examples/sec; 0.445 sec/batch)
2016-11-28 18:51:56.733247: step 9590, loss = 0.35, acc = 0.98 (250.7 examples/sec; 0.511 sec/batch)
2016-11-28 18:52:01.420995: step 9600, loss = 0.31, acc = 0.99 (238.1 examples/sec; 0.538 sec/batch)
At step 9600 cross validation precision: 0.414
2016-11-28 18:52:07.067813: step 9610, loss = 0.32, acc = 0.99 (294.0 examples/sec; 0.435 sec/batch)
2016-11-28 18:52:11.914395: step 9620, loss = 0.34, acc = 0.98 (260.4 examples/sec; 0.492 sec/batch)
2016-11-28 18:52:16.494585: step 9630, loss = 0.37, acc = 0.98 (286.2 examples/sec; 0.447 sec/batch)
2016-11-28 18:52:21.269203: step 9640, loss = 0.33, acc = 0.98 (297.9 examples/sec; 0.430 sec/batch)
2016-11-28 18:52:26.076503: step 9650, loss = 0.36, acc = 0.98 (289.5 examples/sec; 0.442 sec/batch)
2016-11-28 18:52:30.857659: step 9660, loss = 0.44, acc = 0.96 (288.8 examples/sec; 0.443 sec/batch)
2016-11-28 18:52:35.641274: step 9670, loss = 0.31, acc = 1.00 (210.4 examples/sec; 0.608 sec/batch)
2016-11-28 18:52:40.243829: step 9680, loss = 0.33, acc = 1.00 (287.6 examples/sec; 0.445 sec/batch)
2016-11-28 18:52:45.000869: step 9690, loss = 0.37, acc = 0.97 (289.5 examples/sec; 0.442 sec/batch)
2016-11-28 18:52:49.768318: step 9700, loss = 0.33, acc = 0.98 (287.3 examples/sec; 0.445 sec/batch)
At step 9700 cross validation precision: 0.500
2016-11-28 18:52:55.301760: step 9710, loss = 0.36, acc = 0.98 (283.3 examples/sec; 0.452 sec/batch)
2016-11-28 18:53:00.144853: step 9720, loss = 0.33, acc = 0.99 (191.7 examples/sec; 0.668 sec/batch)
2016-11-28 18:53:04.929790: step 9730, loss = 0.37, acc = 0.97 (288.1 examples/sec; 0.444 sec/batch)
2016-11-28 18:53:09.595828: step 9740, loss = 0.33, acc = 0.98 (275.9 examples/sec; 0.464 sec/batch)
2016-11-28 18:53:14.292378: step 9750, loss = 0.34, acc = 0.99 (284.5 examples/sec; 0.450 sec/batch)
2016-11-28 18:53:18.989671: step 9760, loss = 0.33, acc = 0.98 (290.7 examples/sec; 0.440 sec/batch)
2016-11-28 18:53:23.804981: step 9770, loss = 0.34, acc = 0.98 (201.8 examples/sec; 0.634 sec/batch)
2016-11-28 18:53:28.230322: step 9780, loss = 0.34, acc = 0.98 (292.9 examples/sec; 0.437 sec/batch)
2016-11-28 18:53:32.995044: step 9790, loss = 0.33, acc = 1.00 (288.9 examples/sec; 0.443 sec/batch)
2016-11-28 18:53:37.744219: step 9800, loss = 0.38, acc = 0.98 (282.0 examples/sec; 0.454 sec/batch)
At step 9800 cross validation precision: 0.367
2016-11-28 18:53:43.613890: step 9810, loss = 0.37, acc = 0.98 (286.2 examples/sec; 0.447 sec/batch)
2016-11-28 18:53:48.417892: step 9820, loss = 0.35, acc = 0.98 (207.8 examples/sec; 0.616 sec/batch)
2016-11-28 18:53:53.143081: step 9830, loss = 0.36, acc = 0.98 (208.2 examples/sec; 0.615 sec/batch)
2016-11-28 18:53:57.678349: step 9840, loss = 0.38, acc = 0.98 (289.8 examples/sec; 0.442 sec/batch)
2016-11-28 18:54:02.447972: step 9850, loss = 0.36, acc = 0.98 (284.6 examples/sec; 0.450 sec/batch)
2016-11-28 18:54:07.103609: step 9860, loss = 0.33, acc = 0.98 (287.6 examples/sec; 0.445 sec/batch)
2016-11-28 18:54:11.900791: step 9870, loss = 0.43, acc = 0.96 (286.6 examples/sec; 0.447 sec/batch)
2016-11-28 18:54:16.490661: step 9880, loss = 0.44, acc = 0.95 (288.8 examples/sec; 0.443 sec/batch)
2016-11-28 18:54:21.289260: step 9890, loss = 0.32, acc = 0.99 (296.4 examples/sec; 0.432 sec/batch)
2016-11-28 18:54:26.038191: step 9900, loss = 0.34, acc = 0.99 (295.9 examples/sec; 0.433 sec/batch)
At step 9900 cross validation precision: 0.438
2016-11-28 18:54:31.786467: step 9910, loss = 0.35, acc = 0.97 (284.9 examples/sec; 0.449 sec/batch)
2016-11-28 18:54:36.354460: step 9920, loss = 0.38, acc = 0.97 (286.5 examples/sec; 0.447 sec/batch)
2016-11-28 18:54:40.983429: step 9930, loss = 0.45, acc = 0.95 (293.0 examples/sec; 0.437 sec/batch)
2016-11-28 18:54:45.867834: step 9940, loss = 0.37, acc = 0.97 (291.0 examples/sec; 0.440 sec/batch)
2016-11-28 18:54:50.483659: step 9950, loss = 0.36, acc = 0.96 (288.1 examples/sec; 0.444 sec/batch)
2016-11-28 18:54:55.268456: step 9960, loss = 0.32, acc = 1.00 (291.8 examples/sec; 0.439 sec/batch)
2016-11-28 18:54:59.942154: step 9970, loss = 0.32, acc = 0.98 (239.1 examples/sec; 0.535 sec/batch)
2016-11-28 18:55:04.681128: step 9980, loss = 0.32, acc = 0.99 (286.6 examples/sec; 0.447 sec/batch)
2016-11-28 18:55:09.392778: step 9990, loss = 0.35, acc = 0.98 (290.6 examples/sec; 0.440 sec/batch)
2016-11-28 18:55:14.036042: step 10000, loss = 0.36, acc = 0.98 (294.6 examples/sec; 0.435 sec/batch)
At step 10000 cross validation precision: 0.414
2016-11-28 18:55:19.701769: step 10010, loss = 0.37, acc = 0.97 (289.3 examples/sec; 0.442 sec/batch)
2016-11-28 18:55:24.615768: step 10020, loss = 0.35, acc = 0.97 (288.2 examples/sec; 0.444 sec/batch)
2016-11-28 18:55:29.156314: step 10030, loss = 0.41, acc = 0.94 (283.3 examples/sec; 0.452 sec/batch)
2016-11-28 18:55:34.041782: step 10040, loss = 0.41, acc = 0.96 (280.3 examples/sec; 0.457 sec/batch)
2016-11-28 18:55:38.649473: step 10050, loss = 0.37, acc = 0.98 (296.9 examples/sec; 0.431 sec/batch)
2016-11-28 18:55:43.407248: step 10060, loss = 0.36, acc = 0.98 (221.0 examples/sec; 0.579 sec/batch)
2016-11-28 18:55:48.199439: step 10070, loss = 0.38, acc = 0.95 (244.7 examples/sec; 0.523 sec/batch)
2016-11-28 18:55:53.004258: step 10080, loss = 0.32, acc = 0.99 (295.5 examples/sec; 0.433 sec/batch)
2016-11-28 18:55:57.626974: step 10090, loss = 0.38, acc = 0.96 (283.3 examples/sec; 0.452 sec/batch)
2016-11-28 18:56:02.545288: step 10100, loss = 0.39, acc = 0.97 (210.9 examples/sec; 0.607 sec/batch)
At step 10100 cross validation precision: 0.391
2016-11-28 18:56:08.307933: step 10110, loss = 0.36, acc = 0.98 (201.7 examples/sec; 0.635 sec/batch)
2016-11-28 18:56:12.912045: step 10120, loss = 0.32, acc = 0.99 (241.0 examples/sec; 0.531 sec/batch)
2016-11-28 18:56:17.549111: step 10130, loss = 0.31, acc = 0.99 (290.4 examples/sec; 0.441 sec/batch)
2016-11-28 18:56:22.334456: step 10140, loss = 0.37, acc = 0.97 (283.8 examples/sec; 0.451 sec/batch)
2016-11-28 18:56:27.171491: step 10150, loss = 0.35, acc = 0.98 (247.5 examples/sec; 0.517 sec/batch)
2016-11-28 18:56:31.723498: step 10160, loss = 0.36, acc = 0.98 (289.9 examples/sec; 0.441 sec/batch)
2016-11-28 18:56:36.453505: step 10170, loss = 0.34, acc = 0.98 (284.8 examples/sec; 0.449 sec/batch)
2016-11-28 18:56:41.185297: step 10180, loss = 0.39, acc = 0.97 (211.6 examples/sec; 0.605 sec/batch)
2016-11-28 18:56:45.778602: step 10190, loss = 0.39, acc = 0.98 (290.8 examples/sec; 0.440 sec/batch)
2016-11-28 18:56:50.577515: step 10200, loss = 0.34, acc = 0.98 (283.0 examples/sec; 0.452 sec/batch)
At step 10200 cross validation precision: 0.430
2016-11-28 18:56:56.252113: step 10210, loss = 0.31, acc = 1.00 (286.4 examples/sec; 0.447 sec/batch)
2016-11-28 18:57:01.001637: step 10220, loss = 0.31, acc = 1.00 (292.6 examples/sec; 0.437 sec/batch)
2016-11-28 18:57:05.570078: step 10230, loss = 0.37, acc = 0.98 (279.3 examples/sec; 0.458 sec/batch)
2016-11-28 18:57:10.329100: step 10240, loss = 0.41, acc = 0.98 (286.5 examples/sec; 0.447 sec/batch)
2016-11-28 18:57:14.951568: step 10250, loss = 0.33, acc = 0.99 (287.6 examples/sec; 0.445 sec/batch)
2016-11-28 18:57:19.757112: step 10260, loss = 0.31, acc = 0.99 (247.3 examples/sec; 0.518 sec/batch)
2016-11-28 18:57:24.438578: step 10270, loss = 0.37, acc = 0.98 (282.6 examples/sec; 0.453 sec/batch)
2016-11-28 18:57:29.195378: step 10280, loss = 0.38, acc = 0.99 (296.4 examples/sec; 0.432 sec/batch)
2016-11-28 18:57:33.922989: step 10290, loss = 0.36, acc = 0.97 (290.5 examples/sec; 0.441 sec/batch)
2016-11-28 18:57:38.672904: step 10300, loss = 0.31, acc = 0.99 (290.9 examples/sec; 0.440 sec/batch)
At step 10300 cross validation precision: 0.344
2016-11-28 18:57:44.451380: step 10310, loss = 0.41, acc = 0.97 (290.6 examples/sec; 0.440 sec/batch)
2016-11-28 18:57:49.187642: step 10320, loss = 0.37, acc = 0.98 (253.5 examples/sec; 0.505 sec/batch)
2016-11-28 18:57:53.905837: step 10330, loss = 0.35, acc = 0.98 (289.2 examples/sec; 0.443 sec/batch)
2016-11-28 18:57:58.596352: step 10340, loss = 0.34, acc = 0.98 (231.9 examples/sec; 0.552 sec/batch)
2016-11-28 18:58:03.277070: step 10350, loss = 0.33, acc = 0.98 (242.1 examples/sec; 0.529 sec/batch)
2016-11-28 18:58:08.017013: step 10360, loss = 0.35, acc = 0.99 (239.6 examples/sec; 0.534 sec/batch)
2016-11-28 18:58:12.758630: step 10370, loss = 0.39, acc = 0.98 (290.3 examples/sec; 0.441 sec/batch)
2016-11-28 18:58:17.594436: step 10380, loss = 0.33, acc = 1.00 (282.8 examples/sec; 0.453 sec/batch)
2016-11-28 18:58:22.491141: step 10390, loss = 0.38, acc = 0.99 (248.5 examples/sec; 0.515 sec/batch)
2016-11-28 18:58:27.067343: step 10400, loss = 0.46, acc = 0.96 (287.5 examples/sec; 0.445 sec/batch)
At step 10400 cross validation precision: 0.469
2016-11-28 18:58:32.679264: step 10410, loss = 0.37, acc = 0.96 (291.2 examples/sec; 0.440 sec/batch)
2016-11-28 18:58:37.484747: step 10420, loss = 0.40, acc = 0.96 (241.5 examples/sec; 0.530 sec/batch)
2016-11-28 18:58:42.315328: step 10430, loss = 0.46, acc = 0.95 (236.7 examples/sec; 0.541 sec/batch)
2016-11-28 18:58:47.026169: step 10440, loss = 0.46, acc = 0.93 (287.9 examples/sec; 0.445 sec/batch)
2016-11-28 18:58:51.747708: step 10450, loss = 0.40, acc = 0.98 (285.2 examples/sec; 0.449 sec/batch)
2016-11-28 18:58:56.621243: step 10460, loss = 0.40, acc = 0.96 (224.6 examples/sec; 0.570 sec/batch)
2016-11-28 18:59:01.326133: step 10470, loss = 0.42, acc = 0.95 (288.5 examples/sec; 0.444 sec/batch)
2016-11-28 18:59:06.131974: step 10480, loss = 0.50, acc = 0.92 (296.1 examples/sec; 0.432 sec/batch)
2016-11-28 18:59:10.714216: step 10490, loss = 0.44, acc = 0.96 (286.1 examples/sec; 0.447 sec/batch)
2016-11-28 18:59:15.456356: step 10500, loss = 0.38, acc = 0.98 (297.9 examples/sec; 0.430 sec/batch)
At step 10500 cross validation precision: 0.438
2016-11-28 18:59:21.170859: step 10510, loss = 0.33, acc = 0.98 (253.5 examples/sec; 0.505 sec/batch)
2016-11-28 18:59:25.769426: step 10520, loss = 0.33, acc = 1.00 (291.2 examples/sec; 0.440 sec/batch)
2016-11-28 18:59:30.572387: step 10530, loss = 0.32, acc = 1.00 (240.0 examples/sec; 0.533 sec/batch)
2016-11-28 18:59:35.336711: step 10540, loss = 0.31, acc = 1.00 (292.8 examples/sec; 0.437 sec/batch)
2016-11-28 18:59:40.119781: step 10550, loss = 0.38, acc = 0.97 (298.3 examples/sec; 0.429 sec/batch)
2016-11-28 18:59:44.833592: step 10560, loss = 0.37, acc = 0.97 (234.1 examples/sec; 0.547 sec/batch)
2016-11-28 18:59:49.504956: step 10570, loss = 0.32, acc = 1.00 (284.8 examples/sec; 0.449 sec/batch)
2016-11-28 18:59:54.106506: step 10580, loss = 0.40, acc = 0.97 (286.1 examples/sec; 0.447 sec/batch)
2016-11-28 18:59:58.909376: step 10590, loss = 0.33, acc = 0.99 (282.0 examples/sec; 0.454 sec/batch)
2016-11-28 19:00:03.680019: step 10600, loss = 0.41, acc = 0.95 (286.0 examples/sec; 0.448 sec/batch)
At step 10600 cross validation precision: 0.406
2016-11-28 19:00:09.539990: step 10610, loss = 0.35, acc = 0.98 (210.7 examples/sec; 0.608 sec/batch)
2016-11-28 19:00:14.205458: step 10620, loss = 0.35, acc = 0.98 (230.4 examples/sec; 0.555 sec/batch)
2016-11-28 19:00:19.069136: step 10630, loss = 0.34, acc = 0.99 (219.3 examples/sec; 0.584 sec/batch)
2016-11-28 19:00:23.649199: step 10640, loss = 0.35, acc = 0.98 (289.1 examples/sec; 0.443 sec/batch)
2016-11-28 19:00:28.580965: step 10650, loss = 0.32, acc = 0.99 (222.5 examples/sec; 0.575 sec/batch)
2016-11-28 19:00:33.211396: step 10660, loss = 0.34, acc = 0.98 (288.2 examples/sec; 0.444 sec/batch)
2016-11-28 19:00:37.931450: step 10670, loss = 0.33, acc = 0.99 (290.7 examples/sec; 0.440 sec/batch)
2016-11-28 19:00:42.517558: step 10680, loss = 0.32, acc = 0.98 (288.1 examples/sec; 0.444 sec/batch)
2016-11-28 19:00:47.355419: step 10690, loss = 0.32, acc = 0.98 (291.3 examples/sec; 0.439 sec/batch)
2016-11-28 19:00:52.162381: step 10700, loss = 0.32, acc = 0.99 (294.4 examples/sec; 0.435 sec/batch)
At step 10700 cross validation precision: 0.391
2016-11-28 19:00:57.854753: step 10710, loss = 0.33, acc = 0.98 (236.4 examples/sec; 0.541 sec/batch)
2016-11-28 19:01:02.451630: step 10720, loss = 0.33, acc = 0.98 (287.5 examples/sec; 0.445 sec/batch)
2016-11-28 19:01:07.072355: step 10730, loss = 0.34, acc = 0.99 (287.5 examples/sec; 0.445 sec/batch)
2016-11-28 19:01:12.032434: step 10740, loss = 0.31, acc = 1.00 (287.8 examples/sec; 0.445 sec/batch)
2016-11-28 19:01:16.680476: step 10750, loss = 0.33, acc = 0.99 (288.7 examples/sec; 0.443 sec/batch)
2016-11-28 19:01:21.450370: step 10760, loss = 0.33, acc = 0.98 (290.2 examples/sec; 0.441 sec/batch)
2016-11-28 19:01:26.174260: step 10770, loss = 0.36, acc = 0.98 (229.3 examples/sec; 0.558 sec/batch)
2016-11-28 19:01:30.794472: step 10780, loss = 0.32, acc = 0.98 (287.9 examples/sec; 0.445 sec/batch)
2016-11-28 19:01:35.597323: step 10790, loss = 0.32, acc = 0.99 (287.0 examples/sec; 0.446 sec/batch)
2016-11-28 19:01:40.314704: step 10800, loss = 0.31, acc = 0.99 (287.7 examples/sec; 0.445 sec/batch)
At step 10800 cross validation precision: 0.391
2016-11-28 19:01:45.991411: step 10810, loss = 0.43, acc = 0.97 (289.7 examples/sec; 0.442 sec/batch)
2016-11-28 19:01:50.660860: step 10820, loss = 0.36, acc = 0.98 (286.3 examples/sec; 0.447 sec/batch)
2016-11-28 19:01:55.498160: step 10830, loss = 0.32, acc = 0.99 (287.6 examples/sec; 0.445 sec/batch)
2016-11-28 19:02:00.293515: step 10840, loss = 0.34, acc = 0.99 (285.4 examples/sec; 0.448 sec/batch)
2016-11-28 19:02:05.212432: step 10850, loss = 0.34, acc = 0.98 (266.0 examples/sec; 0.481 sec/batch)
2016-11-28 19:02:09.745035: step 10860, loss = 0.39, acc = 0.96 (285.1 examples/sec; 0.449 sec/batch)
2016-11-28 19:02:14.414263: step 10870, loss = 0.32, acc = 1.00 (284.5 examples/sec; 0.450 sec/batch)
2016-11-28 19:02:19.063772: step 10880, loss = 0.35, acc = 0.97 (291.7 examples/sec; 0.439 sec/batch)
2016-11-28 19:02:23.883287: step 10890, loss = 0.32, acc = 0.99 (288.6 examples/sec; 0.444 sec/batch)
2016-11-28 19:02:28.480738: step 10900, loss = 0.31, acc = 0.99 (287.9 examples/sec; 0.445 sec/batch)
At step 10900 cross validation precision: 0.398
2016-11-28 19:02:34.306505: step 10910, loss = 0.32, acc = 0.98 (280.6 examples/sec; 0.456 sec/batch)
2016-11-28 19:02:38.950923: step 10920, loss = 0.31, acc = 0.98 (287.8 examples/sec; 0.445 sec/batch)
2016-11-28 19:02:43.687563: step 10930, loss = 0.34, acc = 0.98 (287.9 examples/sec; 0.445 sec/batch)
2016-11-28 19:02:48.465964: step 10940, loss = 0.34, acc = 0.98 (209.4 examples/sec; 0.611 sec/batch)
2016-11-28 19:02:53.026487: step 10950, loss = 0.32, acc = 0.99 (291.8 examples/sec; 0.439 sec/batch)
2016-11-28 19:02:57.974477: step 10960, loss = 0.31, acc = 1.00 (246.4 examples/sec; 0.520 sec/batch)
2016-11-28 19:03:02.852799: step 10970, loss = 0.30, acc = 0.99 (245.8 examples/sec; 0.521 sec/batch)
2016-11-28 19:03:07.409523: step 10980, loss = 0.31, acc = 0.99 (286.6 examples/sec; 0.447 sec/batch)
2016-11-28 19:03:12.053128: step 10990, loss = 0.47, acc = 0.97 (261.8 examples/sec; 0.489 sec/batch)
2016-11-28 19:03:16.844127: step 11000, loss = 0.34, acc = 0.98 (293.6 examples/sec; 0.436 sec/batch)
At step 11000 cross validation precision: 0.398
2016-11-28 19:03:22.460021: step 11010, loss = 0.33, acc = 0.98 (290.3 examples/sec; 0.441 sec/batch)
2016-11-28 19:03:27.268502: step 11020, loss = 0.33, acc = 0.98 (292.5 examples/sec; 0.438 sec/batch)
2016-11-28 19:03:31.849996: step 11030, loss = 0.39, acc = 0.95 (283.2 examples/sec; 0.452 sec/batch)
2016-11-28 19:03:36.608369: step 11040, loss = 0.38, acc = 0.97 (286.7 examples/sec; 0.446 sec/batch)
2016-11-28 19:03:41.498955: step 11050, loss = 0.40, acc = 0.97 (227.4 examples/sec; 0.563 sec/batch)
2016-11-28 19:03:46.301114: step 11060, loss = 0.40, acc = 0.97 (290.5 examples/sec; 0.441 sec/batch)
2016-11-28 19:03:50.897851: step 11070, loss = 0.31, acc = 1.00 (289.7 examples/sec; 0.442 sec/batch)
2016-11-28 19:03:55.693700: step 11080, loss = 0.33, acc = 0.99 (282.7 examples/sec; 0.453 sec/batch)
2016-11-28 19:04:00.421534: step 11090, loss = 0.36, acc = 0.99 (214.1 examples/sec; 0.598 sec/batch)
2016-11-28 19:04:05.037396: step 11100, loss = 0.39, acc = 0.97 (289.0 examples/sec; 0.443 sec/batch)
At step 11100 cross validation precision: 0.297
2016-11-28 19:04:10.743667: step 11110, loss = 0.37, acc = 0.98 (287.9 examples/sec; 0.445 sec/batch)
2016-11-28 19:04:15.363091: step 11120, loss = 0.34, acc = 0.99 (251.7 examples/sec; 0.509 sec/batch)
2016-11-28 19:04:20.118626: step 11130, loss = 0.33, acc = 1.00 (288.5 examples/sec; 0.444 sec/batch)
2016-11-28 19:04:24.909346: step 11140, loss = 0.32, acc = 0.99 (297.3 examples/sec; 0.431 sec/batch)
2016-11-28 19:04:29.676438: step 11150, loss = 0.32, acc = 0.99 (283.8 examples/sec; 0.451 sec/batch)
2016-11-28 19:04:34.468895: step 11160, loss = 0.31, acc = 1.00 (286.9 examples/sec; 0.446 sec/batch)
2016-11-28 19:04:39.106500: step 11170, loss = 0.33, acc = 0.98 (283.3 examples/sec; 0.452 sec/batch)
2016-11-28 19:04:43.907439: step 11180, loss = 0.31, acc = 0.99 (287.4 examples/sec; 0.445 sec/batch)
2016-11-28 19:04:48.641085: step 11190, loss = 0.31, acc = 0.99 (286.1 examples/sec; 0.447 sec/batch)
2016-11-28 19:04:53.413549: step 11200, loss = 0.34, acc = 0.98 (287.7 examples/sec; 0.445 sec/batch)
At step 11200 cross validation precision: 0.383
2016-11-28 19:04:58.998535: step 11210, loss = 0.37, acc = 0.98 (291.4 examples/sec; 0.439 sec/batch)
2016-11-28 19:05:03.793218: step 11220, loss = 0.34, acc = 0.98 (283.1 examples/sec; 0.452 sec/batch)
2016-11-28 19:05:08.572732: step 11230, loss = 0.33, acc = 0.98 (207.9 examples/sec; 0.616 sec/batch)
2016-11-28 19:05:13.286101: step 11240, loss = 0.32, acc = 0.99 (282.7 examples/sec; 0.453 sec/batch)
2016-11-28 19:05:18.059790: step 11250, loss = 0.32, acc = 0.99 (288.0 examples/sec; 0.444 sec/batch)
2016-11-28 19:05:22.673563: step 11260, loss = 0.31, acc = 1.00 (287.8 examples/sec; 0.445 sec/batch)
2016-11-28 19:05:27.539084: step 11270, loss = 0.31, acc = 0.99 (243.8 examples/sec; 0.525 sec/batch)
2016-11-28 19:05:32.122869: step 11280, loss = 0.29, acc = 1.00 (271.8 examples/sec; 0.471 sec/batch)
2016-11-28 19:05:36.829281: step 11290, loss = 0.29, acc = 1.00 (287.5 examples/sec; 0.445 sec/batch)
2016-11-28 19:05:41.467669: step 11300, loss = 0.30, acc = 0.99 (286.2 examples/sec; 0.447 sec/batch)
At step 11300 cross validation precision: 0.391
2016-11-28 19:05:47.009101: step 11310, loss = 0.30, acc = 0.99 (284.8 examples/sec; 0.449 sec/batch)
2016-11-28 19:05:51.768468: step 11320, loss = 0.29, acc = 1.00 (290.6 examples/sec; 0.440 sec/batch)
2016-11-28 19:05:56.690985: step 11330, loss = 0.31, acc = 0.99 (209.4 examples/sec; 0.611 sec/batch)
2016-11-28 19:06:01.360079: step 11340, loss = 0.29, acc = 1.00 (260.9 examples/sec; 0.491 sec/batch)
2016-11-28 19:06:06.109139: step 11350, loss = 0.33, acc = 0.98 (291.8 examples/sec; 0.439 sec/batch)
2016-11-28 19:06:10.798235: step 11360, loss = 0.36, acc = 0.97 (290.3 examples/sec; 0.441 sec/batch)
2016-11-28 19:06:15.647493: step 11370, loss = 0.31, acc = 0.98 (209.0 examples/sec; 0.612 sec/batch)
2016-11-28 19:06:20.413375: step 11380, loss = 0.28, acc = 1.00 (290.2 examples/sec; 0.441 sec/batch)
2016-11-28 19:06:24.964077: step 11390, loss = 0.28, acc = 1.00 (289.0 examples/sec; 0.443 sec/batch)
2016-11-28 19:06:29.740964: step 11400, loss = 0.31, acc = 0.98 (291.4 examples/sec; 0.439 sec/batch)
At step 11400 cross validation precision: 0.398
2016-11-28 19:06:35.615872: step 11410, loss = 0.31, acc = 0.98 (284.6 examples/sec; 0.450 sec/batch)
2016-11-28 19:06:40.374085: step 11420, loss = 0.28, acc = 1.00 (286.0 examples/sec; 0.447 sec/batch)
2016-11-28 19:06:45.134480: step 11430, loss = 0.28, acc = 1.00 (239.7 examples/sec; 0.534 sec/batch)
2016-11-28 19:06:49.645284: step 11440, loss = 0.29, acc = 1.00 (289.0 examples/sec; 0.443 sec/batch)
2016-11-28 19:06:54.236070: step 11450, loss = 0.29, acc = 0.99 (238.4 examples/sec; 0.537 sec/batch)
2016-11-28 19:06:58.902685: step 11460, loss = 0.30, acc = 1.00 (289.7 examples/sec; 0.442 sec/batch)
2016-11-28 19:07:03.712737: step 11470, loss = 0.29, acc = 0.99 (283.8 examples/sec; 0.451 sec/batch)
2016-11-28 19:07:08.562236: step 11480, loss = 0.35, acc = 0.98 (202.9 examples/sec; 0.631 sec/batch)
2016-11-28 19:07:13.088975: step 11490, loss = 0.34, acc = 0.98 (291.9 examples/sec; 0.438 sec/batch)
2016-11-28 19:07:17.866675: step 11500, loss = 0.33, acc = 0.98 (289.2 examples/sec; 0.443 sec/batch)
At step 11500 cross validation precision: 0.344
2016-11-28 19:07:23.429138: step 11510, loss = 0.29, acc = 0.99 (289.3 examples/sec; 0.442 sec/batch)
2016-11-28 19:07:28.239481: step 11520, loss = 0.27, acc = 1.00 (287.1 examples/sec; 0.446 sec/batch)
2016-11-28 19:07:33.011011: step 11530, loss = 0.32, acc = 0.98 (287.3 examples/sec; 0.446 sec/batch)
2016-11-28 19:07:37.748400: step 11540, loss = 3.22, acc = 0.51 (298.7 examples/sec; 0.428 sec/batch)
2016-11-28 19:07:42.377773: step 11550, loss = 1.15, acc = 0.70 (301.3 examples/sec; 0.425 sec/batch)
2016-11-28 19:07:47.000508: step 11560, loss = 0.79, acc = 0.81 (289.3 examples/sec; 0.442 sec/batch)
2016-11-28 19:07:51.739120: step 11570, loss = 0.67, acc = 0.86 (288.5 examples/sec; 0.444 sec/batch)
2016-11-28 19:07:56.484899: step 11580, loss = 0.62, acc = 0.89 (277.4 examples/sec; 0.461 sec/batch)
2016-11-28 19:08:01.323960: step 11590, loss = 0.53, acc = 0.95 (227.0 examples/sec; 0.564 sec/batch)
2016-11-28 19:08:06.026160: step 11600, loss = 0.57, acc = 0.93 (286.3 examples/sec; 0.447 sec/batch)
At step 11600 cross validation precision: 0.305
2016-11-28 19:08:11.790498: step 11610, loss = 0.50, acc = 0.95 (276.8 examples/sec; 0.462 sec/batch)
2016-11-28 19:08:16.402020: step 11620, loss = 0.50, acc = 0.93 (284.5 examples/sec; 0.450 sec/batch)
2016-11-28 19:08:21.099114: step 11630, loss = 0.42, acc = 0.97 (234.5 examples/sec; 0.546 sec/batch)
2016-11-28 19:08:25.965195: step 11640, loss = 0.50, acc = 0.95 (288.2 examples/sec; 0.444 sec/batch)
2016-11-28 19:08:30.646592: step 11650, loss = 0.52, acc = 0.91 (290.0 examples/sec; 0.441 sec/batch)
2016-11-28 19:08:35.388503: step 11660, loss = 0.42, acc = 0.97 (287.1 examples/sec; 0.446 sec/batch)
2016-11-28 19:08:40.208463: step 11670, loss = 0.52, acc = 0.95 (294.2 examples/sec; 0.435 sec/batch)
2016-11-28 19:08:44.800817: step 11680, loss = 0.42, acc = 0.97 (288.4 examples/sec; 0.444 sec/batch)
2016-11-28 19:08:49.641007: step 11690, loss = 0.43, acc = 0.98 (202.2 examples/sec; 0.633 sec/batch)
2016-11-28 19:08:54.391418: step 11700, loss = 0.54, acc = 0.95 (286.8 examples/sec; 0.446 sec/batch)
At step 11700 cross validation precision: 0.406
2016-11-28 19:09:00.186241: step 11710, loss = 0.45, acc = 0.95 (203.2 examples/sec; 0.630 sec/batch)
2016-11-28 19:09:04.889132: step 11720, loss = 0.46, acc = 0.97 (239.3 examples/sec; 0.535 sec/batch)
2016-11-28 19:09:09.511467: step 11730, loss = 0.38, acc = 0.99 (288.0 examples/sec; 0.444 sec/batch)
2016-11-28 19:09:14.325187: step 11740, loss = 0.43, acc = 0.96 (239.1 examples/sec; 0.535 sec/batch)
2016-11-28 19:09:19.042376: step 11750, loss = 0.39, acc = 0.99 (257.4 examples/sec; 0.497 sec/batch)
2016-11-28 19:09:23.614378: step 11760, loss = 0.39, acc = 0.98 (292.0 examples/sec; 0.438 sec/batch)
2016-11-28 19:09:28.333609: step 11770, loss = 0.41, acc = 0.96 (292.5 examples/sec; 0.438 sec/batch)
2016-11-28 19:09:33.065328: step 11780, loss = 0.47, acc = 0.96 (299.0 examples/sec; 0.428 sec/batch)
2016-11-28 19:09:37.784113: step 11790, loss = 0.38, acc = 0.98 (218.0 examples/sec; 0.587 sec/batch)
2016-11-28 19:09:42.647607: step 11800, loss = 0.38, acc = 0.98 (218.2 examples/sec; 0.587 sec/batch)
At step 11800 cross validation precision: 0.406
2016-11-28 19:09:48.219139: step 11810, loss = 0.42, acc = 0.98 (237.3 examples/sec; 0.539 sec/batch)
2016-11-28 19:09:53.002570: step 11820, loss = 0.43, acc = 0.95 (250.8 examples/sec; 0.510 sec/batch)
2016-11-28 19:09:57.767388: step 11830, loss = 0.39, acc = 0.98 (261.6 examples/sec; 0.489 sec/batch)
2016-11-28 19:10:02.325357: step 11840, loss = 0.37, acc = 0.98 (283.0 examples/sec; 0.452 sec/batch)
2016-11-28 19:10:07.271019: step 11850, loss = 0.37, acc = 1.00 (210.0 examples/sec; 0.609 sec/batch)
2016-11-28 19:10:11.960378: step 11860, loss = 0.46, acc = 0.97 (209.9 examples/sec; 0.610 sec/batch)
2016-11-28 19:10:16.622446: step 11870, loss = 0.42, acc = 0.97 (290.2 examples/sec; 0.441 sec/batch)
2016-11-28 19:10:21.368456: step 11880, loss = 0.41, acc = 0.98 (223.4 examples/sec; 0.573 sec/batch)
2016-11-28 19:10:26.045222: step 11890, loss = 0.38, acc = 0.98 (227.7 examples/sec; 0.562 sec/batch)
2016-11-28 19:10:30.722781: step 11900, loss = 0.44, acc = 0.96 (286.5 examples/sec; 0.447 sec/batch)
At step 11900 cross validation precision: 0.375
2016-11-28 19:10:36.398803: step 11910, loss = 0.37, acc = 0.98 (287.4 examples/sec; 0.445 sec/batch)
2016-11-28 19:10:41.187159: step 11920, loss = 0.38, acc = 0.98 (278.3 examples/sec; 0.460 sec/batch)
2016-11-28 19:10:45.876346: step 11930, loss = 0.39, acc = 0.99 (225.2 examples/sec; 0.568 sec/batch)
2016-11-28 19:10:50.590674: step 11940, loss = 0.35, acc = 1.00 (294.4 examples/sec; 0.435 sec/batch)
2016-11-28 19:10:55.143999: step 11950, loss = 0.38, acc = 0.99 (292.3 examples/sec; 0.438 sec/batch)
2016-11-28 19:10:59.996009: step 11960, loss = 0.42, acc = 0.96 (273.7 examples/sec; 0.468 sec/batch)
2016-11-28 19:11:04.708454: step 11970, loss = 0.41, acc = 0.96 (289.6 examples/sec; 0.442 sec/batch)
2016-11-28 19:11:09.358867: step 11980, loss = 0.39, acc = 0.98 (261.4 examples/sec; 0.490 sec/batch)
2016-11-28 19:11:14.089339: step 11990, loss = 0.37, acc = 1.00 (290.2 examples/sec; 0.441 sec/batch)
2016-11-28 19:11:19.040098: step 12000, loss = 0.37, acc = 0.98 (233.0 examples/sec; 0.549 sec/batch)
At step 12000 cross validation precision: 0.422
2016-11-28 19:11:24.666728: step 12010, loss = 0.35, acc = 0.99 (218.8 examples/sec; 0.585 sec/batch)
2016-11-28 19:11:29.459265: step 12020, loss = 0.39, acc = 0.98 (229.3 examples/sec; 0.558 sec/batch)
2016-11-28 19:11:34.074110: step 12030, loss = 0.41, acc = 0.98 (288.8 examples/sec; 0.443 sec/batch)
2016-11-28 19:11:38.906170: step 12040, loss = 0.42, acc = 0.98 (293.3 examples/sec; 0.436 sec/batch)
2016-11-28 19:11:43.692657: step 12050, loss = 0.39, acc = 0.98 (229.9 examples/sec; 0.557 sec/batch)
2016-11-28 19:11:48.465332: step 12060, loss = 0.43, acc = 0.96 (245.2 examples/sec; 0.522 sec/batch)
2016-11-28 19:11:53.268228: step 12070, loss = 0.39, acc = 0.98 (288.8 examples/sec; 0.443 sec/batch)
2016-11-28 19:11:57.967126: step 12080, loss = 0.35, acc = 0.99 (207.0 examples/sec; 0.618 sec/batch)
2016-11-28 19:12:02.544196: step 12090, loss = 0.40, acc = 0.98 (292.2 examples/sec; 0.438 sec/batch)
2016-11-28 19:12:07.402914: step 12100, loss = 0.35, acc = 1.00 (234.8 examples/sec; 0.545 sec/batch)
At step 12100 cross validation precision: 0.453
2016-11-28 19:12:12.986525: step 12110, loss = 0.36, acc = 0.99 (287.7 examples/sec; 0.445 sec/batch)
2016-11-28 19:12:17.641036: step 12120, loss = 0.36, acc = 0.98 (281.0 examples/sec; 0.455 sec/batch)
2016-11-28 19:12:22.414257: step 12130, loss = 0.37, acc = 0.98 (289.0 examples/sec; 0.443 sec/batch)
2016-11-28 19:12:27.044396: step 12140, loss = 0.36, acc = 0.99 (241.3 examples/sec; 0.531 sec/batch)
2016-11-28 19:12:31.706400: step 12150, loss = 0.39, acc = 0.98 (292.9 examples/sec; 0.437 sec/batch)
2016-11-28 19:12:36.651776: step 12160, loss = 0.38, acc = 0.97 (257.3 examples/sec; 0.497 sec/batch)
2016-11-28 19:12:41.209468: step 12170, loss = 0.34, acc = 0.99 (293.2 examples/sec; 0.437 sec/batch)
2016-11-28 19:12:46.074939: step 12180, loss = 0.42, acc = 0.96 (286.9 examples/sec; 0.446 sec/batch)
2016-11-28 19:12:50.786608: step 12190, loss = 0.36, acc = 0.98 (286.7 examples/sec; 0.446 sec/batch)
2016-11-28 19:12:55.485939: step 12200, loss = 0.34, acc = 0.99 (285.6 examples/sec; 0.448 sec/batch)
At step 12200 cross validation precision: 0.398
2016-11-28 19:13:01.316858: step 12210, loss = 0.32, acc = 1.00 (251.2 examples/sec; 0.509 sec/batch)
2016-11-28 19:13:05.823458: step 12220, loss = 0.34, acc = 0.99 (284.4 examples/sec; 0.450 sec/batch)
2016-11-28 19:13:10.648632: step 12230, loss = 0.33, acc = 1.00 (286.8 examples/sec; 0.446 sec/batch)
2016-11-28 19:13:15.480383: step 12240, loss = 0.34, acc = 0.99 (202.7 examples/sec; 0.631 sec/batch)
2016-11-28 19:13:20.138289: step 12250, loss = 0.35, acc = 0.98 (291.0 examples/sec; 0.440 sec/batch)
2016-11-28 19:13:24.860067: step 12260, loss = 0.34, acc = 0.99 (227.5 examples/sec; 0.563 sec/batch)
2016-11-28 19:13:29.481190: step 12270, loss = 0.34, acc = 0.99 (282.1 examples/sec; 0.454 sec/batch)
2016-11-28 19:13:34.260228: step 12280, loss = 0.43, acc = 0.98 (278.2 examples/sec; 0.460 sec/batch)
2016-11-28 19:13:38.970477: step 12290, loss = 0.35, acc = 0.99 (290.8 examples/sec; 0.440 sec/batch)
2016-11-28 19:13:43.680871: step 12300, loss = 0.32, acc = 1.00 (211.4 examples/sec; 0.606 sec/batch)
At step 12300 cross validation precision: 0.344
2016-11-28 19:13:49.277144: step 12310, loss = 0.34, acc = 0.99 (290.4 examples/sec; 0.441 sec/batch)
2016-11-28 19:13:54.040256: step 12320, loss = 0.37, acc = 0.97 (290.9 examples/sec; 0.440 sec/batch)
2016-11-28 19:13:58.796190: step 12330, loss = 0.33, acc = 0.98 (289.1 examples/sec; 0.443 sec/batch)
2016-11-28 19:14:03.630343: step 12340, loss = 0.36, acc = 0.98 (214.4 examples/sec; 0.597 sec/batch)
2016-11-28 19:14:08.314705: step 12350, loss = 0.45, acc = 0.95 (237.3 examples/sec; 0.539 sec/batch)
2016-11-28 19:14:13.121141: step 12360, loss = 0.34, acc = 0.98 (253.4 examples/sec; 0.505 sec/batch)
2016-11-28 19:14:17.707959: step 12370, loss = 0.35, acc = 0.99 (294.9 examples/sec; 0.434 sec/batch)
2016-11-28 19:14:22.607071: step 12380, loss = 0.39, acc = 0.98 (294.0 examples/sec; 0.435 sec/batch)
2016-11-28 19:14:27.319234: step 12390, loss = 0.34, acc = 0.99 (222.8 examples/sec; 0.574 sec/batch)
2016-11-28 19:14:32.032980: step 12400, loss = 0.38, acc = 0.96 (223.3 examples/sec; 0.573 sec/batch)
At step 12400 cross validation precision: 0.352
2016-11-28 19:14:37.674784: step 12410, loss = 0.37, acc = 0.97 (292.3 examples/sec; 0.438 sec/batch)
2016-11-28 19:14:42.350451: step 12420, loss = 0.37, acc = 0.97 (283.7 examples/sec; 0.451 sec/batch)
2016-11-28 19:14:47.105158: step 12430, loss = 0.42, acc = 0.95 (296.3 examples/sec; 0.432 sec/batch)
2016-11-28 19:14:51.687223: step 12440, loss = 0.39, acc = 0.97 (285.6 examples/sec; 0.448 sec/batch)
2016-11-28 19:14:56.406696: step 12450, loss = 0.35, acc = 0.98 (294.8 examples/sec; 0.434 sec/batch)
2016-11-28 19:15:01.217280: step 12460, loss = 0.35, acc = 0.99 (246.4 examples/sec; 0.519 sec/batch)
2016-11-28 19:15:05.903904: step 12470, loss = 0.35, acc = 0.98 (300.3 examples/sec; 0.426 sec/batch)
2016-11-28 19:15:10.707807: step 12480, loss = 0.44, acc = 0.97 (283.2 examples/sec; 0.452 sec/batch)
2016-11-28 19:15:15.411998: step 12490, loss = 0.34, acc = 0.99 (234.4 examples/sec; 0.546 sec/batch)
2016-11-28 19:15:20.002599: step 12500, loss = 0.37, acc = 0.95 (289.6 examples/sec; 0.442 sec/batch)
At step 12500 cross validation precision: 0.398
2016-11-28 19:15:25.610035: step 12510, loss = 0.34, acc = 0.98 (260.5 examples/sec; 0.491 sec/batch)
2016-11-28 19:15:30.368353: step 12520, loss = 0.35, acc = 0.98 (214.0 examples/sec; 0.598 sec/batch)
2016-11-28 19:15:35.032199: step 12530, loss = 0.36, acc = 0.98 (291.4 examples/sec; 0.439 sec/batch)
2016-11-28 19:15:39.805996: step 12540, loss = 0.32, acc = 1.00 (248.0 examples/sec; 0.516 sec/batch)
2016-11-28 19:15:44.476260: step 12550, loss = 0.40, acc = 0.98 (289.7 examples/sec; 0.442 sec/batch)
2016-11-28 19:15:49.135823: step 12560, loss = 0.52, acc = 0.92 (297.9 examples/sec; 0.430 sec/batch)
2016-11-28 19:15:53.790204: step 12570, loss = 0.34, acc = 0.99 (260.4 examples/sec; 0.491 sec/batch)
2016-11-28 19:15:58.554929: step 12580, loss = 0.43, acc = 0.95 (283.7 examples/sec; 0.451 sec/batch)
2016-11-28 19:16:03.260892: step 12590, loss = 0.46, acc = 0.97 (292.3 examples/sec; 0.438 sec/batch)
2016-11-28 19:16:08.154355: step 12600, loss = 0.34, acc = 0.99 (233.2 examples/sec; 0.549 sec/batch)
At step 12600 cross validation precision: 0.359
2016-11-28 19:16:13.862822: step 12610, loss = 0.46, acc = 0.95 (288.5 examples/sec; 0.444 sec/batch)
2016-11-28 19:16:18.579584: step 12620, loss = 0.42, acc = 0.95 (229.8 examples/sec; 0.557 sec/batch)
2016-11-28 19:16:23.166116: step 12630, loss = 0.39, acc = 0.98 (289.1 examples/sec; 0.443 sec/batch)
2016-11-28 19:16:28.112218: step 12640, loss = 0.34, acc = 1.00 (198.6 examples/sec; 0.644 sec/batch)
2016-11-28 19:16:32.673688: step 12650, loss = 0.33, acc = 1.00 (236.1 examples/sec; 0.542 sec/batch)
2016-11-28 19:16:37.289833: step 12660, loss = 0.38, acc = 0.97 (287.3 examples/sec; 0.445 sec/batch)
2016-11-28 19:16:42.026927: step 12670, loss = 0.36, acc = 0.98 (287.6 examples/sec; 0.445 sec/batch)
2016-11-28 19:16:46.775942: step 12680, loss = 0.36, acc = 0.98 (281.0 examples/sec; 0.455 sec/batch)
2016-11-28 19:16:51.628847: step 12690, loss = 0.37, acc = 0.98 (209.9 examples/sec; 0.610 sec/batch)
2016-11-28 19:16:56.387028: step 12700, loss = 0.36, acc = 0.98 (225.9 examples/sec; 0.567 sec/batch)
At step 12700 cross validation precision: 0.359
2016-11-28 19:17:01.930943: step 12710, loss = 0.36, acc = 0.98 (285.4 examples/sec; 0.449 sec/batch)
2016-11-28 19:17:06.687979: step 12720, loss = 0.33, acc = 1.00 (288.7 examples/sec; 0.443 sec/batch)
2016-11-28 19:17:11.334015: step 12730, loss = 0.36, acc = 0.98 (288.2 examples/sec; 0.444 sec/batch)
2016-11-28 19:17:16.026246: step 12740, loss = 0.34, acc = 0.99 (287.1 examples/sec; 0.446 sec/batch)
2016-11-28 19:17:20.794672: step 12750, loss = 0.34, acc = 0.98 (292.1 examples/sec; 0.438 sec/batch)
2016-11-28 19:17:25.448997: step 12760, loss = 0.38, acc = 0.98 (288.5 examples/sec; 0.444 sec/batch)
2016-11-28 19:17:30.223705: step 12770, loss = 0.34, acc = 0.99 (288.3 examples/sec; 0.444 sec/batch)
2016-11-28 19:17:34.997365: step 12780, loss = 0.34, acc = 0.99 (284.3 examples/sec; 0.450 sec/batch)
2016-11-28 19:17:39.713572: step 12790, loss = 0.37, acc = 0.97 (287.3 examples/sec; 0.446 sec/batch)
2016-11-28 19:17:44.462602: step 12800, loss = 0.41, acc = 0.99 (283.0 examples/sec; 0.452 sec/batch)
At step 12800 cross validation precision: 0.453
2016-11-28 19:17:50.147953: step 12810, loss = 0.36, acc = 0.98 (284.7 examples/sec; 0.450 sec/batch)
2016-11-28 19:17:54.756413: step 12820, loss = 0.33, acc = 0.99 (289.9 examples/sec; 0.442 sec/batch)
2016-11-28 19:17:59.558975: step 12830, loss = 0.34, acc = 0.98 (203.2 examples/sec; 0.630 sec/batch)
2016-11-28 19:18:04.360059: step 12840, loss = 0.33, acc = 0.99 (292.5 examples/sec; 0.438 sec/batch)
2016-11-28 19:18:08.931418: step 12850, loss = 0.31, acc = 1.00 (288.2 examples/sec; 0.444 sec/batch)
2016-11-28 19:18:13.677983: step 12860, loss = 0.36, acc = 0.98 (288.0 examples/sec; 0.444 sec/batch)
2016-11-28 19:18:18.438921: step 12870, loss = 0.36, acc = 0.98 (292.5 examples/sec; 0.438 sec/batch)
2016-11-28 19:18:23.265282: step 12880, loss = 0.35, acc = 0.99 (284.2 examples/sec; 0.450 sec/batch)
2016-11-28 19:18:27.862333: step 12890, loss = 0.34, acc = 0.99 (288.0 examples/sec; 0.444 sec/batch)
2016-11-28 19:18:32.519843: step 12900, loss = 0.34, acc = 0.98 (288.9 examples/sec; 0.443 sec/batch)
At step 12900 cross validation precision: 0.461
2016-11-28 19:18:38.221914: step 12910, loss = 0.35, acc = 0.98 (206.9 examples/sec; 0.619 sec/batch)
2016-11-28 19:18:42.808049: step 12920, loss = 0.33, acc = 0.99 (291.5 examples/sec; 0.439 sec/batch)
2016-11-28 19:18:47.670317: step 12930, loss = 0.39, acc = 0.97 (284.5 examples/sec; 0.450 sec/batch)
2016-11-28 19:18:52.355555: step 12940, loss = 0.38, acc = 0.97 (285.9 examples/sec; 0.448 sec/batch)
2016-11-28 19:18:57.110844: step 12950, loss = 0.34, acc = 0.98 (276.8 examples/sec; 0.462 sec/batch)
2016-11-28 19:19:01.891772: step 12960, loss = 0.45, acc = 0.95 (291.3 examples/sec; 0.439 sec/batch)
2016-11-28 19:19:06.607399: step 12970, loss = 0.41, acc = 0.98 (287.8 examples/sec; 0.445 sec/batch)
2016-11-28 19:19:11.341838: step 12980, loss = 0.47, acc = 0.96 (235.4 examples/sec; 0.544 sec/batch)
2016-11-28 19:19:16.187403: step 12990, loss = 0.42, acc = 0.96 (289.8 examples/sec; 0.442 sec/batch)
2016-11-28 19:19:20.965690: step 13000, loss = 0.41, acc = 0.97 (275.8 examples/sec; 0.464 sec/batch)
At step 13000 cross validation precision: 0.406
2016-11-28 19:19:26.582161: step 13010, loss = 0.47, acc = 0.97 (275.3 examples/sec; 0.465 sec/batch)
2016-11-28 19:19:31.125455: step 13020, loss = 0.44, acc = 0.94 (295.2 examples/sec; 0.434 sec/batch)
2016-11-28 19:19:35.941131: step 13030, loss = 0.33, acc = 0.99 (224.2 examples/sec; 0.571 sec/batch)
2016-11-28 19:19:40.560179: step 13040, loss = 0.33, acc = 0.99 (294.9 examples/sec; 0.434 sec/batch)
2016-11-28 19:19:45.285010: step 13050, loss = 0.35, acc = 0.99 (289.0 examples/sec; 0.443 sec/batch)
2016-11-28 19:19:50.011516: step 13060, loss = 0.36, acc = 0.98 (230.0 examples/sec; 0.557 sec/batch)
2016-11-28 19:19:54.615768: step 13070, loss = 0.35, acc = 0.99 (285.8 examples/sec; 0.448 sec/batch)
2016-11-28 19:19:59.339764: step 13080, loss = 0.38, acc = 0.98 (291.2 examples/sec; 0.440 sec/batch)
2016-11-28 19:20:04.057606: step 13090, loss = 0.39, acc = 0.98 (288.1 examples/sec; 0.444 sec/batch)
2016-11-28 19:20:08.759666: step 13100, loss = 0.34, acc = 1.00 (206.4 examples/sec; 0.620 sec/batch)
At step 13100 cross validation precision: 0.359
2016-11-28 19:20:14.409540: step 13110, loss = 0.42, acc = 0.96 (289.0 examples/sec; 0.443 sec/batch)
2016-11-28 19:20:19.139888: step 13120, loss = 0.43, acc = 0.97 (291.0 examples/sec; 0.440 sec/batch)
2016-11-28 19:20:23.894653: step 13130, loss = 0.40, acc = 0.96 (298.2 examples/sec; 0.429 sec/batch)
2016-11-28 19:20:28.527544: step 13140, loss = 0.37, acc = 0.97 (294.7 examples/sec; 0.434 sec/batch)
2016-11-28 19:20:33.395646: step 13150, loss = 0.39, acc = 0.98 (296.6 examples/sec; 0.432 sec/batch)
2016-11-28 19:20:38.120263: step 13160, loss = 0.36, acc = 0.99 (294.0 examples/sec; 0.435 sec/batch)
2016-11-28 19:20:42.734895: step 13170, loss = 0.36, acc = 0.98 (294.2 examples/sec; 0.435 sec/batch)
2016-11-28 19:20:47.515013: step 13180, loss = 0.34, acc = 1.00 (251.7 examples/sec; 0.509 sec/batch)
2016-11-28 19:20:52.192198: step 13190, loss = 0.36, acc = 0.98 (233.1 examples/sec; 0.549 sec/batch)
2016-11-28 19:20:56.866878: step 13200, loss = 0.40, acc = 0.98 (286.2 examples/sec; 0.447 sec/batch)
At step 13200 cross validation precision: 0.336
2016-11-28 19:21:02.403188: step 13210, loss = 0.40, acc = 0.97 (283.4 examples/sec; 0.452 sec/batch)
2016-11-28 19:21:07.305747: step 13220, loss = 0.49, acc = 0.96 (289.4 examples/sec; 0.442 sec/batch)
2016-11-28 19:21:12.073118: step 13230, loss = 0.37, acc = 0.98 (235.7 examples/sec; 0.543 sec/batch)
2016-11-28 19:21:16.709259: step 13240, loss = 0.35, acc = 0.98 (288.3 examples/sec; 0.444 sec/batch)
2016-11-28 19:21:21.381346: step 13250, loss = 0.38, acc = 0.98 (280.8 examples/sec; 0.456 sec/batch)
2016-11-28 19:21:26.182775: step 13260, loss = 0.33, acc = 0.99 (289.1 examples/sec; 0.443 sec/batch)
2016-11-28 19:21:30.898360: step 13270, loss = 0.33, acc = 1.00 (285.3 examples/sec; 0.449 sec/batch)
2016-11-28 19:21:35.471329: step 13280, loss = 0.42, acc = 0.97 (292.6 examples/sec; 0.438 sec/batch)
2016-11-28 19:21:40.132021: step 13290, loss = 0.36, acc = 0.98 (291.0 examples/sec; 0.440 sec/batch)
2016-11-28 19:21:44.886950: step 13300, loss = 0.43, acc = 0.96 (295.5 examples/sec; 0.433 sec/batch)
At step 13300 cross validation precision: 0.469
2016-11-28 19:21:50.615308: step 13310, loss = 0.39, acc = 0.98 (292.0 examples/sec; 0.438 sec/batch)
2016-11-28 19:21:55.511128: step 13320, loss = 0.34, acc = 0.99 (209.3 examples/sec; 0.611 sec/batch)
2016-11-28 19:22:00.117306: step 13330, loss = 0.43, acc = 0.98 (282.9 examples/sec; 0.452 sec/batch)
2016-11-28 19:22:04.956428: step 13340, loss = 0.31, acc = 1.00 (224.0 examples/sec; 0.571 sec/batch)
2016-11-28 19:22:09.501145: step 13350, loss = 0.34, acc = 0.99 (286.2 examples/sec; 0.447 sec/batch)
2016-11-28 19:22:14.283382: step 13360, loss = 0.31, acc = 1.00 (288.2 examples/sec; 0.444 sec/batch)
2016-11-28 19:22:19.080422: step 13370, loss = 0.35, acc = 0.99 (227.8 examples/sec; 0.562 sec/batch)
2016-11-28 19:22:23.777009: step 13380, loss = 0.32, acc = 0.99 (248.9 examples/sec; 0.514 sec/batch)
2016-11-28 19:22:28.598597: step 13390, loss = 0.37, acc = 0.97 (291.4 examples/sec; 0.439 sec/batch)
2016-11-28 19:22:33.380937: step 13400, loss = 0.31, acc = 1.00 (293.7 examples/sec; 0.436 sec/batch)
At step 13400 cross validation precision: 0.375
2016-11-28 19:22:38.914431: step 13410, loss = 0.34, acc = 0.98 (297.3 examples/sec; 0.431 sec/batch)
2016-11-28 19:22:43.601799: step 13420, loss = 0.32, acc = 0.99 (286.8 examples/sec; 0.446 sec/batch)
2016-11-28 19:22:48.333621: step 13430, loss = 0.34, acc = 0.98 (293.8 examples/sec; 0.436 sec/batch)
2016-11-28 19:22:53.086153: step 13440, loss = 0.31, acc = 1.00 (285.7 examples/sec; 0.448 sec/batch)
2016-11-28 19:22:57.852598: step 13450, loss = 0.36, acc = 0.98 (287.4 examples/sec; 0.445 sec/batch)
2016-11-28 19:23:02.650343: step 13460, loss = 0.32, acc = 0.99 (245.6 examples/sec; 0.521 sec/batch)
2016-11-28 19:23:07.393163: step 13470, loss = 0.34, acc = 0.98 (235.4 examples/sec; 0.544 sec/batch)
2016-11-28 19:23:12.106863: step 13480, loss = 0.34, acc = 0.99 (284.9 examples/sec; 0.449 sec/batch)
2016-11-28 19:23:16.838842: step 13490, loss = 0.33, acc = 0.98 (207.4 examples/sec; 0.617 sec/batch)
2016-11-28 19:23:21.494056: step 13500, loss = 0.34, acc = 0.98 (285.1 examples/sec; 0.449 sec/batch)
At step 13500 cross validation precision: 0.422
2016-11-28 19:23:27.441816: step 13510, loss = 0.33, acc = 1.00 (197.8 examples/sec; 0.647 sec/batch)
2016-11-28 19:23:32.079718: step 13520, loss = 0.46, acc = 0.94 (288.4 examples/sec; 0.444 sec/batch)
2016-11-28 19:23:36.939658: step 13530, loss = 0.50, acc = 0.95 (298.7 examples/sec; 0.429 sec/batch)
2016-11-28 19:23:41.673048: step 13540, loss = 0.39, acc = 0.98 (288.5 examples/sec; 0.444 sec/batch)
2016-11-28 19:23:46.438208: step 13550, loss = 0.45, acc = 0.96 (210.0 examples/sec; 0.610 sec/batch)
2016-11-28 19:23:51.029836: step 13560, loss = 0.37, acc = 0.98 (283.9 examples/sec; 0.451 sec/batch)
2016-11-28 19:23:55.847653: step 13570, loss = 0.41, acc = 0.97 (284.9 examples/sec; 0.449 sec/batch)
2016-11-28 19:24:00.495801: step 13580, loss = 0.37, acc = 0.98 (280.6 examples/sec; 0.456 sec/batch)
2016-11-28 19:24:05.252983: step 13590, loss = 0.36, acc = 0.98 (289.4 examples/sec; 0.442 sec/batch)
2016-11-28 19:24:10.085274: step 13600, loss = 0.34, acc = 0.98 (226.6 examples/sec; 0.565 sec/batch)
At step 13600 cross validation precision: 0.484
2016-11-28 19:24:15.745289: step 13610, loss = 0.35, acc = 0.98 (265.9 examples/sec; 0.481 sec/batch)
2016-11-28 19:24:20.357235: step 13620, loss = 0.37, acc = 0.97 (285.6 examples/sec; 0.448 sec/batch)
2016-11-28 19:24:24.926084: step 13630, loss = 0.59, acc = 0.96 (289.5 examples/sec; 0.442 sec/batch)
2016-11-28 19:24:29.681016: step 13640, loss = 0.33, acc = 0.99 (289.9 examples/sec; 0.442 sec/batch)
2016-11-28 19:24:34.488983: step 13650, loss = 0.37, acc = 0.98 (213.6 examples/sec; 0.599 sec/batch)
2016-11-28 19:24:39.117249: step 13660, loss = 0.36, acc = 0.98 (291.6 examples/sec; 0.439 sec/batch)
2016-11-28 19:24:43.957340: step 13670, loss = 0.37, acc = 0.98 (292.4 examples/sec; 0.438 sec/batch)
2016-11-28 19:24:48.720845: step 13680, loss = 0.34, acc = 0.99 (287.4 examples/sec; 0.445 sec/batch)
2016-11-28 19:24:53.505368: step 13690, loss = 0.33, acc = 0.99 (286.5 examples/sec; 0.447 sec/batch)
2016-11-28 19:24:58.102050: step 13700, loss = 0.40, acc = 0.96 (282.9 examples/sec; 0.452 sec/batch)
At step 13700 cross validation precision: 0.414
2016-11-28 19:25:03.974452: step 13710, loss = 0.39, acc = 0.98 (233.8 examples/sec; 0.547 sec/batch)
2016-11-28 19:25:08.757462: step 13720, loss = 0.37, acc = 0.98 (206.8 examples/sec; 0.619 sec/batch)
2016-11-28 19:25:13.256873: step 13730, loss = 0.40, acc = 0.96 (289.9 examples/sec; 0.442 sec/batch)
2016-11-28 19:25:18.165523: step 13740, loss = 0.33, acc = 0.99 (224.8 examples/sec; 0.569 sec/batch)
2016-11-28 19:25:22.787314: step 13750, loss = 0.36, acc = 0.98 (283.6 examples/sec; 0.451 sec/batch)
2016-11-28 19:25:27.478423: step 13760, loss = 0.35, acc = 0.98 (286.8 examples/sec; 0.446 sec/batch)
2016-11-28 19:25:32.213059: step 13770, loss = 0.34, acc = 0.98 (210.1 examples/sec; 0.609 sec/batch)
2016-11-28 19:25:37.055326: step 13780, loss = 0.31, acc = 1.00 (235.2 examples/sec; 0.544 sec/batch)
2016-11-28 19:25:41.701333: step 13790, loss = 0.32, acc = 1.00 (294.2 examples/sec; 0.435 sec/batch)
2016-11-28 19:25:46.463349: step 13800, loss = 0.35, acc = 0.98 (286.2 examples/sec; 0.447 sec/batch)
At step 13800 cross validation precision: 0.414
2016-11-28 19:25:52.227321: step 13810, loss = 0.35, acc = 0.98 (294.6 examples/sec; 0.434 sec/batch)
2016-11-28 19:25:57.064789: step 13820, loss = 0.35, acc = 0.98 (287.7 examples/sec; 0.445 sec/batch)
2016-11-28 19:26:01.748469: step 13830, loss = 0.33, acc = 0.99 (290.3 examples/sec; 0.441 sec/batch)
2016-11-28 19:26:06.323993: step 13840, loss = 0.33, acc = 0.99 (286.5 examples/sec; 0.447 sec/batch)
2016-11-28 19:26:11.079078: step 13850, loss = 0.33, acc = 0.99 (253.6 examples/sec; 0.505 sec/batch)
2016-11-28 19:26:15.820122: step 13860, loss = 0.32, acc = 0.99 (285.4 examples/sec; 0.449 sec/batch)
2016-11-28 19:26:20.604989: step 13870, loss = 0.36, acc = 0.98 (232.2 examples/sec; 0.551 sec/batch)
2016-11-28 19:26:25.381291: step 13880, loss = 0.32, acc = 0.98 (284.2 examples/sec; 0.450 sec/batch)
2016-11-28 19:26:29.924755: step 13890, loss = 0.30, acc = 1.00 (292.7 examples/sec; 0.437 sec/batch)
2016-11-28 19:26:34.699312: step 13900, loss = 0.39, acc = 0.97 (273.1 examples/sec; 0.469 sec/batch)
At step 13900 cross validation precision: 0.383
2016-11-28 19:26:40.577737: step 13910, loss = 0.33, acc = 0.99 (285.5 examples/sec; 0.448 sec/batch)
2016-11-28 19:26:45.307004: step 13920, loss = 0.35, acc = 0.97 (220.9 examples/sec; 0.579 sec/batch)
2016-11-28 19:26:49.976049: step 13930, loss = 0.30, acc = 0.99 (282.4 examples/sec; 0.453 sec/batch)
2016-11-28 19:26:54.765900: step 13940, loss = 0.31, acc = 1.00 (291.7 examples/sec; 0.439 sec/batch)
2016-11-28 19:26:59.506041: step 13950, loss = 0.30, acc = 1.00 (288.4 examples/sec; 0.444 sec/batch)
2016-11-28 19:27:04.137531: step 13960, loss = 0.42, acc = 0.97 (286.5 examples/sec; 0.447 sec/batch)
2016-11-28 19:27:08.911020: step 13970, loss = 0.30, acc = 1.00 (292.3 examples/sec; 0.438 sec/batch)
2016-11-28 19:27:13.726816: step 13980, loss = 0.30, acc = 1.00 (282.4 examples/sec; 0.453 sec/batch)
2016-11-28 19:27:18.449164: step 13990, loss = 0.31, acc = 0.99 (285.9 examples/sec; 0.448 sec/batch)
2016-11-28 19:27:23.157767: step 14000, loss = 0.30, acc = 1.00 (296.6 examples/sec; 0.432 sec/batch)
At step 14000 cross validation precision: 0.391
2016-11-28 19:27:28.941944: step 14010, loss = 0.35, acc = 0.98 (220.5 examples/sec; 0.581 sec/batch)
2016-11-28 19:27:33.621852: step 14020, loss = 0.30, acc = 1.00 (287.8 examples/sec; 0.445 sec/batch)
2016-11-28 19:27:38.351012: step 14030, loss = 0.31, acc = 1.00 (285.9 examples/sec; 0.448 sec/batch)
2016-11-28 19:27:42.966359: step 14040, loss = 0.36, acc = 0.98 (280.0 examples/sec; 0.457 sec/batch)
2016-11-28 19:27:47.700008: step 14050, loss = 0.34, acc = 0.98 (287.9 examples/sec; 0.445 sec/batch)
2016-11-28 19:27:52.319766: step 14060, loss = 0.32, acc = 0.98 (283.1 examples/sec; 0.452 sec/batch)
2016-11-28 19:27:57.157108: step 14070, loss = 0.38, acc = 0.95 (293.4 examples/sec; 0.436 sec/batch)
2016-11-28 19:28:01.793121: step 14080, loss = 0.36, acc = 0.98 (289.8 examples/sec; 0.442 sec/batch)
2016-11-28 19:28:06.646277: step 14090, loss = 0.33, acc = 0.98 (230.6 examples/sec; 0.555 sec/batch)
2016-11-28 19:28:11.436031: step 14100, loss = 0.44, acc = 0.95 (209.2 examples/sec; 0.612 sec/batch)
At step 14100 cross validation precision: 0.391
2016-11-28 19:28:17.116687: step 14110, loss = 0.35, acc = 0.98 (289.2 examples/sec; 0.443 sec/batch)
2016-11-28 19:28:21.889514: step 14120, loss = 0.37, acc = 0.95 (281.7 examples/sec; 0.454 sec/batch)
2016-11-28 19:28:26.604612: step 14130, loss = 0.36, acc = 0.97 (230.8 examples/sec; 0.555 sec/batch)
2016-11-28 19:28:31.301294: step 14140, loss = 0.32, acc = 0.99 (244.1 examples/sec; 0.524 sec/batch)
2016-11-28 19:28:36.082456: step 14150, loss = 0.39, acc = 0.97 (300.3 examples/sec; 0.426 sec/batch)
2016-11-28 19:28:40.700656: step 14160, loss = 0.32, acc = 0.99 (286.3 examples/sec; 0.447 sec/batch)
2016-11-28 19:28:45.418411: step 14170, loss = 0.34, acc = 0.98 (291.3 examples/sec; 0.439 sec/batch)
2016-11-28 19:28:50.246699: step 14180, loss = 0.44, acc = 0.95 (287.8 examples/sec; 0.445 sec/batch)
2016-11-28 19:28:54.865579: step 14190, loss = 0.44, acc = 0.95 (236.9 examples/sec; 0.540 sec/batch)
2016-11-28 19:28:59.706476: step 14200, loss = 0.32, acc = 0.99 (212.5 examples/sec; 0.602 sec/batch)
At step 14200 cross validation precision: 0.352
2016-11-28 19:29:05.367854: step 14210, loss = 0.40, acc = 0.97 (287.5 examples/sec; 0.445 sec/batch)
2016-11-28 19:29:10.109744: step 14220, loss = 0.43, acc = 0.94 (207.0 examples/sec; 0.618 sec/batch)
2016-11-28 19:29:14.733196: step 14230, loss = 0.31, acc = 1.00 (284.5 examples/sec; 0.450 sec/batch)
2016-11-28 19:29:19.529188: step 14240, loss = 0.39, acc = 0.97 (253.0 examples/sec; 0.506 sec/batch)
2016-11-28 19:29:24.201504: step 14250, loss = 0.41, acc = 0.97 (228.5 examples/sec; 0.560 sec/batch)
2016-11-28 19:29:28.994693: step 14260, loss = 0.31, acc = 1.00 (191.4 examples/sec; 0.669 sec/batch)
2016-11-28 19:29:33.643655: step 14270, loss = 0.32, acc = 1.00 (252.4 examples/sec; 0.507 sec/batch)
2016-11-28 19:29:38.462619: step 14280, loss = 0.40, acc = 0.96 (290.0 examples/sec; 0.441 sec/batch)
2016-11-28 19:29:43.192887: step 14290, loss = 0.32, acc = 0.99 (289.1 examples/sec; 0.443 sec/batch)
2016-11-28 19:29:48.066239: step 14300, loss = 0.33, acc = 0.99 (287.9 examples/sec; 0.445 sec/batch)
At step 14300 cross validation precision: 0.422
2016-11-28 19:29:53.783979: step 14310, loss = 0.35, acc = 0.98 (285.7 examples/sec; 0.448 sec/batch)
2016-11-28 19:29:58.573583: step 14320, loss = 0.33, acc = 0.98 (204.4 examples/sec; 0.626 sec/batch)
2016-11-28 19:30:03.182949: step 14330, loss = 0.39, acc = 0.96 (290.1 examples/sec; 0.441 sec/batch)
2016-11-28 19:30:07.821320: step 14340, loss = 0.36, acc = 0.97 (284.8 examples/sec; 0.449 sec/batch)
2016-11-28 19:30:12.642538: step 14350, loss = 0.31, acc = 1.00 (289.4 examples/sec; 0.442 sec/batch)
2016-11-28 19:30:17.427487: step 14360, loss = 0.33, acc = 0.99 (289.3 examples/sec; 0.442 sec/batch)
2016-11-28 19:30:22.168254: step 14370, loss = 0.33, acc = 0.99 (291.3 examples/sec; 0.439 sec/batch)
2016-11-28 19:30:26.969521: step 14380, loss = 0.32, acc = 0.98 (248.4 examples/sec; 0.515 sec/batch)
2016-11-28 19:30:31.644646: step 14390, loss = 0.31, acc = 0.99 (236.2 examples/sec; 0.542 sec/batch)
2016-11-28 19:30:36.302195: step 14400, loss = 0.34, acc = 0.98 (288.6 examples/sec; 0.443 sec/batch)
At step 14400 cross validation precision: 0.414
2016-11-28 19:30:41.995722: step 14410, loss = 0.37, acc = 0.97 (239.2 examples/sec; 0.535 sec/batch)
2016-11-28 19:30:46.738187: step 14420, loss = 0.40, acc = 0.97 (249.5 examples/sec; 0.513 sec/batch)
2016-11-28 19:30:51.310317: step 14430, loss = 0.39, acc = 0.97 (285.3 examples/sec; 0.449 sec/batch)
2016-11-28 19:30:55.921800: step 14440, loss = 0.39, acc = 0.97 (290.3 examples/sec; 0.441 sec/batch)
2016-11-28 19:31:00.760942: step 14450, loss = 0.40, acc = 0.97 (291.9 examples/sec; 0.439 sec/batch)
2016-11-28 19:31:05.567144: step 14460, loss = 0.47, acc = 0.94 (290.5 examples/sec; 0.441 sec/batch)
2016-11-28 19:31:10.298620: step 14470, loss = 0.41, acc = 0.97 (293.4 examples/sec; 0.436 sec/batch)
2016-11-28 19:31:15.118296: step 14480, loss = 0.35, acc = 0.98 (270.0 examples/sec; 0.474 sec/batch)
2016-11-28 19:31:19.694927: step 14490, loss = 0.43, acc = 0.95 (289.3 examples/sec; 0.442 sec/batch)
2016-11-28 19:31:24.555175: step 14500, loss = 0.37, acc = 0.98 (284.8 examples/sec; 0.449 sec/batch)
At step 14500 cross validation precision: 0.391
2016-11-28 19:31:30.254691: step 14510, loss = 0.42, acc = 0.98 (231.8 examples/sec; 0.552 sec/batch)
2016-11-28 19:31:35.104160: step 14520, loss = 0.41, acc = 0.97 (297.9 examples/sec; 0.430 sec/batch)
2016-11-28 19:31:39.783563: step 14530, loss = 0.36, acc = 0.98 (252.1 examples/sec; 0.508 sec/batch)
2016-11-28 19:31:44.645552: step 14540, loss = 0.39, acc = 0.96 (205.0 examples/sec; 0.624 sec/batch)
2016-11-28 19:31:49.290438: step 14550, loss = 0.42, acc = 0.98 (290.1 examples/sec; 0.441 sec/batch)
2016-11-28 19:31:54.049680: step 14560, loss = 0.38, acc = 0.96 (285.8 examples/sec; 0.448 sec/batch)
2016-11-28 19:31:58.850096: step 14570, loss = 0.35, acc = 0.98 (208.4 examples/sec; 0.614 sec/batch)
2016-11-28 19:32:03.485816: step 14580, loss = 0.42, acc = 0.97 (293.8 examples/sec; 0.436 sec/batch)
2016-11-28 19:32:08.107302: step 14590, loss = 0.34, acc = 0.99 (268.1 examples/sec; 0.477 sec/batch)
2016-11-28 19:32:12.981066: step 14600, loss = 0.33, acc = 0.98 (223.2 examples/sec; 0.573 sec/batch)
At step 14600 cross validation precision: 0.344
2016-11-28 19:32:18.497456: step 14610, loss = 0.44, acc = 0.96 (291.5 examples/sec; 0.439 sec/batch)
2016-11-28 19:32:23.347930: step 14620, loss = 0.32, acc = 1.00 (291.6 examples/sec; 0.439 sec/batch)
2016-11-28 19:32:27.996709: step 14630, loss = 0.35, acc = 0.98 (286.1 examples/sec; 0.447 sec/batch)
2016-11-28 19:32:32.662998: step 14640, loss = 0.34, acc = 0.98 (215.9 examples/sec; 0.593 sec/batch)
2016-11-28 19:32:37.400098: step 14650, loss = 0.38, acc = 0.98 (270.6 examples/sec; 0.473 sec/batch)
2016-11-28 19:32:42.004359: step 14660, loss = 0.37, acc = 0.98 (283.3 examples/sec; 0.452 sec/batch)
2016-11-28 19:32:46.892909: step 14670, loss = 0.35, acc = 0.98 (253.1 examples/sec; 0.506 sec/batch)
2016-11-28 19:32:51.513988: step 14680, loss = 0.38, acc = 0.98 (294.6 examples/sec; 0.434 sec/batch)
2016-11-28 19:32:56.259376: step 14690, loss = 0.38, acc = 0.97 (280.3 examples/sec; 0.457 sec/batch)
2016-11-28 19:33:01.097497: step 14700, loss = 0.36, acc = 0.98 (223.6 examples/sec; 0.572 sec/batch)
At step 14700 cross validation precision: 0.344
2016-11-28 19:33:06.810313: step 14710, loss = 0.34, acc = 0.98 (286.8 examples/sec; 0.446 sec/batch)
2016-11-28 19:33:11.551296: step 14720, loss = 0.40, acc = 0.98 (292.1 examples/sec; 0.438 sec/batch)
2016-11-28 19:33:16.249548: step 14730, loss = 0.31, acc = 1.00 (290.8 examples/sec; 0.440 sec/batch)
2016-11-28 19:33:21.055271: step 14740, loss = 0.38, acc = 0.98 (209.0 examples/sec; 0.612 sec/batch)
2016-11-28 19:33:25.783423: step 14750, loss = 0.39, acc = 0.98 (258.3 examples/sec; 0.496 sec/batch)
2016-11-28 19:33:30.435679: step 14760, loss = 0.45, acc = 0.95 (287.8 examples/sec; 0.445 sec/batch)
2016-11-28 19:33:35.159995: step 14770, loss = 0.34, acc = 0.98 (231.7 examples/sec; 0.553 sec/batch)
2016-11-28 19:33:39.855874: step 14780, loss = 0.37, acc = 0.98 (290.6 examples/sec; 0.441 sec/batch)
2016-11-28 19:33:44.543771: step 14790, loss = 0.38, acc = 0.97 (293.7 examples/sec; 0.436 sec/batch)
2016-11-28 19:33:49.330990: step 14800, loss = 0.36, acc = 0.98 (286.1 examples/sec; 0.447 sec/batch)
At step 14800 cross validation precision: 0.406
2016-11-28 19:33:55.023818: step 14810, loss = 0.38, acc = 0.97 (287.2 examples/sec; 0.446 sec/batch)
2016-11-28 19:33:59.794255: step 14820, loss = 0.31, acc = 1.00 (208.0 examples/sec; 0.615 sec/batch)
2016-11-28 19:34:04.475640: step 14830, loss = 0.34, acc = 0.99 (287.8 examples/sec; 0.445 sec/batch)
2016-11-28 19:34:09.211118: step 14840, loss = 0.33, acc = 1.00 (223.7 examples/sec; 0.572 sec/batch)
2016-11-28 19:34:13.753198: step 14850, loss = 0.34, acc = 0.99 (287.0 examples/sec; 0.446 sec/batch)
2016-11-28 19:34:18.606529: step 14860, loss = 0.34, acc = 0.98 (290.8 examples/sec; 0.440 sec/batch)
2016-11-28 19:34:23.406088: step 14870, loss = 0.36, acc = 0.98 (285.4 examples/sec; 0.448 sec/batch)
2016-11-28 19:34:28.040804: step 14880, loss = 0.35, acc = 0.98 (294.5 examples/sec; 0.435 sec/batch)
2016-11-28 19:34:32.799868: step 14890, loss = 0.38, acc = 0.97 (286.4 examples/sec; 0.447 sec/batch)
2016-11-28 19:34:37.576229: step 14900, loss = 0.35, acc = 0.99 (283.4 examples/sec; 0.452 sec/batch)
At step 14900 cross validation precision: 0.406
2016-11-28 19:34:43.233970: step 14910, loss = 0.31, acc = 1.00 (289.8 examples/sec; 0.442 sec/batch)
2016-11-28 19:34:48.041491: step 14920, loss = 0.40, acc = 0.97 (299.5 examples/sec; 0.427 sec/batch)
2016-11-28 19:34:52.771031: step 14930, loss = 0.33, acc = 0.99 (214.5 examples/sec; 0.597 sec/batch)
2016-11-28 19:34:57.459376: step 14940, loss = 0.38, acc = 0.98 (289.2 examples/sec; 0.443 sec/batch)
2016-11-28 19:35:02.233077: step 14950, loss = 0.31, acc = 1.00 (223.9 examples/sec; 0.572 sec/batch)
2016-11-28 19:35:06.949578: step 14960, loss = 0.36, acc = 0.98 (287.3 examples/sec; 0.445 sec/batch)
2016-11-28 19:35:11.581710: step 14970, loss = 0.46, acc = 0.95 (294.3 examples/sec; 0.435 sec/batch)
2016-11-28 19:35:16.189981: step 14980, loss = 0.33, acc = 0.99 (290.4 examples/sec; 0.441 sec/batch)
2016-11-28 19:35:20.996302: step 14990, loss = 0.37, acc = 0.96 (289.1 examples/sec; 0.443 sec/batch)
2016-11-28 19:35:25.604240: step 15000, loss = 0.34, acc = 1.00 (285.8 examples/sec; 0.448 sec/batch)
At step 15000 cross validation precision: 0.391
2016-11-28 19:35:31.398724: step 15010, loss = 0.39, acc = 0.98 (194.9 examples/sec; 0.657 sec/batch)
2016-11-28 19:35:36.119335: step 15020, loss = 0.34, acc = 0.99 (230.1 examples/sec; 0.556 sec/batch)
2016-11-28 19:35:40.933943: step 15030, loss = 0.35, acc = 0.98 (288.5 examples/sec; 0.444 sec/batch)
2016-11-28 19:35:45.549305: step 15040, loss = 0.34, acc = 0.99 (291.3 examples/sec; 0.439 sec/batch)
2016-11-28 19:35:50.312191: step 15050, loss = 0.32, acc = 1.00 (290.6 examples/sec; 0.441 sec/batch)
2016-11-28 19:35:54.994921: step 15060, loss = 0.35, acc = 0.98 (287.4 examples/sec; 0.445 sec/batch)
2016-11-28 19:35:59.810084: step 15070, loss = 0.38, acc = 0.98 (286.6 examples/sec; 0.447 sec/batch)
2016-11-28 19:36:04.432347: step 15080, loss = 0.36, acc = 0.98 (289.2 examples/sec; 0.443 sec/batch)
2016-11-28 19:36:09.213414: step 15090, loss = 0.38, acc = 0.96 (281.7 examples/sec; 0.454 sec/batch)
2016-11-28 19:36:14.168297: step 15100, loss = 0.34, acc = 0.98 (241.4 examples/sec; 0.530 sec/batch)
At step 15100 cross validation precision: 0.484
2016-11-28 19:36:19.794351: step 15110, loss = 0.35, acc = 0.99 (288.3 examples/sec; 0.444 sec/batch)
2016-11-28 19:36:24.436219: step 15120, loss = 0.35, acc = 0.98 (290.4 examples/sec; 0.441 sec/batch)
2016-11-28 19:36:29.286755: step 15130, loss = 0.32, acc = 0.99 (270.3 examples/sec; 0.474 sec/batch)
2016-11-28 19:36:33.889502: step 15140, loss = 0.35, acc = 0.98 (297.7 examples/sec; 0.430 sec/batch)
2016-11-28 19:36:38.670996: step 15150, loss = 0.37, acc = 0.98 (288.7 examples/sec; 0.443 sec/batch)
2016-11-28 19:36:43.399782: step 15160, loss = 0.34, acc = 0.99 (286.8 examples/sec; 0.446 sec/batch)
2016-11-28 19:36:48.052079: step 15170, loss = 0.33, acc = 0.99 (232.4 examples/sec; 0.551 sec/batch)
2016-11-28 19:36:52.693831: step 15180, loss = 0.36, acc = 0.98 (285.6 examples/sec; 0.448 sec/batch)
2016-11-28 19:36:57.504440: step 15190, loss = 0.33, acc = 0.99 (255.1 examples/sec; 0.502 sec/batch)
2016-11-28 19:37:02.201026: step 15200, loss = 0.34, acc = 0.99 (289.4 examples/sec; 0.442 sec/batch)
At step 15200 cross validation precision: 0.430
2016-11-28 19:37:07.926852: step 15210, loss = 0.36, acc = 0.97 (284.5 examples/sec; 0.450 sec/batch)
2016-11-28 19:37:12.704037: step 15220, loss = 0.39, acc = 0.98 (282.3 examples/sec; 0.453 sec/batch)
2016-11-28 19:37:17.375282: step 15230, loss = 0.36, acc = 0.98 (242.0 examples/sec; 0.529 sec/batch)
2016-11-28 19:37:21.965656: step 15240, loss = 0.39, acc = 0.97 (294.1 examples/sec; 0.435 sec/batch)
2016-11-28 19:37:26.740490: step 15250, loss = 0.31, acc = 0.99 (292.0 examples/sec; 0.438 sec/batch)
2016-11-28 19:37:31.503990: step 15260, loss = 0.32, acc = 0.99 (275.5 examples/sec; 0.465 sec/batch)
2016-11-28 19:37:36.223643: step 15270, loss = 0.35, acc = 0.98 (285.8 examples/sec; 0.448 sec/batch)
2016-11-28 19:37:41.008689: step 15280, loss = 0.31, acc = 1.00 (203.8 examples/sec; 0.628 sec/batch)
2016-11-28 19:37:45.682827: step 15290, loss = 0.35, acc = 0.98 (283.8 examples/sec; 0.451 sec/batch)
2016-11-28 19:37:50.607449: step 15300, loss = 0.32, acc = 1.00 (208.4 examples/sec; 0.614 sec/batch)
At step 15300 cross validation precision: 0.414
2016-11-28 19:37:56.287481: step 15310, loss = 0.30, acc = 1.00 (287.1 examples/sec; 0.446 sec/batch)
2016-11-28 19:38:01.025498: step 15320, loss = 0.31, acc = 0.99 (285.4 examples/sec; 0.448 sec/batch)
2016-11-28 19:38:05.627940: step 15330, loss = 0.36, acc = 0.97 (283.5 examples/sec; 0.451 sec/batch)
2016-11-28 19:38:10.596361: step 15340, loss = 0.34, acc = 0.97 (274.4 examples/sec; 0.466 sec/batch)
2016-11-28 19:38:15.176353: step 15350, loss = 0.31, acc = 0.99 (293.6 examples/sec; 0.436 sec/batch)
2016-11-28 19:38:19.954947: step 15360, loss = 0.30, acc = 1.00 (291.8 examples/sec; 0.439 sec/batch)
2016-11-28 19:38:24.662760: step 15370, loss = 0.33, acc = 0.99 (283.5 examples/sec; 0.451 sec/batch)
2016-11-28 19:38:29.330021: step 15380, loss = 0.34, acc = 0.98 (220.6 examples/sec; 0.580 sec/batch)
2016-11-28 19:38:33.911338: step 15390, loss = 0.41, acc = 0.95 (292.7 examples/sec; 0.437 sec/batch)
2016-11-28 19:38:38.780439: step 15400, loss = 0.37, acc = 0.98 (291.4 examples/sec; 0.439 sec/batch)
At step 15400 cross validation precision: 0.398
2016-11-28 19:38:44.420998: step 15410, loss = 0.38, acc = 0.97 (295.4 examples/sec; 0.433 sec/batch)
2016-11-28 19:38:49.194352: step 15420, loss = 0.34, acc = 0.98 (235.5 examples/sec; 0.544 sec/batch)
2016-11-28 19:38:53.988356: step 15430, loss = 0.36, acc = 0.98 (283.6 examples/sec; 0.451 sec/batch)
2016-11-28 19:38:58.706837: step 15440, loss = 0.40, acc = 0.97 (282.2 examples/sec; 0.454 sec/batch)
2016-11-28 19:39:03.563160: step 15450, loss = 0.35, acc = 0.98 (283.9 examples/sec; 0.451 sec/batch)
2016-11-28 19:39:08.292947: step 15460, loss = 0.37, acc = 0.99 (234.4 examples/sec; 0.546 sec/batch)
2016-11-28 19:39:12.837641: step 15470, loss = 0.34, acc = 0.99 (292.6 examples/sec; 0.437 sec/batch)
2016-11-28 19:39:17.509111: step 15480, loss = 0.35, acc = 0.98 (286.1 examples/sec; 0.447 sec/batch)
2016-11-28 19:39:22.317419: step 15490, loss = 0.32, acc = 1.00 (258.0 examples/sec; 0.496 sec/batch)
2016-11-28 19:39:26.898399: step 15500, loss = 0.33, acc = 0.98 (292.8 examples/sec; 0.437 sec/batch)
At step 15500 cross validation precision: 0.359
2016-11-28 19:39:32.615678: step 15510, loss = 0.36, acc = 0.98 (286.9 examples/sec; 0.446 sec/batch)
2016-11-28 19:39:37.293998: step 15520, loss = 0.33, acc = 0.98 (284.2 examples/sec; 0.450 sec/batch)
2016-11-28 19:39:42.062989: step 15530, loss = 0.37, acc = 0.98 (289.3 examples/sec; 0.442 sec/batch)
2016-11-28 19:39:46.774022: step 15540, loss = 0.36, acc = 0.98 (295.8 examples/sec; 0.433 sec/batch)
2016-11-28 19:39:51.599543: step 15550, loss = 0.31, acc = 1.00 (287.4 examples/sec; 0.445 sec/batch)
2016-11-28 19:39:56.320989: step 15560, loss = 0.38, acc = 0.99 (225.2 examples/sec; 0.568 sec/batch)
2016-11-28 19:40:00.979830: step 15570, loss = 0.33, acc = 0.99 (286.7 examples/sec; 0.446 sec/batch)
2016-11-28 19:40:05.934912: step 15580, loss = 0.30, acc = 1.00 (266.4 examples/sec; 0.480 sec/batch)
2016-11-28 19:40:10.529232: step 15590, loss = 0.33, acc = 0.98 (289.0 examples/sec; 0.443 sec/batch)
2016-11-28 19:40:15.294742: step 15600, loss = 0.32, acc = 0.99 (289.4 examples/sec; 0.442 sec/batch)
At step 15600 cross validation precision: 0.391
2016-11-28 19:40:21.047037: step 15610, loss = 0.31, acc = 0.99 (250.3 examples/sec; 0.511 sec/batch)
2016-11-28 19:40:25.637461: step 15620, loss = 0.37, acc = 0.98 (289.1 examples/sec; 0.443 sec/batch)
2016-11-28 19:40:30.333157: step 15630, loss = 0.32, acc = 0.99 (289.7 examples/sec; 0.442 sec/batch)
2016-11-28 19:40:34.942003: step 15640, loss = 0.31, acc = 1.00 (292.0 examples/sec; 0.438 sec/batch)
2016-11-28 19:40:39.576354: step 15650, loss = 0.32, acc = 0.99 (285.7 examples/sec; 0.448 sec/batch)
2016-11-28 19:40:44.349549: step 15660, loss = 0.34, acc = 0.97 (285.9 examples/sec; 0.448 sec/batch)
2016-11-28 19:40:49.104903: step 15670, loss = 0.32, acc = 0.99 (286.1 examples/sec; 0.447 sec/batch)
2016-11-28 19:40:53.884048: step 15680, loss = 0.34, acc = 0.99 (288.3 examples/sec; 0.444 sec/batch)
2016-11-28 19:40:58.541676: step 15690, loss = 0.31, acc = 0.99 (294.9 examples/sec; 0.434 sec/batch)
2016-11-28 19:41:03.285518: step 15700, loss = 0.34, acc = 0.98 (200.0 examples/sec; 0.640 sec/batch)
At step 15700 cross validation precision: 0.367
2016-11-28 19:41:08.982517: step 15710, loss = 0.31, acc = 1.00 (266.2 examples/sec; 0.481 sec/batch)
2016-11-28 19:41:13.726295: step 15720, loss = 0.33, acc = 0.98 (285.9 examples/sec; 0.448 sec/batch)
2016-11-28 19:41:18.351136: step 15730, loss = 0.32, acc = 0.99 (290.1 examples/sec; 0.441 sec/batch)
2016-11-28 19:41:23.171279: step 15740, loss = 0.31, acc = 0.99 (283.8 examples/sec; 0.451 sec/batch)
2016-11-28 19:41:27.966287: step 15750, loss = 0.33, acc = 0.99 (248.4 examples/sec; 0.515 sec/batch)
2016-11-28 19:41:32.763026: step 15760, loss = 0.32, acc = 0.99 (285.2 examples/sec; 0.449 sec/batch)
2016-11-28 19:41:37.519490: step 15770, loss = 0.39, acc = 0.96 (283.2 examples/sec; 0.452 sec/batch)
2016-11-28 19:41:42.122839: step 15780, loss = 0.31, acc = 1.00 (283.3 examples/sec; 0.452 sec/batch)
2016-11-28 19:41:46.961049: step 15790, loss = 0.41, acc = 0.98 (285.3 examples/sec; 0.449 sec/batch)
2016-11-28 19:41:51.734470: step 15800, loss = 0.42, acc = 0.96 (286.5 examples/sec; 0.447 sec/batch)
At step 15800 cross validation precision: 0.406
2016-11-28 19:41:57.383051: step 15810, loss = 0.47, acc = 0.95 (195.3 examples/sec; 0.655 sec/batch)
2016-11-28 19:42:02.180019: step 15820, loss = 0.41, acc = 0.97 (292.0 examples/sec; 0.438 sec/batch)
2016-11-28 19:42:07.023919: step 15830, loss = 0.35, acc = 0.99 (273.7 examples/sec; 0.468 sec/batch)
2016-11-28 19:42:11.646043: step 15840, loss = 0.34, acc = 0.99 (256.0 examples/sec; 0.500 sec/batch)
2016-11-28 19:42:16.444898: step 15850, loss = 0.37, acc = 0.98 (208.7 examples/sec; 0.613 sec/batch)
2016-11-28 19:42:21.002560: step 15860, loss = 0.42, acc = 0.96 (290.1 examples/sec; 0.441 sec/batch)
2016-11-28 19:42:25.686180: step 15870, loss = 0.47, acc = 0.95 (290.4 examples/sec; 0.441 sec/batch)
2016-11-28 19:42:30.361390: step 15880, loss = 0.39, acc = 0.96 (243.4 examples/sec; 0.526 sec/batch)
2016-11-28 19:42:35.231292: step 15890, loss = 0.36, acc = 0.98 (227.3 examples/sec; 0.563 sec/batch)
2016-11-28 19:42:39.820018: step 15900, loss = 0.38, acc = 0.97 (286.9 examples/sec; 0.446 sec/batch)
At step 15900 cross validation precision: 0.430
2016-11-28 19:42:45.430592: step 15910, loss = 0.38, acc = 0.97 (291.5 examples/sec; 0.439 sec/batch)
2016-11-28 19:42:50.148255: step 15920, loss = 0.34, acc = 0.98 (288.1 examples/sec; 0.444 sec/batch)
2016-11-28 19:42:54.920222: step 15930, loss = 0.33, acc = 1.00 (253.3 examples/sec; 0.505 sec/batch)
2016-11-28 19:42:59.656209: step 15940, loss = 0.37, acc = 0.96 (288.2 examples/sec; 0.444 sec/batch)
2016-11-28 19:43:04.423151: step 15950, loss = 0.33, acc = 1.00 (286.6 examples/sec; 0.447 sec/batch)
2016-11-28 19:43:09.011322: step 15960, loss = 0.34, acc = 0.99 (284.8 examples/sec; 0.449 sec/batch)
2016-11-28 19:43:13.850557: step 15970, loss = 0.35, acc = 0.98 (292.2 examples/sec; 0.438 sec/batch)
2016-11-28 19:43:18.582894: step 15980, loss = 0.35, acc = 0.98 (288.5 examples/sec; 0.444 sec/batch)
2016-11-28 19:43:23.079644: step 15990, loss = 0.36, acc = 0.98 (293.5 examples/sec; 0.436 sec/batch)
2016-11-28 19:43:27.854851: step 16000, loss = 0.40, acc = 0.95 (280.4 examples/sec; 0.456 sec/batch)
At step 16000 cross validation precision: 0.375
2016-11-28 19:43:33.445537: step 16010, loss = 0.32, acc = 1.00 (284.2 examples/sec; 0.450 sec/batch)
2016-11-28 19:43:38.222203: step 16020, loss = 0.32, acc = 1.00 (283.5 examples/sec; 0.452 sec/batch)
2016-11-28 19:43:43.045313: step 16030, loss = 0.34, acc = 0.99 (292.8 examples/sec; 0.437 sec/batch)
2016-11-28 19:43:47.725036: step 16040, loss = 0.35, acc = 0.98 (290.3 examples/sec; 0.441 sec/batch)
2016-11-28 19:43:52.387754: step 16050, loss = 0.32, acc = 0.99 (292.1 examples/sec; 0.438 sec/batch)
2016-11-28 19:43:57.257072: step 16060, loss = 0.35, acc = 0.97 (217.6 examples/sec; 0.588 sec/batch)
2016-11-28 19:44:01.805936: step 16070, loss = 0.35, acc = 0.98 (288.7 examples/sec; 0.443 sec/batch)
2016-11-28 19:44:06.719863: step 16080, loss = 0.33, acc = 0.98 (241.9 examples/sec; 0.529 sec/batch)
2016-11-28 19:44:11.413708: step 16090, loss = 0.33, acc = 0.99 (287.7 examples/sec; 0.445 sec/batch)
2016-11-28 19:44:16.337489: step 16100, loss = 0.33, acc = 0.98 (227.2 examples/sec; 0.563 sec/batch)
At step 16100 cross validation precision: 0.438
2016-11-28 19:44:21.860890: step 16110, loss = 0.32, acc = 0.99 (285.3 examples/sec; 0.449 sec/batch)
2016-11-28 19:44:26.639532: step 16120, loss = 0.35, acc = 0.98 (295.4 examples/sec; 0.433 sec/batch)
2016-11-28 19:44:31.359656: step 16130, loss = 0.33, acc = 0.99 (288.5 examples/sec; 0.444 sec/batch)
2016-11-28 19:44:36.033392: step 16140, loss = 0.40, acc = 0.99 (285.6 examples/sec; 0.448 sec/batch)
2016-11-28 19:44:40.795543: step 16150, loss = 0.32, acc = 0.99 (281.0 examples/sec; 0.455 sec/batch)
2016-11-28 19:44:45.518622: step 16160, loss = 0.31, acc = 1.00 (280.7 examples/sec; 0.456 sec/batch)
2016-11-28 19:44:50.325218: step 16170, loss = 0.30, acc = 1.00 (225.8 examples/sec; 0.567 sec/batch)
2016-11-28 19:44:54.980170: step 16180, loss = 0.35, acc = 0.98 (289.6 examples/sec; 0.442 sec/batch)
2016-11-28 19:44:59.764363: step 16190, loss = 0.36, acc = 0.98 (247.8 examples/sec; 0.517 sec/batch)
2016-11-28 19:45:04.668306: step 16200, loss = 0.33, acc = 0.99 (290.5 examples/sec; 0.441 sec/batch)
At step 16200 cross validation precision: 0.352
2016-11-28 19:45:10.905867: step 16210, loss = 0.34, acc = 0.98 (197.9 examples/sec; 0.647 sec/batch)
2016-11-28 19:45:15.874776: step 16220, loss = 0.35, acc = 0.98 (286.3 examples/sec; 0.447 sec/batch)
2016-11-28 19:45:20.866286: step 16230, loss = 0.44, acc = 0.95 (237.5 examples/sec; 0.539 sec/batch)
2016-11-28 19:45:25.902792: step 16240, loss = 0.35, acc = 0.98 (286.1 examples/sec; 0.447 sec/batch)
2016-11-28 19:45:30.794622: step 16250, loss = 0.35, acc = 0.98 (239.6 examples/sec; 0.534 sec/batch)
2016-11-28 19:45:35.543303: step 16260, loss = 0.36, acc = 0.98 (281.1 examples/sec; 0.455 sec/batch)
2016-11-28 19:45:40.458736: step 16270, loss = 0.35, acc = 0.98 (280.3 examples/sec; 0.457 sec/batch)
2016-11-28 19:45:45.235124: step 16280, loss = 0.37, acc = 0.98 (289.7 examples/sec; 0.442 sec/batch)
2016-11-28 19:45:49.957278: step 16290, loss = 0.33, acc = 0.99 (290.1 examples/sec; 0.441 sec/batch)
2016-11-28 19:45:54.723108: step 16300, loss = 0.37, acc = 0.96 (222.0 examples/sec; 0.577 sec/batch)
At step 16300 cross validation precision: 0.328
2016-11-28 19:46:00.505268: step 16310, loss = 0.36, acc = 0.98 (234.0 examples/sec; 0.547 sec/batch)
2016-11-28 19:46:05.391206: step 16320, loss = 0.37, acc = 0.97 (225.9 examples/sec; 0.567 sec/batch)
2016-11-28 19:46:09.999654: step 16330, loss = 0.34, acc = 0.98 (286.5 examples/sec; 0.447 sec/batch)
2016-11-28 19:46:15.108877: step 16340, loss = 0.33, acc = 0.98 (190.2 examples/sec; 0.673 sec/batch)
2016-11-28 19:46:19.941436: step 16350, loss = 0.34, acc = 0.98 (268.6 examples/sec; 0.476 sec/batch)
2016-11-28 19:46:24.714637: step 16360, loss = 0.38, acc = 0.98 (286.0 examples/sec; 0.448 sec/batch)
2016-11-28 19:46:29.546134: step 16370, loss = 0.35, acc = 0.98 (288.1 examples/sec; 0.444 sec/batch)
2016-11-28 19:46:34.318616: step 16380, loss = 0.37, acc = 0.99 (282.9 examples/sec; 0.452 sec/batch)
2016-11-28 19:46:39.253701: step 16390, loss = 0.32, acc = 1.00 (291.8 examples/sec; 0.439 sec/batch)
2016-11-28 19:46:44.037317: step 16400, loss = 0.33, acc = 0.99 (253.3 examples/sec; 0.505 sec/batch)
At step 16400 cross validation precision: 0.375
2016-11-28 19:46:49.768041: step 16410, loss = 0.33, acc = 0.98 (293.9 examples/sec; 0.436 sec/batch)
2016-11-28 19:46:54.675824: step 16420, loss = 0.33, acc = 0.98 (207.4 examples/sec; 0.617 sec/batch)
2016-11-28 19:46:59.461420: step 16430, loss = 0.32, acc = 0.98 (284.5 examples/sec; 0.450 sec/batch)
2016-11-28 19:47:04.218348: step 16440, loss = 0.31, acc = 0.99 (285.2 examples/sec; 0.449 sec/batch)
2016-11-28 19:47:09.012851: step 16450, loss = 0.33, acc = 0.99 (290.8 examples/sec; 0.440 sec/batch)
2016-11-28 19:47:13.791105: step 16460, loss = 0.30, acc = 1.00 (282.1 examples/sec; 0.454 sec/batch)
2016-11-28 19:47:18.612488: step 16470, loss = 0.30, acc = 1.00 (282.0 examples/sec; 0.454 sec/batch)
2016-11-28 19:47:23.227117: step 16480, loss = 0.30, acc = 1.00 (289.2 examples/sec; 0.443 sec/batch)
2016-11-28 19:47:27.992434: step 16490, loss = 0.30, acc = 1.00 (289.2 examples/sec; 0.443 sec/batch)
2016-11-28 19:47:32.953712: step 16500, loss = 0.30, acc = 1.00 (231.7 examples/sec; 0.552 sec/batch)
At step 16500 cross validation precision: 0.469
2016-11-28 19:47:38.579381: step 16510, loss = 0.32, acc = 0.98 (285.7 examples/sec; 0.448 sec/batch)
2016-11-28 19:47:43.213218: step 16520, loss = 0.30, acc = 1.00 (283.3 examples/sec; 0.452 sec/batch)
2016-11-28 19:47:48.195523: step 16530, loss = 0.33, acc = 0.99 (288.3 examples/sec; 0.444 sec/batch)
2016-11-28 19:47:52.970134: step 16540, loss = 0.31, acc = 1.00 (264.0 examples/sec; 0.485 sec/batch)
2016-11-28 19:47:57.754707: step 16550, loss = 0.31, acc = 1.00 (210.1 examples/sec; 0.609 sec/batch)
2016-11-28 19:48:02.566084: step 16560, loss = 0.31, acc = 0.99 (253.6 examples/sec; 0.505 sec/batch)
2016-11-28 19:48:07.340745: step 16570, loss = 0.30, acc = 1.00 (289.9 examples/sec; 0.442 sec/batch)
2016-11-28 19:48:12.160961: step 16580, loss = 0.30, acc = 1.00 (194.3 examples/sec; 0.659 sec/batch)
2016-11-28 19:48:16.939302: step 16590, loss = 0.30, acc = 1.00 (290.8 examples/sec; 0.440 sec/batch)
2016-11-28 19:48:21.898427: step 16600, loss = 0.31, acc = 0.99 (209.9 examples/sec; 0.610 sec/batch)
At step 16600 cross validation precision: 0.336
2016-11-28 19:48:27.615519: step 16610, loss = 0.30, acc = 1.00 (285.6 examples/sec; 0.448 sec/batch)
2016-11-28 19:48:32.289142: step 16620, loss = 0.30, acc = 1.00 (286.6 examples/sec; 0.447 sec/batch)
2016-11-28 19:48:37.143698: step 16630, loss = 0.30, acc = 1.00 (279.9 examples/sec; 0.457 sec/batch)
2016-11-28 19:48:42.039944: step 16640, loss = 0.30, acc = 1.00 (223.8 examples/sec; 0.572 sec/batch)
2016-11-28 19:48:46.618556: step 16650, loss = 0.30, acc = 1.00 (280.6 examples/sec; 0.456 sec/batch)
2016-11-28 19:48:51.423016: step 16660, loss = 0.30, acc = 0.99 (280.7 examples/sec; 0.456 sec/batch)
2016-11-28 19:48:56.324184: step 16670, loss = 0.30, acc = 1.00 (206.8 examples/sec; 0.619 sec/batch)
2016-11-28 19:49:01.148134: step 16680, loss = 0.30, acc = 1.00 (208.9 examples/sec; 0.613 sec/batch)
2016-11-28 19:49:05.724500: step 16690, loss = 0.30, acc = 1.00 (289.2 examples/sec; 0.443 sec/batch)
2016-11-28 19:49:10.514071: step 16700, loss = 0.30, acc = 1.00 (291.1 examples/sec; 0.440 sec/batch)
At step 16700 cross validation precision: 0.453
2016-11-28 19:49:16.269150: step 16710, loss = 0.29, acc = 1.00 (292.7 examples/sec; 0.437 sec/batch)
2016-11-28 19:49:20.985065: step 16720, loss = 0.30, acc = 1.00 (289.1 examples/sec; 0.443 sec/batch)
2016-11-28 19:49:25.878503: step 16730, loss = 0.31, acc = 0.99 (227.5 examples/sec; 0.563 sec/batch)
2016-11-28 19:49:30.650339: step 16740, loss = 0.30, acc = 1.00 (210.1 examples/sec; 0.609 sec/batch)
2016-11-28 19:49:35.245816: step 16750, loss = 0.30, acc = 1.00 (296.3 examples/sec; 0.432 sec/batch)
2016-11-28 19:49:40.145434: step 16760, loss = 0.30, acc = 1.00 (293.7 examples/sec; 0.436 sec/batch)
2016-11-28 19:49:44.788005: step 16770, loss = 0.29, acc = 1.00 (288.7 examples/sec; 0.443 sec/batch)
2016-11-28 19:49:49.538705: step 16780, loss = 0.29, acc = 1.00 (288.6 examples/sec; 0.444 sec/batch)
2016-11-28 19:49:54.413409: step 16790, loss = 0.30, acc = 1.00 (229.2 examples/sec; 0.558 sec/batch)
2016-11-28 19:49:59.217774: step 16800, loss = 0.29, acc = 1.00 (280.3 examples/sec; 0.457 sec/batch)
At step 16800 cross validation precision: 0.438
2016-11-28 19:50:04.970788: step 16810, loss = 0.29, acc = 1.00 (211.0 examples/sec; 0.607 sec/batch)
2016-11-28 19:50:09.846628: step 16820, loss = 0.29, acc = 1.00 (280.1 examples/sec; 0.457 sec/batch)
2016-11-28 19:50:14.514320: step 16830, loss = 0.29, acc = 1.00 (291.4 examples/sec; 0.439 sec/batch)
2016-11-28 19:50:19.246311: step 16840, loss = 0.29, acc = 1.00 (245.6 examples/sec; 0.521 sec/batch)
2016-11-28 19:50:23.913997: step 16850, loss = 0.29, acc = 1.00 (291.8 examples/sec; 0.439 sec/batch)
2016-11-28 19:50:28.686496: step 16860, loss = 0.30, acc = 1.00 (287.6 examples/sec; 0.445 sec/batch)
2016-11-28 19:50:33.483545: step 16870, loss = 0.29, acc = 1.00 (282.3 examples/sec; 0.453 sec/batch)
2016-11-28 19:50:38.221152: step 16880, loss = 0.29, acc = 1.00 (210.4 examples/sec; 0.608 sec/batch)
2016-11-28 19:50:42.996358: step 16890, loss = 0.30, acc = 1.00 (285.5 examples/sec; 0.448 sec/batch)
2016-11-28 19:50:47.785214: step 16900, loss = 0.29, acc = 1.00 (249.3 examples/sec; 0.513 sec/batch)
At step 16900 cross validation precision: 0.438
2016-11-28 19:50:53.523278: step 16910, loss = 0.29, acc = 1.00 (290.2 examples/sec; 0.441 sec/batch)
2016-11-28 19:50:58.346173: step 16920, loss = 0.29, acc = 1.00 (290.4 examples/sec; 0.441 sec/batch)
2016-11-28 19:51:03.129413: step 16930, loss = 0.30, acc = 1.00 (251.0 examples/sec; 0.510 sec/batch)
2016-11-28 19:51:07.855409: step 16940, loss = 0.29, acc = 1.00 (287.9 examples/sec; 0.445 sec/batch)
2016-11-28 19:51:12.622295: step 16950, loss = 0.29, acc = 1.00 (288.3 examples/sec; 0.444 sec/batch)
2016-11-28 19:51:17.421447: step 16960, loss = 0.29, acc = 1.00 (286.6 examples/sec; 0.447 sec/batch)
2016-11-28 19:51:22.355592: step 16970, loss = 0.29, acc = 1.00 (233.8 examples/sec; 0.548 sec/batch)
2016-11-28 19:51:27.075233: step 16980, loss = 0.29, acc = 1.00 (287.2 examples/sec; 0.446 sec/batch)
2016-11-28 19:51:31.936875: step 16990, loss = 0.29, acc = 1.00 (291.0 examples/sec; 0.440 sec/batch)
2016-11-28 19:51:36.794294: step 17000, loss = 0.29, acc = 1.00 (289.3 examples/sec; 0.442 sec/batch)
At step 17000 cross validation precision: 0.383
2016-11-28 19:51:42.553140: step 17010, loss = 0.29, acc = 1.00 (281.0 examples/sec; 0.456 sec/batch)
2016-11-28 19:51:47.387337: step 17020, loss = 0.29, acc = 1.00 (285.3 examples/sec; 0.449 sec/batch)
2016-11-28 19:51:52.329046: step 17030, loss = 0.29, acc = 1.00 (271.8 examples/sec; 0.471 sec/batch)
2016-11-28 19:51:56.936418: step 17040, loss = 0.29, acc = 1.00 (278.1 examples/sec; 0.460 sec/batch)
2016-11-28 19:52:01.894454: step 17050, loss = 0.29, acc = 1.00 (290.2 examples/sec; 0.441 sec/batch)
2016-11-28 19:52:06.589829: step 17060, loss = 0.32, acc = 0.98 (284.4 examples/sec; 0.450 sec/batch)
2016-11-28 19:52:11.540300: step 17070, loss = 0.29, acc = 1.00 (279.1 examples/sec; 0.459 sec/batch)
2016-11-28 19:52:16.349340: step 17080, loss = 0.29, acc = 1.00 (300.5 examples/sec; 0.426 sec/batch)
2016-11-28 19:52:20.995577: step 17090, loss = 0.29, acc = 1.00 (292.5 examples/sec; 0.438 sec/batch)
2016-11-28 19:52:25.852147: step 17100, loss = 0.28, acc = 1.00 (266.2 examples/sec; 0.481 sec/batch)
At step 17100 cross validation precision: 0.336
2016-11-28 19:52:31.540299: step 17110, loss = 0.29, acc = 1.00 (280.5 examples/sec; 0.456 sec/batch)
2016-11-28 19:52:36.506075: step 17120, loss = 0.29, acc = 1.00 (201.3 examples/sec; 0.636 sec/batch)
2016-11-28 19:52:41.211346: step 17130, loss = 0.28, acc = 1.00 (266.2 examples/sec; 0.481 sec/batch)
2016-11-28 19:52:46.106464: step 17140, loss = 0.29, acc = 1.00 (210.0 examples/sec; 0.609 sec/batch)
2016-11-28 19:52:50.849912: step 17150, loss = 0.28, acc = 1.00 (288.2 examples/sec; 0.444 sec/batch)
2016-11-28 19:52:55.662061: step 17160, loss = 0.29, acc = 1.00 (293.3 examples/sec; 0.436 sec/batch)
2016-11-28 19:53:00.466205: step 17170, loss = 0.29, acc = 1.00 (207.2 examples/sec; 0.618 sec/batch)
2016-11-28 19:53:05.258225: step 17180, loss = 0.28, acc = 1.00 (221.7 examples/sec; 0.577 sec/batch)
2016-11-28 19:53:09.921361: step 17190, loss = 0.28, acc = 1.00 (234.9 examples/sec; 0.545 sec/batch)
2016-11-28 19:53:14.750705: step 17200, loss = 0.28, acc = 1.00 (291.4 examples/sec; 0.439 sec/batch)
At step 17200 cross validation precision: 0.445
2016-11-28 19:53:20.500389: step 17210, loss = 0.28, acc = 1.00 (290.3 examples/sec; 0.441 sec/batch)
2016-11-28 19:53:25.260062: step 17220, loss = 0.28, acc = 1.00 (276.5 examples/sec; 0.463 sec/batch)
2016-11-28 19:53:29.992230: step 17230, loss = 0.29, acc = 1.00 (290.2 examples/sec; 0.441 sec/batch)
2016-11-28 19:53:34.818396: step 17240, loss = 0.28, acc = 1.00 (280.2 examples/sec; 0.457 sec/batch)
2016-11-28 19:53:39.529235: step 17250, loss = 0.28, acc = 1.00 (287.0 examples/sec; 0.446 sec/batch)
2016-11-28 19:53:44.450308: step 17260, loss = 0.28, acc = 1.00 (221.2 examples/sec; 0.579 sec/batch)
2016-11-28 19:53:49.236497: step 17270, loss = 0.29, acc = 1.00 (289.7 examples/sec; 0.442 sec/batch)
2016-11-28 19:53:53.970107: step 17280, loss = 0.28, acc = 1.00 (226.2 examples/sec; 0.566 sec/batch)
2016-11-28 19:53:58.679686: step 17290, loss = 0.28, acc = 1.00 (286.0 examples/sec; 0.448 sec/batch)
2016-11-28 19:54:03.483111: step 17300, loss = 0.29, acc = 1.00 (289.6 examples/sec; 0.442 sec/batch)
At step 17300 cross validation precision: 0.438
2016-11-28 19:54:09.274268: step 17310, loss = 0.28, acc = 1.00 (208.9 examples/sec; 0.613 sec/batch)
2016-11-28 19:54:14.071518: step 17320, loss = 0.28, acc = 1.00 (259.4 examples/sec; 0.493 sec/batch)
2016-11-28 19:54:18.905111: step 17330, loss = 0.28, acc = 1.00 (288.5 examples/sec; 0.444 sec/batch)
2016-11-28 19:54:23.740843: step 17340, loss = 0.28, acc = 1.00 (257.5 examples/sec; 0.497 sec/batch)
2016-11-28 19:54:28.462119: step 17350, loss = 0.28, acc = 1.00 (283.7 examples/sec; 0.451 sec/batch)
2016-11-28 19:54:33.231998: step 17360, loss = 0.29, acc = 1.00 (208.4 examples/sec; 0.614 sec/batch)
2016-11-28 19:54:38.009107: step 17370, loss = 0.28, acc = 1.00 (292.4 examples/sec; 0.438 sec/batch)
2016-11-28 19:54:42.826290: step 17380, loss = 0.28, acc = 1.00 (293.3 examples/sec; 0.436 sec/batch)
2016-11-28 19:54:47.612469: step 17390, loss = 0.28, acc = 1.00 (283.5 examples/sec; 0.452 sec/batch)
2016-11-28 19:54:52.497935: step 17400, loss = 0.28, acc = 1.00 (275.4 examples/sec; 0.465 sec/batch)
At step 17400 cross validation precision: 0.391
2016-11-28 19:54:58.238036: step 17410, loss = 0.28, acc = 1.00 (289.8 examples/sec; 0.442 sec/batch)
2016-11-28 19:55:03.088941: step 17420, loss = 0.28, acc = 1.00 (291.1 examples/sec; 0.440 sec/batch)
2016-11-28 19:55:07.929384: step 17430, loss = 0.28, acc = 1.00 (240.3 examples/sec; 0.533 sec/batch)
2016-11-28 19:55:12.607951: step 17440, loss = 0.28, acc = 1.00 (292.5 examples/sec; 0.438 sec/batch)
2016-11-28 19:55:17.479363: step 17450, loss = 0.28, acc = 1.00 (285.0 examples/sec; 0.449 sec/batch)
2016-11-28 19:55:22.252101: step 17460, loss = 0.29, acc = 0.99 (289.9 examples/sec; 0.441 sec/batch)
2016-11-28 19:55:27.032136: step 17470, loss = 0.28, acc = 1.00 (284.4 examples/sec; 0.450 sec/batch)
2016-11-28 19:55:31.955928: step 17480, loss = 0.28, acc = 1.00 (247.2 examples/sec; 0.518 sec/batch)
2016-11-28 19:55:36.689786: step 17490, loss = 0.28, acc = 1.00 (225.2 examples/sec; 0.568 sec/batch)
2016-11-28 19:55:41.438994: step 17500, loss = 0.28, acc = 1.00 (291.5 examples/sec; 0.439 sec/batch)
At step 17500 cross validation precision: 0.477
2016-11-28 19:55:47.058123: step 17510, loss = 0.28, acc = 1.00 (288.0 examples/sec; 0.444 sec/batch)
2016-11-28 19:55:51.968294: step 17520, loss = 0.27, acc = 1.00 (292.9 examples/sec; 0.437 sec/batch)
2016-11-28 19:55:56.693531: step 17530, loss = 0.28, acc = 1.00 (287.8 examples/sec; 0.445 sec/batch)
2016-11-28 19:56:01.462147: step 17540, loss = 0.28, acc = 1.00 (285.7 examples/sec; 0.448 sec/batch)
2016-11-28 19:56:06.280410: step 17550, loss = 0.28, acc = 1.00 (290.7 examples/sec; 0.440 sec/batch)
2016-11-28 19:56:11.154659: step 17560, loss = 0.28, acc = 1.00 (285.0 examples/sec; 0.449 sec/batch)
2016-11-28 19:56:15.870282: step 17570, loss = 0.28, acc = 1.00 (288.4 examples/sec; 0.444 sec/batch)
2016-11-28 19:56:20.597924: step 17580, loss = 0.28, acc = 1.00 (246.0 examples/sec; 0.520 sec/batch)
2016-11-28 19:56:25.432384: step 17590, loss = 0.28, acc = 1.00 (286.8 examples/sec; 0.446 sec/batch)
2016-11-28 19:56:30.152493: step 17600, loss = 0.27, acc = 1.00 (288.7 examples/sec; 0.443 sec/batch)
At step 17600 cross validation precision: 0.406
2016-11-28 19:56:35.871269: step 17610, loss = 0.28, acc = 1.00 (285.9 examples/sec; 0.448 sec/batch)
2016-11-28 19:56:40.641261: step 17620, loss = 0.27, acc = 1.00 (277.3 examples/sec; 0.462 sec/batch)
2016-11-28 19:56:45.454032: step 17630, loss = 0.27, acc = 1.00 (241.5 examples/sec; 0.530 sec/batch)
2016-11-28 19:56:50.147474: step 17640, loss = 0.27, acc = 1.00 (226.8 examples/sec; 0.564 sec/batch)
2016-11-28 19:56:54.975448: step 17650, loss = 0.27, acc = 1.00 (247.5 examples/sec; 0.517 sec/batch)
2016-11-28 19:56:59.681700: step 17660, loss = 0.27, acc = 1.00 (290.1 examples/sec; 0.441 sec/batch)
2016-11-28 19:57:04.425194: step 17670, loss = 0.27, acc = 1.00 (292.9 examples/sec; 0.437 sec/batch)
2016-11-28 19:57:09.356846: step 17680, loss = 0.27, acc = 1.00 (205.8 examples/sec; 0.622 sec/batch)
2016-11-28 19:57:14.005186: step 17690, loss = 0.27, acc = 1.00 (288.1 examples/sec; 0.444 sec/batch)
2016-11-28 19:57:18.876460: step 17700, loss = 0.28, acc = 1.00 (290.3 examples/sec; 0.441 sec/batch)
At step 17700 cross validation precision: 0.430
2016-11-28 19:57:24.811142: step 17710, loss = 0.28, acc = 1.00 (287.2 examples/sec; 0.446 sec/batch)
2016-11-28 19:57:29.618465: step 17720, loss = 0.27, acc = 1.00 (295.0 examples/sec; 0.434 sec/batch)
2016-11-28 19:57:34.252661: step 17730, loss = 0.27, acc = 1.00 (287.7 examples/sec; 0.445 sec/batch)
2016-11-28 19:57:39.216669: step 17740, loss = 0.27, acc = 1.00 (289.8 examples/sec; 0.442 sec/batch)
2016-11-28 19:57:44.172970: step 17750, loss = 0.27, acc = 1.00 (244.6 examples/sec; 0.523 sec/batch)
2016-11-28 19:57:49.085822: step 17760, loss = 0.27, acc = 1.00 (271.2 examples/sec; 0.472 sec/batch)
2016-11-28 19:57:53.892937: step 17770, loss = 0.27, acc = 1.00 (191.6 examples/sec; 0.668 sec/batch)
2016-11-28 19:57:58.546272: step 17780, loss = 0.27, acc = 1.00 (271.5 examples/sec; 0.471 sec/batch)
2016-11-28 19:58:03.501445: step 17790, loss = 0.27, acc = 1.00 (201.4 examples/sec; 0.636 sec/batch)
2016-11-28 19:58:08.224731: step 17800, loss = 0.27, acc = 1.00 (284.2 examples/sec; 0.450 sec/batch)
At step 17800 cross validation precision: 0.414
2016-11-28 19:58:13.928701: step 17810, loss = 0.27, acc = 1.00 (282.2 examples/sec; 0.454 sec/batch)
2016-11-28 19:58:18.868759: step 17820, loss = 0.27, acc = 1.00 (266.8 examples/sec; 0.480 sec/batch)
2016-11-28 19:58:23.633227: step 17830, loss = 0.27, acc = 1.00 (290.1 examples/sec; 0.441 sec/batch)
2016-11-28 19:58:28.248708: step 17840, loss = 0.27, acc = 1.00 (262.2 examples/sec; 0.488 sec/batch)
2016-11-28 19:58:32.962670: step 17850, loss = 0.27, acc = 1.00 (286.4 examples/sec; 0.447 sec/batch)
2016-11-28 19:58:37.848181: step 17860, loss = 0.27, acc = 1.00 (290.5 examples/sec; 0.441 sec/batch)
2016-11-28 19:58:42.559636: step 17870, loss = 0.27, acc = 1.00 (294.7 examples/sec; 0.434 sec/batch)
2016-11-28 19:58:47.533982: step 17880, loss = 0.27, acc = 1.00 (227.6 examples/sec; 0.562 sec/batch)
2016-11-28 19:58:52.256010: step 17890, loss = 0.27, acc = 1.00 (291.9 examples/sec; 0.439 sec/batch)
2016-11-28 19:58:57.157574: step 17900, loss = 0.27, acc = 1.00 (249.2 examples/sec; 0.514 sec/batch)
At step 17900 cross validation precision: 0.328
2016-11-28 19:59:02.812168: step 17910, loss = 0.27, acc = 1.00 (287.0 examples/sec; 0.446 sec/batch)
2016-11-28 19:59:07.606151: step 17920, loss = 0.28, acc = 0.99 (280.2 examples/sec; 0.457 sec/batch)
2016-11-28 19:59:12.422850: step 17930, loss = 0.27, acc = 1.00 (288.3 examples/sec; 0.444 sec/batch)
2016-11-28 19:59:17.167991: step 17940, loss = 0.27, acc = 1.00 (279.6 examples/sec; 0.458 sec/batch)
2016-11-28 19:59:22.042699: step 17950, loss = 0.27, acc = 1.00 (290.8 examples/sec; 0.440 sec/batch)
2016-11-28 19:59:26.863295: step 17960, loss = 0.27, acc = 1.00 (249.2 examples/sec; 0.514 sec/batch)
2016-11-28 19:59:31.564251: step 17970, loss = 0.26, acc = 1.00 (290.9 examples/sec; 0.440 sec/batch)
2016-11-28 19:59:36.290569: step 17980, loss = 0.27, acc = 1.00 (296.5 examples/sec; 0.432 sec/batch)
2016-11-28 19:59:41.011981: step 17990, loss = 0.27, acc = 1.00 (284.9 examples/sec; 0.449 sec/batch)
2016-11-28 19:59:45.859485: step 18000, loss = 0.27, acc = 1.00 (293.1 examples/sec; 0.437 sec/batch)
At step 18000 cross validation precision: 0.414
2016-11-28 19:59:51.433524: step 18010, loss = 0.27, acc = 1.00 (280.7 examples/sec; 0.456 sec/batch)
2016-11-28 19:59:56.351630: step 18020, loss = 0.27, acc = 1.00 (251.1 examples/sec; 0.510 sec/batch)
2016-11-28 20:00:00.978731: step 18030, loss = 0.26, acc = 1.00 (300.1 examples/sec; 0.427 sec/batch)
2016-11-28 20:00:05.769255: step 18040, loss = 0.26, acc = 1.00 (287.4 examples/sec; 0.445 sec/batch)
2016-11-28 20:00:10.496010: step 18050, loss = 0.26, acc = 1.00 (289.7 examples/sec; 0.442 sec/batch)
2016-11-28 20:00:15.466751: step 18060, loss = 0.27, acc = 1.00 (240.0 examples/sec; 0.533 sec/batch)
2016-11-28 20:00:20.024070: step 18070, loss = 0.26, acc = 1.00 (299.9 examples/sec; 0.427 sec/batch)
2016-11-28 20:00:24.853586: step 18080, loss = 0.27, acc = 1.00 (287.9 examples/sec; 0.445 sec/batch)
2016-11-28 20:00:29.717749: step 18090, loss = 0.26, acc = 1.00 (287.8 examples/sec; 0.445 sec/batch)
2016-11-28 20:00:34.540792: step 18100, loss = 0.26, acc = 1.00 (287.6 examples/sec; 0.445 sec/batch)
At step 18100 cross validation precision: 0.438
2016-11-28 20:00:40.200274: step 18110, loss = 0.26, acc = 1.00 (295.6 examples/sec; 0.433 sec/batch)
2016-11-28 20:00:44.843606: step 18120, loss = 0.26, acc = 1.00 (280.8 examples/sec; 0.456 sec/batch)
2016-11-28 20:00:49.890924: step 18130, loss = 0.26, acc = 1.00 (283.8 examples/sec; 0.451 sec/batch)
2016-11-28 20:00:54.752505: step 18140, loss = 0.26, acc = 1.00 (243.6 examples/sec; 0.525 sec/batch)
2016-11-28 20:00:59.572313: step 18150, loss = 0.26, acc = 1.00 (290.5 examples/sec; 0.441 sec/batch)
2016-11-28 20:01:04.389924: step 18160, loss = 0.26, acc = 1.00 (195.3 examples/sec; 0.655 sec/batch)
2016-11-28 20:01:09.011080: step 18170, loss = 0.26, acc = 1.00 (282.8 examples/sec; 0.453 sec/batch)
2016-11-28 20:01:14.018584: step 18180, loss = 0.26, acc = 1.00 (246.1 examples/sec; 0.520 sec/batch)
2016-11-28 20:01:18.737813: step 18190, loss = 0.27, acc = 0.99 (288.1 examples/sec; 0.444 sec/batch)
2016-11-28 20:01:23.338726: step 18200, loss = 0.26, acc = 1.00 (276.7 examples/sec; 0.463 sec/batch)
At step 18200 cross validation precision: 0.453
2016-11-28 20:01:29.084035: step 18210, loss = 0.26, acc = 1.00 (285.4 examples/sec; 0.449 sec/batch)
2016-11-28 20:01:33.955299: step 18220, loss = 0.26, acc = 1.00 (226.3 examples/sec; 0.566 sec/batch)
2016-11-28 20:01:38.508992: step 18230, loss = 0.26, acc = 1.00 (285.8 examples/sec; 0.448 sec/batch)
2016-11-28 20:01:43.294398: step 18240, loss = 0.26, acc = 1.00 (237.5 examples/sec; 0.539 sec/batch)
